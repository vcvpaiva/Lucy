Content from: https://en.wikipedia.org/wiki/Pythagorean_theorem

$$ a 2 + b 2 = c 2 {\displaystyle a^{2}+b^{2}=c^{2}} $$ In mathematics, the Pythagorean theorem or Pythagoras' theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b and the hypotenuse c, sometimes called the Pythagorean equation: $$ a 2 + b 2 = c 2 . {\displaystyle a^{2}+b^{2}=c^{2}.} $$ The theorem is named for the Greek philosopher Pythagoras, born around 570 BC. The theorem has been proved numerous times by many different methods – possibly the most for any mathematical theorem. The proofs are diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. When Euclidean space is represented by a Cartesian coordinate system in analytic geometry, Euclidean distance satisfies the Pythagorean relation: the squared distance between two points equals the sum of squares of the difference in each coordinate between the points. The theorem can be generalized in various ways: to higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and to objects that are not triangles at all but n-dimensional solids. In one rearrangement proof, two squares are used whose sides have a measure of a + b a+b and which contain four right triangles whose sides are a, b and c, with the hypotenuse being c. In the square on the right side, the triangles are placed such that the corners of the square correspond to the corners of the right angle in the triangles, forming a square in the center whose sides are length c. Each outer square has an area of ( a + b ) 2 (a+b)^{2} as well as 2 a b + c 2 2ab+c^{2} , with 2 a b 2ab representing the total area of the four triangles. Within the big square on the left side, the four triangles are moved to form two similar rectangles with sides of length a and b. These rectangles in their new position have now delineated two new squares, one having side length a is formed in the bottom-left corner, and another square of side length b formed in the top-right corner. In this new position, this left side now has a square of area ( a + b ) 2 (a+b)^{2} as well as 2 a b + a 2 + b 2 2ab+a^{2+b^{2}} . Since both squares have the area of ( a + b ) 2 (a+b)^{2} it follows that the other measure of the square area also equal each other such that 2 a b + c 2 2ab+c^{2} = 2 a b + a 2 + b 2 2ab+a^{2+b^{2}} . With the area of the four triangles removed from both side of the equation what remains is a 2 + b 2 = c 2 . a^{2+b^{2}=c^{2}.} $$ a + b {\displaystyle a+b} $$ $$ ( a + b ) 2 {\displaystyle (a+b)^{2}} $$ $$ 2 a b + c 2 {\displaystyle 2ab+c^{2}} $$ $$ 2 a b {\displaystyle 2ab} $$ $$ ( a + b ) 2 {\displaystyle (a+b)^{2}} $$ $$ 2 a b + a 2 + b 2 {\displaystyle 2ab+a^{2}+b^{2}} $$ $$ ( a + b ) 2 {\displaystyle (a+b)^{2}} $$ $$ 2 a b + c 2 {\displaystyle 2ab+c^{2}} $$ $$ 2 a b + a 2 + b 2 {\displaystyle 2ab+a^{2}+b^{2}} $$ $$ a 2 + b 2 = c 2 . {\displaystyle a^{2}+b^{2}=c^{2}.} $$ In another proof rectangles in the second box can also be placed such that both have one corner that correspond to consecutive corners of the square. In this way they also form two boxes, this time in consecutive corners, with areas a 2 a^{2} and b 2 b^{2} which will again lead to a second square of with the area 2 a b + a 2 + b 2 2ab+a^{2+b^{2}} . $$ a 2 {\displaystyle a^{2}} $$ $$ b 2 {\displaystyle b^{2}} $$ $$ 2 a b + a 2 + b 2 {\displaystyle 2ab+a^{2}+b^{2}} $$ English mathematician Sir Thomas Heath gives this proof in his commentary on Proposition I.47 in Euclid's Elements, and mentions the proposals of German mathematicians Carl Anton Bretschneider and Hermann Hankel that Pythagoras may have known this proof. Heath himself favors a different proposal for a Pythagorean proof, but acknowledges from the outset of his discussion "that the Greek literature which we possess belonging to the first five centuries after Pythagoras contains no statement specifying this or any other particular great geometric discovery to him." Recent scholarship has cast increasing doubt on any sort of role for Pythagoras as a creator of mathematics, although debate about this continues. The theorem can be proved algebraically using four copies of the same triangle arranged symmetrically around a square with side c, as shown in the lower part of the diagram. This results in a larger square, with side a + b and area (a + b)2. The four triangles and the square side c must have the same area as the larger square, $$ ( b + a ) 2 = c 2 + 4 a b 2 = c 2 + 2 a b , {\displaystyle (b+a)^{2}=c^{2}+4{\frac {ab}{2}}=c^{2}+2ab,} $$ giving $$ c 2 = ( b + a ) 2 − 2 a b = b 2 + 2 a b + a 2 − 2 a b = a 2 + b 2 . {\displaystyle c^{2}=(b+a)^{2}-2ab=b^{2}+2ab+a^{2}-2ab=a^{2}+b^{2}.} $$ A similar proof uses four copies of a right triangle with sides a, b and c, arranged inside a square with side c as in the top half of the diagram. The triangles are similar with area 1 2 a b {\tfrac {1{2}}ab} , while the small square has side b − a and area (b − a)2. The area of the large square is therefore $$ 1 2 a b {\displaystyle {\tfrac {1}{2}}ab} $$ $$ ( b − a ) 2 + 4 a b 2 = ( b − a ) 2 + 2 a b = b 2 − 2 a b + a 2 + 2 a b = a 2 + b 2 . {\displaystyle (b-a)^{2}+4{\frac {ab}{2}}=(b-a)^{2}+2ab=b^{2}-2ab+a^{2}+2ab=a^{2}+b^{2}.} $$ But this is a square with side c and area c2, so $$ c 2 = a 2 + b 2 . {\displaystyle c^{2}=a^{2}+b^{2}.} $$ This theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book The Pythagorean Proposition contains 370 proofs. $$ A × B 2 . {\displaystyle A\times B^{2}.} $$ This proof is based on the proportionality of the sides of three similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles. Let ABC represent a right triangle, with the right angle located at C, as shown on the figure. Draw the altitude from point C, and call H its intersection with the side AB. Point H divides the length of the hypotenuse c into parts d and e. The new triangle, ACH, is similar to triangle ABC, because they both have a right angle (by definition of the altitude), and they share the angle at A, meaning that the third angle will be the same in both triangles as well, marked as θ in the figure. By a similar reasoning, the triangle CBH is also similar to ABC. The proof of similarity of the triangles requires the triangle postulate: The sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides: $$ B C A B = B H B C and A C A B = A H A C . {\displaystyle {\frac {BC}{AB}}={\frac {BH}{BC}}{\text{ and }}{\frac {AC}{AB}}={\frac {AH}{AC}}.} $$ The first result equates the cosines of the angles θ, whereas the second result equates their sines. These ratios can be written as $$ B C 2 = A B × B H and A C 2 = A B × A H . {\displaystyle BC^{2}=AB\times BH{\text{ and }}AC^{2}=AB\times AH.} $$ Summing these two equalities results in $$ B C 2 + A C 2 = A B × B H + A B × A H = A B ( A H + B H ) = A B 2 , {\displaystyle BC^{2}+AC^{2}=AB\times BH+AB\times AH=AB(AH+BH)=AB^{2},} $$ which, after simplification, demonstrates the Pythagorean theorem: $$ B C 2 + A C 2 = A B 2 . {\displaystyle BC^{2}+AC^{2}=AB^{2}.} $$ The role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the Elements, and that the theory of proportions needed further development at that time. Albert Einstein gave a proof by dissection in which the pieces do not need to be moved. Instead of using a square on the hypotenuse and two squares on the legs, one can use any other shape that includes the hypotenuse, and two similar shapes that each include one of two legs instead of the hypotenuse (see Similar figures on the three sides). In Einstein's proof, the shape that includes the hypotenuse is the right triangle itself. The dissection consists of dropping a perpendicular from the vertex of the right angle of the triangle to the hypotenuse, thus splitting the whole triangle into two parts. Those two parts have the same shape as the original right triangle, and have the legs of the original triangle as their hypotenuses, and the sum of their areas is that of the original triangle. Because the ratio of the area of a right triangle to the square of its hypotenuse is the same for similar triangles, the relationship between the areas of the three triangles holds for the squares of the sides of the large triangle as well. In outline, here is how the proof in Euclid's Elements proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow. Let A, B, C be the vertices of a right triangle, with a right angle at A. Drop a perpendicular from A to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs. For the formal proof, we require four elementary lemmata: Next, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square. The proof is as follows: This proof, which appears in Euclid's Elements as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares. This is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used. Another by rearrangement is given by the middle animation. A large square is formed with area c2, from four identical right triangles with sides a, b and c, fitted around a small central square. Then two rectangles are formed with sides a and b by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas a2 and b2, which must have the same area as the initial large square. The third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse – or conversely the large square can be divided as shown into pieces that fill the other two. This way of cutting one figure into pieces and rearranging them to get another figure is called dissection. This shows the area of the large square equals that of the two smaller ones. As shown in the accompanying animation, area-preserving shear mappings and translations can transform the squares on the sides adjacent to the right-angle onto the square on the hypotenuse, together covering it exactly. Each shear leaves the base and height unchanged, thus leaving the area unchanged too. The translations also leave the area unchanged, as they do not alter the shapes at all. Each square is first sheared into a parallelogram, and then into a rectangle which can be translated onto one section of the square on the hypotenuse. A related proof was published by future U.S. President James A. Garfield (then a U.S. Representative) (see diagram). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is $$ 1 2 ( b + a ) 2 . {\displaystyle {\frac {1}{2}}(b+a)^{2}.} $$ The inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of 1 2 {\frac {1{2}}} , which is removed by multiplying by two to give the result. $$ 1 2 {\displaystyle {\frac {1}{2}}} $$ One can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus. The triangle ABC is a right triangle, as shown in the upper part of the diagram, with BC the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length y, the side AC of length x and the side AB of length a, as seen in the lower diagram part. If x is increased by a small amount dx by extending the side AC slightly to D, then y also increases by dy. These form two sides of a triangle, CDE, which (with E chosen so CE is perpendicular to the hypotenuse) is a right triangle approximately similar to ABC. Therefore, the ratios of their sides must be the same, that is: $$ d y d x = x y . {\displaystyle {\frac {dy}{dx}}={\frac {x}{y}}.} $$ This can be rewritten as y d y = x d x y\,dy=x\,dx , which is a differential equation that can be solved by direct integration: $$ y d y = x d x {\displaystyle y\,dy=x\,dx} $$ $$ ∫ y d y = ∫ x d x , {\displaystyle \int y\,dy=\int x\,dx\,,} $$ giving $$ y 2 = x 2 + C . {\displaystyle y^{2}=x^{2}+C.} $$ The constant can be deduced from x = 0, y = a to give the equation $$ y 2 = x 2 + a 2 . {\displaystyle y^{2}=x^{2}+a^{2}.} $$ This is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of dx and dy. The converse of the theorem is also true: Given a triangle with sides of length a, b, and c, if a2 + b2 = c2, then the angle between sides a and b is a right angle. For any three positive real numbers a, b, and c such that a2 + b2 = c2, there exists a triangle with sides a, b and c as a consequence of the converse of the triangle inequality. This converse appears in Euclid's Elements (Book I, Proposition 48): "If in a triangle the square on one of the sides equals the sum of the squares on the remaining two sides of the triangle, then the angle contained by the remaining two sides of the triangle is right." It can be proved using the law of cosines or as follows: Let ABC be a triangle with side lengths a, b, and c, with a2 + b2 = c2. Construct a second triangle with sides of length a and b containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length c = √a2 + b2, the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths a, b and c, the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths a and b in the original triangle is a right angle. The above proof of the converse makes use of the Pythagorean theorem itself. The converse can also be proved without assuming the Pythagorean theorem. A corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let c be chosen to be the longest of the three sides and a + b > c (otherwise there is no triangle according to the triangle inequality). The following statements apply: Edsger W. Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language: where α is the angle opposite to side a, β is the angle opposite to side b, γ is the angle opposite to side c, and sgn is the sign function. A Pythagorean triple has three positive integers a, b, and c, such that a2 + b2 = c2. In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Such a triple is commonly written (a, b, c). Some well-known examples are (3, 4, 5) and (5, 12, 13). A primitive Pythagorean triple is one in which a, b and c are coprime (the greatest common divisor of a, b and c is 1). The following is a list of primitive Pythagorean triples with values less than 100: Given a right triangle with sides a , b , c a,b,c and altitude d d (a line from the right angle and perpendicular to the hypotenuse c c ). The Pythagorean theorem has, $$ a , b , c {\displaystyle a,b,c} $$ $$ d {\displaystyle d} $$ $$ c {\displaystyle c} $$ $$ a 2 + b 2 = c 2 {\displaystyle a^{2}+b^{2}=c^{2}} $$ while the inverse Pythagorean theorem relates the two legs a , b a,b to the altitude d d , $$ a , b {\displaystyle a,b} $$ $$ d {\displaystyle d} $$ $$ 1 a 2 + 1 b 2 = 1 d 2 {\displaystyle {\frac {1}{a^{2}}}+{\frac {1}{b^{2}}}={\frac {1}{d^{2}}}} $$ The equation can be transformed to, $$ 1 ( x z ) 2 + 1 ( y z ) 2 = 1 ( x y ) 2 {\displaystyle {\frac {1}{(xz)^{2}}}+{\frac {1}{(yz)^{2}}}={\frac {1}{(xy)^{2}}}} $$ where x 2 + y 2 = z 2 x^{2+y^{2}=z^{2}} for any non-zero real x , y , z x,y,z . If the a , b , d a,b,d are to be integers, the smallest solution a > b > d a>b>d is then $$ x 2 + y 2 = z 2 {\displaystyle x^{2}+y^{2}=z^{2}} $$ $$ x , y , z {\displaystyle x,y,z} $$ $$ a , b , d {\displaystyle a,b,d} $$ $$ a > b > d {\displaystyle a>b>d} $$ $$ 1 20 2 + 1 15 2 = 1 12 2 {\displaystyle {\frac {1}{20^{2}}}+{\frac {1}{15^{2}}}={\frac {1}{12^{2}}}} $$ using the smallest Pythagorean triple 3 , 4 , 5 3,4,5 . The reciprocal Pythagorean theorem is a special case of the optic equation $$ 3 , 4 , 5 {\displaystyle 3,4,5} $$ $$ 1 p + 1 q = 1 r {\displaystyle {\frac {1}{p}}+{\frac {1}{q}}={\frac {1}{r}}} $$ where the denominators are squares and also for a heptagonal triangle whose sides p , q , r p,q,r are square numbers. $$ p , q , r {\displaystyle p,q,r} $$ One of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras' theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation. The figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled "1") that is the chosen unit for measurement. In each right triangle, Pythagoras' theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as √2, √3, √5 . For more detail, see Quadratic irrational. Incommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum (ca. 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable. A careful discussion of Hippasus's contributions is found in Fritz. For any complex number $$ z = x + i y , {\displaystyle z=x+iy,} $$ the absolute value or modulus is given by $$ r = | z | = x 2 + y 2 . {\displaystyle r=|z|={\sqrt {x^{2}+y^{2}}}.} $$ So the three quantities, r, x and y are related by the Pythagorean equation, $$ r 2 = x 2 + y 2 . {\displaystyle r^{2}=x^{2}+y^{2}.} $$ Note that r is defined to be a positive number or zero but x and y can be negative as well as positive. Geometrically r is the distance of the z from zero or the origin O in the complex plane. This can be generalised to find the distance between two points, z1 and z2 say. The required distance is given by $$ | z 1 − z 2 | = ( x 1 − x 2 ) 2 + ( y 1 − y 2 ) 2 , {\displaystyle |z_{1}-z_{2}|={\sqrt {(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}},} $$ so again they are related by a version of the Pythagorean equation, $$ | z 1 − z 2 | 2 = ( x 1 − x 2 ) 2 + ( y 1 − y 2 ) 2 . {\displaystyle |z_{1}-z_{2}|^{2}=(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}.} $$ The distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If (x1, y1) and (x2, y2) are points in the plane, then the distance between them, also called the Euclidean distance, is given by $$ ( x 1 − x 2 ) 2 + ( y 1 − y 2 ) 2 . {\displaystyle {\sqrt {(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}}.} $$ More generally, in Euclidean n-space, the Euclidean distance between two points, A = ( a 1 , a 2 , … , a n ) A\,=\,(a_{1,a_{2},\dots ,a_{n})} and B = ( b 1 , b 2 , … , b n ) B\,=\,(b_{1,b_{2},\dots ,b_{n})} , is defined, by generalization of the Pythagorean theorem, as: $$ A = ( a 1 , a 2 , … , a n ) {\displaystyle A\,=\,(a_{1},a_{2},\dots ,a_{n})} $$ $$ B = ( b 1 , b 2 , … , b n ) {\displaystyle B\,=\,(b_{1},b_{2},\dots ,b_{n})} $$ $$ ( a 1 − b 1 ) 2 + ( a 2 − b 2 ) 2 + ⋯ + ( a n − b n ) 2 = ∑ i = 1 n ( a i − b i ) 2 . {\displaystyle {\sqrt {(a_{1}-b_{1})^{2}+(a_{2}-b_{2})^{2}+\cdots +(a_{n}-b_{n})^{2}}}={\sqrt {\sum _{i=1}^{n}(a_{i}-b_{i})^{2}}}.} $$ If instead of Euclidean distance, the square of this value (the squared Euclidean distance, or SED) is used, the resulting equation avoids square roots and is simply a sum of the SED of the coordinates: $$ ( a 1 − b 1 ) 2 + ( a 2 − b 2 ) 2 + ⋯ + ( a n − b n ) 2 = ∑ i = 1 n ( a i − b i ) 2 . {\displaystyle (a_{1}-b_{1})^{2}+(a_{2}-b_{2})^{2}+\cdots +(a_{n}-b_{n})^{2}=\sum _{i=1}^{n}(a_{i}-b_{i})^{2}.} $$ The squared form is a smooth, convex function of both points, and is widely used in optimization theory and statistics, forming the basis of least squares. If Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras' theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates (r, θ) can be introduced as: $$ x = r cos ⁡ θ , y = r sin ⁡ θ . {\displaystyle x=r\cos \theta ,\ y=r\sin \theta .} $$ Then two points with locations (r1, θ1) and (r2, θ2) are separated by a distance s: $$ s 2 = ( x 1 − x 2 ) 2 + ( y 1 − y 2 ) 2 = ( r 1 cos ⁡ θ 1 − r 2 cos ⁡ θ 2 ) 2 + ( r 1 sin ⁡ θ 1 − r 2 sin ⁡ θ 2 ) 2 . {\displaystyle s^{2}=(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}=(r_{1}\cos \theta _{1}-r_{2}\cos \theta _{2})^{2}+(r_{1}\sin \theta _{1}-r_{2}\sin \theta _{2})^{2}.} $$ Performing the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as: $$ s 2 = r 1 2 + r 2 2 − 2 r 1 r 2 ( cos ⁡ θ 1 cos ⁡ θ 2 + sin ⁡ θ 1 sin ⁡ θ 2 ) = r 1 2 + r 2 2 − 2 r 1 r 2 cos ⁡ ( θ 1 − θ 2 ) = r 1 2 + r 2 2 − 2 r 1 r 2 cos ⁡ Δ θ , {\displaystyle {\begin{aligned}s^{2}&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\left(\cos \theta _{1}\cos \theta _{2}+\sin \theta _{1}\sin \theta _{2}\right)\\&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\cos \left(\theta _{1}-\theta _{2}\right)\\&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\cos \Delta \theta ,\end{aligned}}} $$ using the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the generalized Pythagorean theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle Δθ = π/2, and the form corresponding to Pythagoras' theorem is regained: s 2 = r 1 2 + r 2 2 . s^{2=r_{1}^{2}+r_{2}^{2}.} The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles. $$ s 2 = r 1 2 + r 2 2 . {\displaystyle s^{2}=r_{1}^{2}+r_{2}^{2}.} $$ In a right triangle with sides a, b and hypotenuse c, trigonometry determines the sine and cosine of the angle θ between side a and the hypotenuse as: $$ sin ⁡ θ = b c , cos ⁡ θ = a c . {\displaystyle \sin \theta ={\frac {b}{c}},\quad \cos \theta ={\frac {a}{c}}.} $$ From that it follows: $$ cos 2 θ + sin 2 θ = a 2 + b 2 c 2 = 1 , {\displaystyle {\cos }^{2}\theta +{\sin }^{2}\theta ={\frac {a^{2}+b^{2}}{c^{2}}}=1,} $$ where the last step applies Pythagoras' theorem. This relation between sine and cosine is sometimes called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin θ and adjacent side of size cos θ in units of the hypotenuse. The Pythagorean theorem relates the cross product and dot product in a similar way: $$ ‖ a × b ‖ 2 + ( a ⋅ b ) 2 = ‖ a ‖ 2 ‖ b ‖ 2 . {\displaystyle \|\mathbf {a} \times \mathbf {b} \|^{2}+(\mathbf {a} \cdot \mathbf {b} )^{2}=\|\mathbf {a} \|^{2}\|\mathbf {b} \|^{2}.} $$ This can be seen from the definitions of the cross product and dot product, as $$ a × b = a b n sin ⁡ θ a ⋅ b = a b cos ⁡ θ , {\displaystyle {\begin{aligned}\mathbf {a} \times \mathbf {b} &=ab\mathbf {n} \sin {\theta }\\\mathbf {a} \cdot \mathbf {b} &=ab\cos {\theta },\end{aligned}}} $$ with n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity. This can also be used to define the cross product. By rearranging the following equation is obtained $$ ‖ a × b ‖ 2 = ‖ a ‖ 2 ‖ b ‖ 2 − ( a ⋅ b ) 2 . {\displaystyle \|\mathbf {a} \times \mathbf {b} \|^{2}=\|\mathbf {a} \|^{2}\|\mathbf {b} \|^{2}-(\mathbf {a} \cdot \mathbf {b} )^{2}.} $$ This can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions. If the first four of the Euclidean geometry axioms are assumed to be true then the Pythagorean theorem is equivalent to the fifth. That is, Euclid's fifth postulate implies the Pythagorean theorem and vice-versa. The Pythagorean theorem generalizes beyond the areas of squares on the three sides to any similar figures. This was known by Hippocrates of Chios in the 5th century BC, and was included by Euclid in his Elements: If one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side. This extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are a:b:c). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle). The basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas A, B and C are erected on sides with corresponding lengths a, b and c then: $$ A a 2 = B b 2 = C c 2 , {\displaystyle {\frac {A}{a^{2}}}={\frac {B}{b^{2}}}={\frac {C}{c^{2}}}\,,} $$ $$ ⇒ A + B = a 2 c 2 C + b 2 c 2 C . {\displaystyle \Rightarrow A+B={\frac {a^{2}}{c^{2}}}C+{\frac {b^{2}}{c^{2}}}C\,.} $$ But, by the Pythagorean theorem, a2 + b2 = c2, so A + B = C. Conversely, if we can prove that A + B = C for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle C on its hypotenuse, and two similar right triangles (A and B ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus A + B = C and reversing the above logic leads to the Pythagorean theorem a2 + b2 = c2. (See also Einstein's proof by dissection without rearrangement) The Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines, which states that a 2 + b 2 − 2 a b cos ⁡ θ = c 2 a^{2+b^{2}-2ab\cos {\theta }=c^{2}} where θ \theta is the angle between sides a a and b b . $$ a 2 + b 2 − 2 a b cos ⁡ θ = c 2 {\displaystyle a^{2}+b^{2}-2ab\cos {\theta }=c^{2}} $$ $$ θ {\displaystyle \theta } $$ $$ a {\displaystyle a} $$ $$ b {\displaystyle b} $$ When θ \theta is π 2 {\frac {\pi {2}}} radians or 90°, then cos ⁡ θ = 0 \cos {\theta =0} , and the formula reduces to the usual Pythagorean theorem. $$ θ {\displaystyle \theta } $$ $$ π 2 {\displaystyle {\frac {\pi }{2}}} $$ $$ cos ⁡ θ = 0 {\displaystyle \cos {\theta }=0} $$ At any selected angle of a general triangle of sides a, b, c, inscribe an isosceles triangle such that the equal angles at its base θ are the same as the selected angle. Suppose the selected angle θ is opposite the side labeled c. Inscribing the isosceles triangle forms triangle CAD with angle θ opposite side b and with side r along c. A second triangle is formed with angle θ opposite side a and a side with length s along c, as shown in the figure. Thābit ibn Qurra stated that the sides of the three triangles were related as: $$ a 2 + b 2 = c ( r + s ) . {\displaystyle a^{2}+b^{2}=c(r+s)\ .} $$ As the angle θ approaches π/2, the base of the isosceles triangle narrows, and lengths r and s overlap less and less. When θ = π/2, ADB becomes a right triangle, r + s = c, and the original Pythagorean theorem is regained. One proof observes that triangle ABC has the same angles as triangle CAD, but in opposite order. (The two triangles share the angle at vertex A, both contain the angle θ, and so also have the same third angle by the triangle postulate.) Consequently, ABC is similar to the reflection of CAD, the triangle DAC in the lower panel. Taking the ratio of sides opposite and adjacent to θ, $$ c b = b r . {\displaystyle {\frac {c}{b}}={\frac {b}{r}}\ .} $$ Likewise, for the reflection of the other triangle, $$ c a = a s . {\displaystyle {\frac {c}{a}}={\frac {a}{s}}\ .} $$ Clearing fractions and adding these two relations: $$ c s + c r = a 2 + b 2 , {\displaystyle cs+cr=a^{2}+b^{2}\ ,} $$ the required result. The theorem remains valid if the angle θ \theta is obtuse so the lengths r and s are non-overlapping. $$ θ {\displaystyle \theta } $$ Pappus's area theorem is a further generalization, that applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares (squares are a special case, of course). The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras' theorem, and was considered a generalization by Pappus of Alexandria in 4 AD The lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base b and height h. However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms. In terms of solid geometry, Pythagoras' theorem can be applied to three dimensions as follows. Consider the cuboid shown in the figure. The length of face diagonal AC is found from Pythagoras' theorem as: $$ A C ¯ 2 = A B ¯ 2 + B C ¯ 2 , {\displaystyle {\overline {AC}}^{\,2}={\overline {AB}}^{\,2}+{\overline {BC}}^{\,2}\,,} $$ where these three sides form a right triangle. Using diagonal AC and the horizontal edge CD, the length of body diagonal AD then is found by a second application of Pythagoras' theorem as: $$ A D ¯ 2 = A C ¯ 2 + C D ¯ 2 , {\displaystyle {\overline {AD}}^{\,2}={\overline {AC}}^{\,2}+{\overline {CD}}^{\,2}\,,} $$ or, doing it all in one step: $$ A D ¯ 2 = A B ¯ 2 + B C ¯ 2 + C D ¯ 2 . {\displaystyle {\overline {AD}}^{\,2}={\overline {AB}}^{\,2}+{\overline {BC}}^{\,2}+{\overline {CD}}^{\,2}\,.} $$ This result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {vk} (the three mutually perpendicular sides): $$ ‖ v ‖ 2 = ∑ k = 1 3 ‖ v k ‖ 2 . {\displaystyle \|\mathbf {v} \|^{2}=\sum _{k=1}^{3}\|\mathbf {v} _{k}\|^{2}.} $$ This one-step formulation may be viewed as a generalization of Pythagoras' theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras' theorem to a succession of right triangles in a sequence of orthogonal planes. A substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the "n-dimensional Pythagorean theorem": Let x 1 , x 2 , … , x n x_{1,x_{2},\ldots ,x_{n}} be orthogonal vectors in Rn. Consider the n-dimensional simplex S with vertices 0 , x 1 , … , x n 0,x_{1,\ldots ,x_{n}} . (Think of the (n − 1)-dimensional simplex with vertices x 1 , … , x n x_{1,\ldots ,x_{n}} not including the origin as the "hypotenuse" of S and the remaining (n − 1)-dimensional faces of S as its "legs".) Then the square of the volume of the hypotenuse of S is the sum of the squares of the volumes of the n legs. $$ x 1 , x 2 , … , x n {\displaystyle x_{1},x_{2},\ldots ,x_{n}} $$ $$ 0 , x 1 , … , x n {\displaystyle 0,x_{1},\ldots ,x_{n}} $$ $$ x 1 , … , x n {\displaystyle x_{1},\ldots ,x_{n}} $$ This statement is illustrated in three dimensions by the tetrahedron in the figure. The "hypotenuse" is the base of the tetrahedron at the back of the figure, and the "legs" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the "legs" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras' theorem applies. In a different wording: Given an n-rectangular n-dimensional simplex, the square of the (n − 1)-content of the facet opposing the right vertex will equal the sum of the squares of the (n − 1)-contents of the remaining facets. The Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis. In an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product ⟨ v , w ⟩ \langle \mathbf {v ,\mathbf {w} \rangle } is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the standard inner product or the Euclidean inner product. However, other inner products are possible. $$ ⟨ v , w ⟩ {\displaystyle \langle \mathbf {v} ,\mathbf {w} \rangle } $$ The concept of length is replaced by the concept of the norm ‖v‖ of a vector v, defined as: $$ ‖ v ‖ ≡ ⟨ v , v ⟩ . {\displaystyle \lVert \mathbf {v} \rVert \equiv {\sqrt {\langle \mathbf {v} ,\mathbf {v} \rangle }}\,.} $$ In an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have $$ ‖ v + w ‖ 2 = ‖ v ‖ 2 + ‖ w ‖ 2 . {\displaystyle \left\|\mathbf {v} +\mathbf {w} \right\|^{2}=\left\|\mathbf {v} \right\|^{2}+\left\|\mathbf {w} \right\|^{2}.} $$ Here the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product: $$ ‖ v + w ‖ 2 = ⟨ v + w , v + w ⟩ = ⟨ v , v ⟩ + ⟨ w , w ⟩ + ⟨ v , w ⟩ + ⟨ w , v ⟩ = ‖ v ‖ 2 + ‖ w ‖ 2 , {\displaystyle {\begin{aligned}\left\|\mathbf {v} +\mathbf {w} \right\|^{2}&=\langle \mathbf {v+w} ,\ \mathbf {v+w} \rangle \\[3mu]&=\langle \mathbf {v} ,\ \mathbf {v} \rangle +\langle \mathbf {w} ,\ \mathbf {w} \rangle +\langle \mathbf {v,\ w} \rangle +\langle \mathbf {w,\ v} \rangle \\[3mu]&=\left\|\mathbf {v} \right\|^{2}+\left\|\mathbf {w} \right\|^{2},\end{aligned}}} $$ where ⟨ v , w ⟩ = ⟨ w , v ⟩ = 0 \langle \mathbf {v,\ w \rangle =\langle \mathbf {w,\ v} \rangle =0} because of orthogonality. $$ ⟨ v , w ⟩ = ⟨ w , v ⟩ = 0 {\displaystyle \langle \mathbf {v,\ w} \rangle =\langle \mathbf {w,\ v} \rangle =0} $$ A further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the parallelogram law: $$ 2 ‖ v ‖ 2 + 2 ‖ w ‖ 2 = ‖ v + w ‖ 2 + ‖ v − w ‖ 2 , {\displaystyle 2\|\mathbf {v} \|^{2}+2\|\mathbf {w} \|^{2}=\|\mathbf {v+w} \|^{2}+\|\mathbf {v-w} \|^{2}\ ,} $$ which says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is ipso facto a norm corresponding to an inner product. The Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v1, v2, ..., vn are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation $$ ‖ ∑ k = 1 n v k ‖ 2 = ∑ k = 1 n ‖ v k ‖ 2 {\displaystyle {\biggl \|}\sum _{k=1}^{n}\mathbf {v} _{k}{\biggr \|}^{2}=\sum _{k=1}^{n}\|\mathbf {v} _{k}\|^{2}} $$ Another generalization of the Pythagorean theorem applies to Lebesgue-measurable sets of objects in any number of dimensions. Specifically, the square of the measure of an m-dimensional set of objects in one or more parallel m-dimensional flats in n-dimensional Euclidean space is equal to the sum of the squares of the measures of the orthogonal projections of the object(s) onto all m-dimensional coordinate subspaces. In mathematical terms: $$ μ m s 2 = ∑ i = 1 x μ 2 m p i {\displaystyle \mu _{ms}^{2}=\sum _{i=1}^{x}\mathbf {\mu ^{2}} _{mp_{i}}} $$ where: $$ μ m {\displaystyle \mu _{m}} $$ $$ s {\displaystyle s} $$ $$ μ m s {\displaystyle \mu _{ms}} $$ $$ p {\displaystyle p} $$ $$ μ m p i {\displaystyle \mu _{mp_{i}}} $$ $$ i {\displaystyle i} $$ $$ x {\displaystyle x} $$ $$ x = ( n m ) = n ! m ! ( n − m ) ! {\displaystyle x={\binom {n}{m}}={\frac {n!}{m!(n-m)!}}} $$ The Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, were the Pythagorean theorem to fail for some right triangle, then the plane in which this triangle is contained cannot be Euclidean. More precisely, the Pythagorean theorem implies, and is implied by, Euclid's Parallel (Fifth) Postulate. Thus, right triangles in a non-Euclidean geometry do not satisfy the Pythagorean theorem. For example, in spherical geometry, all three sides of the right triangle (say a, b, and c) bounding an octant of the unit sphere have length equal to π/2, and all its angles are right angles, which violates the Pythagorean theorem because a 2 + b 2 = 2 c 2 > c 2 a^{2+b^{2}=2c^{2}>c^{2}} . $$ a 2 + b 2 = 2 c 2 > c 2 {\displaystyle a^{2}+b^{2}=2c^{2}>c^{2}} $$ Here two cases of non-Euclidean geometry are considered—spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines. However, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say A+B = C. The sides are then related as follows: the sum of the areas of the circles with diameters a and b equals the area of the circle with diameter c. For any right triangle on a sphere of radius R (for example, if γ in the figure is a right angle), with sides a, b, c, the relation between the sides takes the form: $$ cos ⁡ c R = cos ⁡ a R cos ⁡ b R . {\displaystyle \cos {\frac {c}{R}}=\cos {\frac {a}{R}}\,\cos {\frac {b}{R}}.} $$ This equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles: $$ cos ⁡ c R = cos ⁡ a R cos ⁡ b R + sin ⁡ a R sin ⁡ b R cos ⁡ γ . {\displaystyle \cos {\frac {c}{R}}=\cos {\frac {a}{R}}\,\cos {\frac {b}{R}}+\sin {\frac {a}{R}}\,\sin {\frac {b}{R}}\,\cos {\gamma }.} $$ For infinitesimal triangles on the sphere (or equivalently, for finite spherical triangles on a sphere of infinite radius), the spherical relation between the sides of a right triangle reduces to the Euclidean form of the Pythagorean theorem. To see how, assume we have a spherical triangle of fixed side lengths a, b, and c on a sphere with expanding radius R. As R approaches infinity the quantities a/R, b/R, and c/R tend to zero and the spherical Pythagorean identity reduces to 1 = 1 , 1=1, so we must look at its asymptotic expansion. $$ 1 = 1 , {\displaystyle 1=1,} $$ The Maclaurin series for the cosine function can be written as cos ⁡ x = 1 − 1 2 x 2 + O ( x 4 ) {\textstyle \cos x=1-{\tfrac {1}{2}}x^{2}+O{\left(x^{4}\right)}} with the remainder term in big O notation. Letting x = c / R x=c/R be a side of the triangle, and treating the expression as an asymptotic expansion in terms of R for a fixed c, $$ cos ⁡ x = 1 − 1 2 x 2 + O ( x 4 ) {\textstyle \cos x=1-{\tfrac {1}{2}}x^{2}+O{\left(x^{4}\right)}} $$ $$ x = c / R {\displaystyle x=c/R} $$ $$ cos ⁡ c R = 1 − c 2 2 R 2 + O ( R − 4 ) {\displaystyle {\begin{aligned}\cos {\frac {c}{R}}=1-{\frac {c^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}\end{aligned}}} $$ and likewise for a and b. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields $$ 1 − c 2 2 R 2 + O ( R − 4 ) = ( 1 − a 2 2 R 2 + O ( R − 4 ) ) ( 1 − b 2 2 R 2 + O ( R − 4 ) ) = 1 − a 2 2 R 2 − b 2 2 R 2 + O ( R − 4 ) . {\displaystyle {\begin{aligned}1-{\frac {c^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}&=\left(1-{\frac {a^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}\right)\left(1-{\frac {b^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}\right)\\&=1-{\frac {a^{2}}{2R^{2}}}-{\frac {b^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}.\end{aligned}}} $$ Subtracting 1 and then negating each side, $$ c 2 2 R 2 = a 2 2 R 2 + b 2 2 R 2 + O ( R − 4 ) . {\displaystyle {\frac {c^{2}}{2R^{2}}}={\frac {a^{2}}{2R^{2}}}+{\frac {b^{2}}{2R^{2}}}+O{\left(R^{-4}\right)}.} $$ Multiplying through by 2R2, the asymptotic expansion for c in terms of fixed a, b and variable R is $$ c 2 = a 2 + b 2 + O ( R − 2 ) . {\displaystyle c^{2}=a^{2}+b^{2}+O{\left(R^{-2}\right)}.} $$ The Euclidean Pythagorean relationship c 2 = a 2 + b 2 {\textstyle c^{2}=a^{2}+b^{2}} is recovered in the limit, as the remainder vanishes when the radius R approaches infinity. $$ c 2 = a 2 + b 2 {\textstyle c^{2}=a^{2}+b^{2}} $$ For practical computation in spherical trigonometry with small right triangles, cosines can be replaced with sines using the double-angle identity cos ⁡ 2 θ = 1 − 2 sin 2 ⁡ θ \cos {2\theta =1-2\sin ^{2}{\theta }} to avoid loss of significance. Then the spherical Pythagorean theorem can alternately be written as $$ cos ⁡ 2 θ = 1 − 2 sin 2 ⁡ θ {\displaystyle \cos {2\theta }=1-2\sin ^{2}{\theta }} $$ $$ sin 2 ⁡ c 2 R = sin 2 ⁡ a 2 R + sin 2 ⁡ b 2 R − 2 sin 2 ⁡ a 2 R sin 2 ⁡ b 2 R . {\displaystyle \sin ^{2}{\frac {c}{2R}}=\sin ^{2}{\frac {a}{2R}}+\sin ^{2}{\frac {b}{2R}}-2\sin ^{2}{\frac {a}{2R}}\,\sin ^{2}{\frac {b}{2R}}.} $$ In a hyperbolic space with uniform Gaussian curvature −1/R2, for a right triangle with legs a, b, and hypotenuse c, the relation between the sides takes the form: $$ cosh ⁡ c R = cosh ⁡ a R cosh ⁡ b R {\displaystyle \cosh {\frac {c}{R}}=\cosh {\frac {a}{R}}\,\cosh {\frac {b}{R}}} $$ where cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles: $$ cosh ⁡ c R = cosh ⁡ a R cosh ⁡ b R − sinh ⁡ a R sinh ⁡ b R cos ⁡ γ , {\displaystyle \cosh {\frac {c}{R}}=\cosh {\frac {a}{R}}\ \cosh {\frac {b}{R}}-\sinh {\frac {a}{R}}\ \sinh {\frac {b}{R}}\ \cos \gamma \ ,} $$ with γ the angle at the vertex opposite the side c. By using the Maclaurin series for the hyperbolic cosine, cosh x ≈ 1 + x2/2, it can be shown that as a hyperbolic triangle becomes very small (that is, as a, b, and c all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras' theorem. For small right triangles (a, b << R), the hyperbolic cosines can be eliminated to avoid loss of significance, giving $$ sinh 2 ⁡ c 2 R = sinh 2 ⁡ a 2 R + sinh 2 ⁡ b 2 R + 2 sinh 2 ⁡ a 2 R sinh 2 ⁡ b 2 R . {\displaystyle \sinh ^{2}{\frac {c}{2R}}=\sinh ^{2}{\frac {a}{2R}}+\sinh ^{2}{\frac {b}{2R}}+2\sinh ^{2}{\frac {a}{2R}}\sinh ^{2}{\frac {b}{2R}}\,.} $$ For any uniform curvature K (positive, zero, or negative), in very small right triangles (|K|a2, |K|b2 << 1) with hypotenuse c, it can be shown that $$ c 2 = a 2 + b 2 − K 3 a 2 b 2 − K 2 45 a 2 b 2 ( a 2 + b 2 ) − 2 K 3 945 a 2 b 2 ( a 2 − b 2 ) 2 + O ( K 4 c 10 ) . {\displaystyle c^{2}=a^{2}+b^{2}-{\frac {K}{3}}a^{2}b^{2}-{\frac {K^{2}}{45}}a^{2}b^{2}(a^{2}+b^{2})-{\frac {2K^{3}}{945}}a^{2}b^{2}(a^{2}-b^{2})^{2}+O(K^{4}c^{10})\,.} $$ The Pythagorean theorem applies to infinitesimal triangles seen in differential geometry. In three dimensional space, the distance between two infinitesimally separated points satisfies $$ d s 2 = d x 2 + d y 2 + d z 2 , {\displaystyle ds^{2}=dx^{2}+dy^{2}+dz^{2},} $$ with ds the element of distance and (dx, dy, dz) the components of the vector separating the two points. Such a space is called a Euclidean space. However, in Riemannian geometry, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form: $$ d s 2 = ∑ i , j n g i j d x i d x j {\displaystyle ds^{2}=\sum _{i,j}^{n}g_{ij}\,dx_{i}\,dx_{j}} $$ which is called the metric tensor. (Sometimes, by abuse of language, the same term is applied to the set of coefficients gij.) It may be a function of position, and often describes curved space. A simple example is Euclidean (flat) space expressed in curvilinear coordinates. For example, in polar coordinates: $$ d s 2 = d r 2 + r 2 d θ 2 . {\displaystyle ds^{2}=dr^{2}+r^{2}d\theta ^{2}\ .} $$ There is debate whether the Pythagorean theorem was discovered once, or many times in many places, and the date of first discovery is uncertain, as is the date of the first proof. Historians of Mesopotamian mathematics have concluded that the Pythagorean rule was in widespread use during the Old Babylonian period (20th to 16th centuries BC), over a thousand years before Pythagoras was born. The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system. Written c. 1800 BC, the Egyptian Middle Kingdom Berlin Papyrus 6619 includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet Plimpton 322, written near Larsa also c. 1800 BC, contains many entries closely related to Pythagorean triples. In India, the Baudhayana Shulba Sutra, the dates of which are given variously as between the 8th and 5th century BC, contains a list of Pythagorean triples and a statement of the Pythagorean theorem, both in the special case of the isosceles right triangle and in the general case, as does the Apastamba Shulba Sutra (c. 600 BC).[a] Byzantine Neoplatonic philosopher and mathematician Proclus, writing in the fifth century AD, states two arithmetic rules, "one of them attributed to Plato, the other to Pythagoras", for generating special Pythagorean triples. The rule attributed to Pythagoras (c. 570 – c. 495 BC) starts from an odd number and produces a triple with leg and hypotenuse differing by one unit; the rule attributed to Plato (428/427 or 424/423 – 348/347 BC) starts from an even number and produces a triple with leg and hypotenuse differing by two units. According to Thomas L. Heath (1861–1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. Classicist Kurt von Fritz wrote, "Whether this formula is rightly attributed to Pythagoras personally ... one can safely assume that it belongs to the very oldest period of Pythagorean mathematics." Around 300 BC, in Euclid's Elements, the oldest extant axiomatic proof of the theorem is presented. With contents known much earlier, but in surviving texts dating from roughly the 1st century BC, the Chinese text Zhoubi Suanjing (周髀算经), (The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven) gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle — in China it is called the "Gougu theorem" (勾股定理). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in The Nine Chapters on the Mathematical Art, together with a mention of right triangles. Some believe the theorem arose first in China in the 11th century BC, where it is alternatively known as the "Shang Gao theorem" (商高定理), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the Zhoubi Suanjing.

End of content from: https://en.wikipedia.org/wiki/Pythagorean_theorem

--------------------------------------------------------------------------------

Content from: https://en.wikipedia.org/wiki/Fourier_transform

In physics, engineering and mathematics, the Fourier transform (FT) is an integral transform that takes a function as input and outputs another function that describes the extent to which various frequencies are present in the original function. The output of the transform is a complex-valued function of frequency. The term Fourier transform refers to both this complex-valued function and the mathematical operation. When a distinction needs to be made, the output of the operation is sometimes called the frequency domain representation of the original function. The Fourier transform is analogous to decomposing the sound of a musical chord into the intensities of its constituent pitches. Functions that are localized in the time domain have Fourier transforms that are spread out across the frequency domain and vice versa, a phenomenon known as the uncertainty principle. The critical case for this principle is the Gaussian function, of substantial importance in probability theory and statistics as well as in the study of physical phenomena exhibiting normal distribution (e.g., diffusion). The Fourier transform of a Gaussian function is another Gaussian function. Joseph Fourier introduced sine and cosine transforms (which correspond to the imaginary and real components of the modern Fourier transform) in his study of heat transfer, where Gaussian functions appear as solutions of the heat equation. The Fourier transform can be formally defined as an improper Riemann integral, making it an integral transform, although this definition is not suitable for many applications requiring a more sophisticated integration theory.[note 1] For example, many relatively simple applications use the Dirac delta function, which can be treated formally as if it were a function, but the justification requires a mathematically more sophisticated viewpoint.[note 2] The Fourier transform can also be generalized to functions of several variables on Euclidean space, sending a function of 3-dimensional 'position space' to a function of 3-dimensional momentum (or a function of space and time to a function of 4-momentum). This idea makes the spatial Fourier transform very natural in the study of waves, as well as in quantum mechanics, where it is important to be able to represent wave solutions as functions of either position or momentum and sometimes both. In general, functions to which Fourier methods are applicable are complex-valued, and possibly vector-valued.[note 3] Still further generalization is possible to functions on groups, which, besides the original Fourier transform on R or Rn, notably includes the discrete-time Fourier transform (DTFT, group = Z), the discrete Fourier transform (DFT, group = Z mod N) and the Fourier series or circular Fourier transform (group = S1, the unit circle ≈ closed finite interval with endpoints identified). The latter is routinely employed to handle periodic functions. The fast Fourier transform (FFT) is an algorithm for computing the DFT. The Fourier transform is an analysis process, decomposing a complex-valued function f ( x ) \textstyle f(x) into its constituent frequencies and their amplitudes. The inverse process is synthesis, which recreates f ( x ) \textstyle f(x) from its transform. $$ f ( x ) {\displaystyle \textstyle f(x)} $$ $$ f ( x ) {\displaystyle \textstyle f(x)} $$ We can start with an analogy, the Fourier series, which analyzes f ( x ) \textstyle f(x) on a bounded interval x ∈ [ − P / 2 , P / 2 ] , \textstyle x\in [-P/2,P/2], for some positive real number P . P. The constituent frequencies are a discrete set of harmonics at frequencies n P , n ∈ Z , {\tfrac {n{P}},n\in \mathbb {Z} ,} whose amplitude and phase are given by the analysis formula: c n = 1 P ∫ − P / 2 P / 2 f ( x ) e − i 2 π n P x d x . c_{n={\frac {1}{P}}\int _{-P/2}^{P/2}f(x)\,e^{-i2\pi {\frac {n}{P}}x}\,dx.} The actual Fourier series is the synthesis formula: f ( x ) = ∑ n = − ∞ ∞ c n e i 2 π n P x , x ∈ [ − P / 2 , P / 2 ] . f(x)=\sum _{n=-\infty ^{\infty }c_{n}\,e^{i2\pi {\tfrac {n}{P}}x},\quad \textstyle x\in [-P/2,P/2].} On an unbounded interval, P → ∞ , P\to \infty , the constituent frequencies are a continuum: n P → ξ ∈ R , {\tfrac {n{P}}\to \xi \in \mathbb {R} ,} and c n c_{n} is replaced by a function: $$ f ( x ) {\displaystyle \textstyle f(x)} $$ $$ x ∈ [ − P / 2 , P / 2 ] , {\displaystyle \textstyle x\in [-P/2,P/2],} $$ $$ P . {\displaystyle P.} $$ $$ n P , n ∈ Z , {\displaystyle {\tfrac {n}{P}},n\in \mathbb {Z} ,} $$ $$ c n = 1 P ∫ − P / 2 P / 2 f ( x ) e − i 2 π n P x d x . {\displaystyle c_{n}={\frac {1}{P}}\int _{-P/2}^{P/2}f(x)\,e^{-i2\pi {\frac {n}{P}}x}\,dx.} $$ $$ f ( x ) = ∑ n = − ∞ ∞ c n e i 2 π n P x , x ∈ [ − P / 2 , P / 2 ] . {\displaystyle f(x)=\sum _{n=-\infty }^{\infty }c_{n}\,e^{i2\pi {\tfrac {n}{P}}x},\quad \textstyle x\in [-P/2,P/2].} $$ $$ P → ∞ , {\displaystyle P\to \infty ,} $$ $$ n P → ξ ∈ R , {\displaystyle {\tfrac {n}{P}}\to \xi \in \mathbb {R} ,} $$ $$ c n {\displaystyle c_{n}} $$ f ^ ( ξ ) = ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x . {\widehat {f}(\xi )=\int _{-\infty }^{\infty }f(x)\ e^{-i2\pi \xi x}\,dx.} $$ f ^ ( ξ ) = ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x . {\displaystyle {\widehat {f}}(\xi )=\int _{-\infty }^{\infty }f(x)\ e^{-i2\pi \xi x}\,dx.} $$ Evaluating Eq.1 for all values of ξ \xi produces the frequency-domain function. The integral can diverge at some frequencies. (see § Fourier transform for periodic functions) But it converges for all frequencies when f ( x ) f(x) decays with all derivatives as x → ± ∞ x\to \pm \infty : lim x → ∞ f ( n ) ( x ) = 0 , n = 0 , 1 , 2 , … \lim _{x\to \infty f^{(n)}(x)=0,n=0,1,2,\dots } . (See Schwartz function). By the Riemann–Lebesgue lemma, the transformed function f ^ {\widehat {f}} also decays with all derivatives. $$ ξ {\displaystyle \xi } $$ $$ f ( x ) {\displaystyle f(x)} $$ $$ x → ± ∞ {\displaystyle x\to \pm \infty } $$ $$ lim x → ∞ f ( n ) ( x ) = 0 , n = 0 , 1 , 2 , … {\displaystyle \lim _{x\to \infty }f^{(n)}(x)=0,n=0,1,2,\dots } $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ The complex number f ^ ( ξ ) {\widehat {f}(\xi )} , in polar coordinates, conveys both amplitude and phase of frequency ξ . \xi . The intuitive interpretation of Eq.1 is that the effect of multiplying f ( x ) f(x) by e − i 2 π ξ x e^{-i2\pi \xi x} is to subtract ξ \xi from every frequency component of function f ( x ) . f(x). [note 4] Only the component that was at frequency ξ \xi can produce a non-zero value of the infinite integral, because (at least formally) all the other shifted components are oscillatory and integrate to zero. (see § Example) $$ f ^ ( ξ ) {\displaystyle {\widehat {f}}(\xi )} $$ $$ ξ . {\displaystyle \xi .} $$ $$ f ( x ) {\displaystyle f(x)} $$ $$ e − i 2 π ξ x {\displaystyle e^{-i2\pi \xi x}} $$ $$ ξ {\displaystyle \xi } $$ $$ f ( x ) . {\displaystyle f(x).} $$ $$ ξ {\displaystyle \xi } $$ The corresponding synthesis formula is: f ( x ) = ∫ − ∞ ∞ f ^ ( ξ ) e i 2 π ξ x d ξ , ∀ x ∈ R . f(x)=\int _{-\infty ^{\infty }{\widehat {f}}(\xi )\ e^{i2\pi \xi x}\,d\xi ,\quad \forall \ x\in \mathbb {R} .} $$ f ( x ) = ∫ − ∞ ∞ f ^ ( ξ ) e i 2 π ξ x d ξ , ∀ x ∈ R . {\displaystyle f(x)=\int _{-\infty }^{\infty }{\widehat {f}}(\xi )\ e^{i2\pi \xi x}\,d\xi ,\quad \forall \ x\in \mathbb {R} .} $$ Eq.2 is a representation of f ( x ) f(x) as a weighted summation of complex exponential functions. $$ f ( x ) {\displaystyle f(x)} $$ This is also known as the Fourier inversion theorem, and was first introduced in Fourier's Analytical Theory of Heat. The functions f f and f ^ {\widehat {f}} are referred to as a Fourier transform pair. A common notation for designating transform pairs is: f ( x ) ⟷ F f ^ ( ξ ) , f(x)\ {\stackrel {\mathcal {F}{\longleftrightarrow }}\ {\widehat {f}}(\xi ),} for example rect ⁡ ( x ) ⟷ F sinc ⁡ ( ξ ) . \operatorname {rect (x)\ {\stackrel {\mathcal {F}}{\longleftrightarrow }}\ \operatorname {sinc} (\xi ).} $$ f {\displaystyle f} $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ $$ f ( x ) ⟷ F f ^ ( ξ ) , {\displaystyle f(x)\ {\stackrel {\mathcal {F}}{\longleftrightarrow }}\ {\widehat {f}}(\xi ),} $$ $$ rect ⁡ ( x ) ⟷ F sinc ⁡ ( ξ ) . {\displaystyle \operatorname {rect} (x)\ {\stackrel {\mathcal {F}}{\longleftrightarrow }}\ \operatorname {sinc} (\xi ).} $$ Until now, we have been dealing with Schwartz functions, which decay rapidly at infinity, with all derivatives. This excludes many functions of practical importance from the definition, such as the rect function. A measurable function f : R → C f:\mathbb {R \to \mathbb {C} } is called (Lebesgue) integrable if the Lebesgue integral of its absolute value is finite: ‖ f ‖ 1 = ∫ R | f ( x ) | d x < ∞ . \|f\|_{1=\int _{\mathbb {R} }|f(x)|\,dx<\infty .} Two measurable functions are equivalent if they are equal except on a set of measure zero. The set of all equivalence classes of integrable functions is denoted L 1 ( R ) L^{1(\mathbb {R} )} . Then: $$ f : R → C {\displaystyle f:\mathbb {R} \to \mathbb {C} } $$ $$ ‖ f ‖ 1 = ∫ R | f ( x ) | d x < ∞ . {\displaystyle \|f\|_{1}=\int _{\mathbb {R} }|f(x)|\,dx<\infty .} $$ $$ L 1 ( R ) {\displaystyle L^{1}(\mathbb {R} )} $$ Definition — The Fourier transform of a Lebesgue integrable function f ∈ L 1 ( R ) f\in L^{1(\mathbb {R} )} is defined by the formula Eq.1. $$ f ∈ L 1 ( R ) {\displaystyle f\in L^{1}(\mathbb {R} )} $$ The integral Eq.1 is well-defined for all ξ ∈ R , \xi \in \mathbb {R ,} because of the assumption ‖ f ‖ 1 < ∞ \|f\|_{1<\infty } . (It can be shown that the function f ^ ∈ L ∞ ∩ C ( R ) {\widehat {f}\in L^{\infty }\cap C(\mathbb {R} )} is bounded and uniformly continuous in the frequency domain, and moreover, by the Riemann–Lebesgue lemma, it is zero at infinity.) $$ ξ ∈ R , {\displaystyle \xi \in \mathbb {R} ,} $$ $$ ‖ f ‖ 1 < ∞ {\displaystyle \|f\|_{1}<\infty } $$ $$ f ^ ∈ L ∞ ∩ C ( R ) {\displaystyle {\widehat {f}}\in L^{\infty }\cap C(\mathbb {R} )} $$ However, the class of Lebesgue integrable functions is not ideal from the point of view of the Fourier transform because there is no easy characterization of the image, and thus no easy characterization of the inverse transform. While Eq.1 defines the Fourier transform for (complex-valued) functions in L 1 ( R ) L^{1(\mathbb {R} )} , it is easy to see that it is not well-defined for other integrability classes, most importantly L 2 ( R ) L^{2(\mathbb {R} )} . For functions in L 1 ∩ L 2 ( R ) L^{1\cap L^{2}(\mathbb {R} )} , and with the conventions of Eq.1, the Fourier transform is a unitary operator with respect to the Hilbert inner product on L 2 ( R ) L^{2(\mathbb {R} )} , restricted to the dense subspace of integrable functions. Therefore, it admits a unique continuous extension to a unitary operator on L 2 ( R ) L^{2(\mathbb {R} )} , also called the Fourier transform. This extension is important in part because the Fourier transform preserves the space L 2 ( R ) L^{2(\mathbb {R} )} so that, unlike the case of L 1 L^{1} , the Fourier transform and inverse transform are on the same footing, being transformations of the same space of functions to itself. $$ L 1 ( R ) {\displaystyle L^{1}(\mathbb {R} )} $$ $$ L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} $$ $$ L 1 ∩ L 2 ( R ) {\displaystyle L^{1}\cap L^{2}(\mathbb {R} )} $$ $$ L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} $$ $$ L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} $$ $$ L 2 ( R ) {\displaystyle L^{2}(\mathbb {R} )} $$ $$ L 1 {\displaystyle L^{1}} $$ Importantly, for functions in L 2 L^{2} , the Fourier transform is no longer given by Eq.1 (interpreted as a Lebesgue integral). For example, the function f ( x ) = ( 1 + x 2 ) − 1 / 2 f(x)=(1+x^{2)^{-1/2}} is in L 2 L^{2} but not L 1 L^{1} , so the integral Eq.1 diverges. In such cases, the Fourier transform can be obtained explicitly by regularizing the integral, and then passing to a limit. In practice, the integral is often regarded as an improper integral instead of a proper Lebesgue integral, but sometimes for convergence one needs to use weak limit or principal value instead of the (pointwise) limits implicit in an improper integral. Titchmarsh (1986) and Dym & McKean (1985) each gives three rigorous ways of extending the Fourier transform to square integrable functions using this procedure. $$ L 2 {\displaystyle L^{2}} $$ $$ f ( x ) = ( 1 + x 2 ) − 1 / 2 {\displaystyle f(x)=(1+x^{2})^{-1/2}} $$ $$ L 2 {\displaystyle L^{2}} $$ $$ L 1 {\displaystyle L^{1}} $$ The conventions chosen in this article are those of harmonic analysis, and are characterized as the unique conventions such that the Fourier transform is both unitary on L2 and an algebra homomorphism from L1 to L∞, without renormalizing the Lebesgue measure. When the independent variable ( x x ) represents time (often denoted by t t ), the transform variable ( ξ \xi ) represents frequency (often denoted by f f ). For example, if time is measured in seconds, then frequency is in hertz. The Fourier transform can also be written in terms of angular frequency, ω = 2 π ξ , \omega =2\pi \xi , whose units are radians per second. $$ x {\displaystyle x} $$ $$ t {\displaystyle t} $$ $$ ξ {\displaystyle \xi } $$ $$ f {\displaystyle f} $$ $$ ω = 2 π ξ , {\displaystyle \omega =2\pi \xi ,} $$ The substitution ξ = ω 2 π \xi ={\tfrac {\omega {2\pi }}} into Eq.1 produces this convention, where function f ^ {\widehat {f}} is relabeled f 1 ^ : {\widehat {f_{1}}:} f 3 ^ ( ω ) ≜ ∫ − ∞ ∞ f ( x ) ⋅ e − i ω x d x = f 1 ^ ( ω 2 π ) , f ( x ) = 1 2 π ∫ − ∞ ∞ f 3 ^ ( ω ) ⋅ e i ω x d ω . {\begin{aligned{\widehat {f_{3}}}(\omega )&\triangleq \int _{-\infty }^{\infty }f(x)\cdot e^{-i\omega \,x}\,dx={\widehat {f_{1}}}\left({\tfrac {\omega }{2\pi }}\right),\\f(x)&={\frac {1}{2\pi }}\int _{-\infty }^{\infty }{\widehat {f_{3}}}(\omega )\cdot e^{i\omega \,x}\,d\omega .\end{aligned}}} Unlike the Eq.1 definition, the Fourier transform is no longer a unitary transformation, and there is less symmetry between the formulas for the transform and its inverse. Those properties are restored by splitting the 2 π 2\pi factor evenly between the transform and its inverse, which leads to another convention: f 2 ^ ( ω ) ≜ 1 2 π ∫ − ∞ ∞ f ( x ) ⋅ e − i ω x d x = 1 2 π ⋅ f 1 ^ ( ω 2 π ) , f ( x ) = 1 2 π ∫ − ∞ ∞ f 2 ^ ( ω ) ⋅ e i ω x d ω . {\begin{aligned{\widehat {f_{2}}}(\omega )&\triangleq {\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }f(x)\cdot e^{-i\omega x}\,dx={\frac {1}{\sqrt {2\pi }}}\cdot {\widehat {f_{1}}}\left({\tfrac {\omega }{2\pi }}\right),\\f(x)&={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }{\widehat {f_{2}}}(\omega )\cdot e^{i\omega x}\,d\omega .\end{aligned}}} Variations of all three conventions can be created by conjugating the complex-exponential kernel of both the forward and the reverse transform. The signs must be opposites. $$ ξ = ω 2 π {\displaystyle \xi ={\tfrac {\omega }{2\pi }}} $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ $$ f 1 ^ : {\displaystyle {\widehat {f_{1}}}:} $$ $$ f 3 ^ ( ω ) ≜ ∫ − ∞ ∞ f ( x ) ⋅ e − i ω x d x = f 1 ^ ( ω 2 π ) , f ( x ) = 1 2 π ∫ − ∞ ∞ f 3 ^ ( ω ) ⋅ e i ω x d ω . {\displaystyle {\begin{aligned}{\widehat {f_{3}}}(\omega )&\triangleq \int _{-\infty }^{\infty }f(x)\cdot e^{-i\omega \,x}\,dx={\widehat {f_{1}}}\left({\tfrac {\omega }{2\pi }}\right),\\f(x)&={\frac {1}{2\pi }}\int _{-\infty }^{\infty }{\widehat {f_{3}}}(\omega )\cdot e^{i\omega \,x}\,d\omega .\end{aligned}}} $$ $$ 2 π {\displaystyle 2\pi } $$ $$ f 2 ^ ( ω ) ≜ 1 2 π ∫ − ∞ ∞ f ( x ) ⋅ e − i ω x d x = 1 2 π ⋅ f 1 ^ ( ω 2 π ) , f ( x ) = 1 2 π ∫ − ∞ ∞ f 2 ^ ( ω ) ⋅ e i ω x d ω . {\displaystyle {\begin{aligned}{\widehat {f_{2}}}(\omega )&\triangleq {\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }f(x)\cdot e^{-i\omega x}\,dx={\frac {1}{\sqrt {2\pi }}}\cdot {\widehat {f_{1}}}\left({\tfrac {\omega }{2\pi }}\right),\\f(x)&={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }{\widehat {f_{2}}}(\omega )\cdot e^{i\omega x}\,d\omega .\end{aligned}}} $$ $$ f 1 ^ ( ξ ) ≜ ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x = 2 π f 2 ^ ( 2 π ξ ) = f 3 ^ ( 2 π ξ ) f ( x ) = ∫ − ∞ ∞ f 1 ^ ( ξ ) e i 2 π x ξ d ξ {\displaystyle {\begin{aligned}{\widehat {f_{1}}}(\xi )\ &\triangleq \ \int _{-\infty }^{\infty }f(x)\,e^{-i2\pi \xi x}\,dx={\sqrt {2\pi }}\ \ {\widehat {f_{2}}}(2\pi \xi )={\widehat {f_{3}}}(2\pi \xi )\\f(x)&=\int _{-\infty }^{\infty }{\widehat {f_{1}}}(\xi )\,e^{i2\pi x\xi }\,d\xi \end{aligned}}} $$ $$ f 2 ^ ( ω ) ≜ 1 2 π ∫ − ∞ ∞ f ( x ) e − i ω x d x = 1 2 π f 1 ^ ( ω 2 π ) = 1 2 π f 3 ^ ( ω ) f ( x ) = 1 2 π ∫ − ∞ ∞ f 2 ^ ( ω ) e i ω x d ω {\displaystyle {\begin{aligned}{\widehat {f_{2}}}(\omega )\ &\triangleq \ {\frac {1}{\sqrt {2\pi }}}\ \int _{-\infty }^{\infty }f(x)\,e^{-i\omega x}\,dx={\frac {1}{\sqrt {2\pi }}}\ \ {\widehat {f_{1}}}\!\left({\frac {\omega }{2\pi }}\right)={\frac {1}{\sqrt {2\pi }}}\ \ {\widehat {f_{3}}}(\omega )\\f(x)&={\frac {1}{\sqrt {2\pi }}}\ \int _{-\infty }^{\infty }{\widehat {f_{2}}}(\omega )\,e^{i\omega x}\,d\omega \end{aligned}}} $$ $$ f 3 ^ ( ω ) ≜ ∫ − ∞ ∞ f ( x ) e − i ω x d x = f 1 ^ ( ω 2 π ) = 2 π f 2 ^ ( ω ) f ( x ) = 1 2 π ∫ − ∞ ∞ f 3 ^ ( ω ) e i ω x d ω {\displaystyle {\begin{aligned}{\widehat {f_{3}}}(\omega )\ &\triangleq \ \int _{-\infty }^{\infty }f(x)\,e^{-i\omega x}\,dx={\widehat {f_{1}}}\left({\frac {\omega }{2\pi }}\right)={\sqrt {2\pi }}\ \ {\widehat {f_{2}}}(\omega )\\f(x)&={\frac {1}{2\pi }}\int _{-\infty }^{\infty }{\widehat {f_{3}}}(\omega )\,e^{i\omega x}\,d\omega \end{aligned}}} $$ $$ f 1 ^ ( ξ ) ≜ ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x = ( 2 π ) n 2 f 2 ^ ( 2 π ξ ) = f 3 ^ ( 2 π ξ ) f ( x ) = ∫ R n f 1 ^ ( ξ ) e i 2 π ξ ⋅ x d ξ {\displaystyle {\begin{aligned}{\widehat {f_{1}}}(\xi )\ &\triangleq \ \int _{\mathbb {R} ^{n}}f(x)e^{-i2\pi \xi \cdot x}\,dx=(2\pi )^{\frac {n}{2}}{\widehat {f_{2}}}(2\pi \xi )={\widehat {f_{3}}}(2\pi \xi )\\f(x)&=\int _{\mathbb {R} ^{n}}{\widehat {f_{1}}}(\xi )e^{i2\pi \xi \cdot x}\,d\xi \end{aligned}}} $$ $$ f 2 ^ ( ω ) ≜ 1 ( 2 π ) n 2 ∫ R n f ( x ) e − i ω ⋅ x d x = 1 ( 2 π ) n 2 f 1 ^ ( ω 2 π ) = 1 ( 2 π ) n 2 f 3 ^ ( ω ) f ( x ) = 1 ( 2 π ) n 2 ∫ R n f 2 ^ ( ω ) e i ω ⋅ x d ω {\displaystyle {\begin{aligned}{\widehat {f_{2}}}(\omega )\ &\triangleq \ {\frac {1}{(2\pi )^{\frac {n}{2}}}}\int _{\mathbb {R} ^{n}}f(x)e^{-i\omega \cdot x}\,dx={\frac {1}{(2\pi )^{\frac {n}{2}}}}{\widehat {f_{1}}}\!\left({\frac {\omega }{2\pi }}\right)={\frac {1}{(2\pi )^{\frac {n}{2}}}}{\widehat {f_{3}}}(\omega )\\f(x)&={\frac {1}{(2\pi )^{\frac {n}{2}}}}\int _{\mathbb {R} ^{n}}{\widehat {f_{2}}}(\omega )e^{i\omega \cdot x}\,d\omega \end{aligned}}} $$ $$ f 3 ^ ( ω ) ≜ ∫ R n f ( x ) e − i ω ⋅ x d x = f 1 ^ ( ω 2 π ) = ( 2 π ) n 2 f 2 ^ ( ω ) f ( x ) = 1 ( 2 π ) n ∫ R n f 3 ^ ( ω ) e i ω ⋅ x d ω {\displaystyle {\begin{aligned}{\widehat {f_{3}}}(\omega )\ &\triangleq \ \int _{\mathbb {R} ^{n}}f(x)e^{-i\omega \cdot x}\,dx={\widehat {f_{1}}}\left({\frac {\omega }{2\pi }}\right)=(2\pi )^{\frac {n}{2}}{\widehat {f_{2}}}(\omega )\\f(x)&={\frac {1}{(2\pi )^{n}}}\int _{\mathbb {R} ^{n}}{\widehat {f_{3}}}(\omega )e^{i\omega \cdot x}\,d\omega \end{aligned}}} $$ For 1 < p < 2 1<p<2 , the Fourier transform can be defined on L p ( R ) L^{p(\mathbb {R} )} by Marcinkiewicz interpolation. $$ 1 < p < 2 {\displaystyle 1<p<2} $$ $$ L p ( R ) {\displaystyle L^{p}(\mathbb {R} )} $$ The Fourier transform can be defined on domains other than the real line. The Fourier transform on Euclidean space and the Fourier transform on locally abelian groups are discussed later in the article. The Fourier transform can also be defined for tempered distributions, dual to the space of rapidly decreasing functions (Schwartz functions). A Schwartz function is a smooth function that decays at infinity, along with all of its derivatives. The space of Schwartz functions is denoted by S ( R ) {\mathcal {S}(\mathbb {R} )} , and its dual S ′ ( R ) {\mathcal {S}'(\mathbb {R} )} is the space of tempered distributions. It is easy to see, by differentiating under the integral and applying the Riemann-Lebesgue lemma, that the Fourier transform of a Schwartz function (defined by the formula Eq.1) is again a Schwartz function. The Fourier transform of a tempered distribution T ∈ S ′ ( R ) T\in {\mathcal {S}'(\mathbb {R} )} is defined by duality: ⟨ T ^ , ϕ ⟩ = ⟨ T , ϕ ^ ⟩ ; ∀ ϕ ∈ S ( R ) . \langle {\widehat {T},\phi \rangle =\langle T,{\widehat {\phi }}\rangle ;\quad \forall \phi \in {\mathcal {S}}(\mathbb {R} ).} $$ S ( R ) {\displaystyle {\mathcal {S}}(\mathbb {R} )} $$ $$ S ′ ( R ) {\displaystyle {\mathcal {S}}'(\mathbb {R} )} $$ $$ T ∈ S ′ ( R ) {\displaystyle T\in {\mathcal {S}}'(\mathbb {R} )} $$ $$ ⟨ T ^ , ϕ ⟩ = ⟨ T , ϕ ^ ⟩ ; ∀ ϕ ∈ S ( R ) . {\displaystyle \langle {\widehat {T}},\phi \rangle =\langle T,{\widehat {\phi }}\rangle ;\quad \forall \phi \in {\mathcal {S}}(\mathbb {R} ).} $$ Many other characterizations of the Fourier transform exist. For example, one uses the Stone–von Neumann theorem: the Fourier transform is the unique unitary intertwiner for the symplectic and Euclidean Schrödinger representations of the Heisenberg group. In 1822, Fourier claimed (see Joseph Fourier § The Analytic Theory of Heat) that any function, whether continuous or discontinuous, can be expanded into a series of sines. That important work was corrected and expanded upon by others to provide the foundation for the various forms of the Fourier transform used since. $$ A ⋅ e i 2 π ξ t {\displaystyle A\cdot e^{i2\pi \xi t}} $$ $$ y ( t ) {\displaystyle y(t)} $$ In general, the coefficients f ^ ( ξ ) {\widehat {f}(\xi )} are complex numbers, which have two equivalent forms (see Euler's formula): f ^ ( ξ ) = A e i θ ⏟ polar coordinate form = A cos ⁡ ( θ ) + i A sin ⁡ ( θ ) ⏟ rectangular coordinate form . {\widehat {f}(\xi )=\underbrace {Ae^{i\theta }} _{\text{polar coordinate form}}=\underbrace {A\cos(\theta )+iA\sin(\theta )} _{\text{rectangular coordinate form}}.} $$ f ^ ( ξ ) {\displaystyle {\widehat {f}}(\xi )} $$ $$ f ^ ( ξ ) = A e i θ ⏟ polar coordinate form = A cos ⁡ ( θ ) + i A sin ⁡ ( θ ) ⏟ rectangular coordinate form . {\displaystyle {\widehat {f}}(\xi )=\underbrace {Ae^{i\theta }} _{\text{polar coordinate form}}=\underbrace {A\cos(\theta )+iA\sin(\theta )} _{\text{rectangular coordinate form}}.} $$ The product with e i 2 π ξ x e^{i2\pi \xi x} (Eq.2) has these forms: f ^ ( ξ ) ⋅ e i 2 π ξ x = A e i θ ⋅ e i 2 π ξ x = A e i ( 2 π ξ x + θ ) ⏟ polar coordinate form = A cos ⁡ ( 2 π ξ x + θ ) + i A sin ⁡ ( 2 π ξ x + θ ) ⏟ rectangular coordinate form . {\begin{aligned{\widehat {f}}(\xi )\cdot e^{i2\pi \xi x}&=Ae^{i\theta }\cdot e^{i2\pi \xi x}\\&=\underbrace {Ae^{i(2\pi \xi x+\theta )}} _{\text{polar coordinate form}}\\&=\underbrace {A\cos(2\pi \xi x+\theta )+iA\sin(2\pi \xi x+\theta )} _{\text{rectangular coordinate form}}.\end{aligned}}} $$ e i 2 π ξ x {\displaystyle e^{i2\pi \xi x}} $$ $$ f ^ ( ξ ) ⋅ e i 2 π ξ x = A e i θ ⋅ e i 2 π ξ x = A e i ( 2 π ξ x + θ ) ⏟ polar coordinate form = A cos ⁡ ( 2 π ξ x + θ ) + i A sin ⁡ ( 2 π ξ x + θ ) ⏟ rectangular coordinate form . {\displaystyle {\begin{aligned}{\widehat {f}}(\xi )\cdot e^{i2\pi \xi x}&=Ae^{i\theta }\cdot e^{i2\pi \xi x}\\&=\underbrace {Ae^{i(2\pi \xi x+\theta )}} _{\text{polar coordinate form}}\\&=\underbrace {A\cos(2\pi \xi x+\theta )+iA\sin(2\pi \xi x+\theta )} _{\text{rectangular coordinate form}}.\end{aligned}}} $$ It is noteworthy how easily the product was simplified using the polar form, and how easily the rectangular form was deduced by an application of Euler's formula. Euler's formula introduces the possibility of negative ξ . \xi . And Eq.1 is defined ∀ ξ ∈ R . \forall \xi \in \mathbb {R .} Only certain complex-valued f ( x ) f(x) have transforms f ^ = 0 , ∀ ξ < 0 {\widehat {f}=0,\ \forall \ \xi <0} (See Analytic signal. A simple example is e i 2 π ξ 0 x ( ξ 0 > 0 ) . e^{i2\pi \xi _{0x}\ (\xi _{0}>0).} ) But negative frequency is necessary to characterize all other complex-valued f ( x ) , f(x), found in signal processing, partial differential equations, radar, nonlinear optics, quantum mechanics, and others. $$ ξ . {\displaystyle \xi .} $$ $$ ∀ ξ ∈ R . {\displaystyle \forall \xi \in \mathbb {R} .} $$ $$ f ( x ) {\displaystyle f(x)} $$ $$ f ^ = 0 , ∀ ξ < 0 {\displaystyle {\widehat {f}}=0,\ \forall \ \xi <0} $$ $$ e i 2 π ξ 0 x ( ξ 0 > 0 ) . {\displaystyle e^{i2\pi \xi _{0}x}\ (\xi _{0}>0).} $$ $$ f ( x ) , {\displaystyle f(x),} $$ For a real-valued f ( x ) , f(x), Eq.1 has the symmetry property f ^ ( − ξ ) = f ^ ∗ ( ξ ) {\widehat {f}(-\xi )={\widehat {f}}^{*}(\xi )} (see § Conjugation below). This redundancy enables Eq.2 to distinguish f ( x ) = cos ⁡ ( 2 π ξ 0 x ) f(x)=\cos(2\pi \xi _{0x)} from e i 2 π ξ 0 x . e^{i2\pi \xi _{0x}.} But of course it cannot tell us the actual sign of ξ 0 , \xi _{0,} because cos ⁡ ( 2 π ξ 0 x ) \cos(2\pi \xi _{0x)} and cos ⁡ ( 2 π ( − ξ 0 ) x ) \cos(2\pi (-\xi _{0)x)} are indistinguishable on just the real numbers line. $$ f ( x ) , {\displaystyle f(x),} $$ $$ f ^ ( − ξ ) = f ^ ∗ ( ξ ) {\displaystyle {\widehat {f}}(-\xi )={\widehat {f}}^{*}(\xi )} $$ $$ f ( x ) = cos ⁡ ( 2 π ξ 0 x ) {\displaystyle f(x)=\cos(2\pi \xi _{0}x)} $$ $$ e i 2 π ξ 0 x . {\displaystyle e^{i2\pi \xi _{0}x}.} $$ $$ ξ 0 , {\displaystyle \xi _{0},} $$ $$ cos ⁡ ( 2 π ξ 0 x ) {\displaystyle \cos(2\pi \xi _{0}x)} $$ $$ cos ⁡ ( 2 π ( − ξ 0 ) x ) {\displaystyle \cos(2\pi (-\xi _{0})x)} $$ The Fourier transform of a periodic function cannot be defined using the integral formula directly. In order for integral in Eq.1 to be defined the function must be absolutely integrable. Instead it is common to use Fourier series. It is possible to extend the definition to include periodic functions by viewing them as tempered distributions. This makes it possible to see a connection between the Fourier series and the Fourier transform for periodic functions that have a convergent Fourier series. If f ( x ) f(x) is a periodic function, with period P P , that has a convergent Fourier series, then: f ^ ( ξ ) = ∑ n = − ∞ ∞ c n ⋅ δ ( ξ − n P ) , {\widehat {f}(\xi )=\sum _{n=-\infty }^{\infty }c_{n}\cdot \delta \left(\xi -{\tfrac {n}{P}}\right),} where c n c_{n} are the Fourier series coefficients of f f , and δ \delta is the Dirac delta function. In other words, the Fourier transform is a Dirac comb function whose teeth are multiplied by the Fourier series coefficients. $$ f ( x ) {\displaystyle f(x)} $$ $$ P {\displaystyle P} $$ $$ f ^ ( ξ ) = ∑ n = − ∞ ∞ c n ⋅ δ ( ξ − n P ) , {\displaystyle {\widehat {f}}(\xi )=\sum _{n=-\infty }^{\infty }c_{n}\cdot \delta \left(\xi -{\tfrac {n}{P}}\right),} $$ $$ c n {\displaystyle c_{n}} $$ $$ f {\displaystyle f} $$ $$ δ {\displaystyle \delta } $$ The Fourier transform of an integrable function f f can be sampled at regular intervals of arbitrary length 1 P . {\tfrac {1{P}}.} These samples can be deduced from one cycle of a periodic function f P f_{P} which has Fourier series coefficients proportional to those samples by the Poisson summation formula: f P ( x ) ≜ ∑ n = − ∞ ∞ f ( x + n P ) = 1 P ∑ k = − ∞ ∞ f ^ ( k P ) e i 2 π k P x , ∀ k ∈ Z f_{P(x)\triangleq \sum _{n=-\infty }^{\infty }f(x+nP)={\frac {1}{P}}\sum _{k=-\infty }^{\infty }{\widehat {f}}\left({\tfrac {k}{P}}\right)e^{i2\pi {\frac {k}{P}}x},\quad \forall k\in \mathbb {Z} } $$ f {\displaystyle f} $$ $$ 1 P . {\displaystyle {\tfrac {1}{P}}.} $$ $$ f P {\displaystyle f_{P}} $$ $$ f P ( x ) ≜ ∑ n = − ∞ ∞ f ( x + n P ) = 1 P ∑ k = − ∞ ∞ f ^ ( k P ) e i 2 π k P x , ∀ k ∈ Z {\displaystyle f_{P}(x)\triangleq \sum _{n=-\infty }^{\infty }f(x+nP)={\frac {1}{P}}\sum _{k=-\infty }^{\infty }{\widehat {f}}\left({\tfrac {k}{P}}\right)e^{i2\pi {\frac {k}{P}}x},\quad \forall k\in \mathbb {Z} } $$ The integrability of f f ensures the periodic summation converges. Therefore, the samples f ^ ( k P ) {\widehat {f}\left({\tfrac {k}{P}}\right)} can be determined by Fourier series analysis: f ^ ( k P ) = ∫ P f P ( x ) ⋅ e − i 2 π k P x d x . {\widehat {f}\left({\tfrac {k}{P}}\right)=\int _{P}f_{P}(x)\cdot e^{-i2\pi {\frac {k}{P}}x}\,dx.} $$ f {\displaystyle f} $$ $$ f ^ ( k P ) {\displaystyle {\widehat {f}}\left({\tfrac {k}{P}}\right)} $$ $$ f ^ ( k P ) = ∫ P f P ( x ) ⋅ e − i 2 π k P x d x . {\displaystyle {\widehat {f}}\left({\tfrac {k}{P}}\right)=\int _{P}f_{P}(x)\cdot e^{-i2\pi {\frac {k}{P}}x}\,dx.} $$ When f ( x ) f(x) has compact support, f P ( x ) f_{P(x)} has a finite number of terms within the interval of integration. When f ( x ) f(x) does not have compact support, numerical evaluation of f P ( x ) f_{P(x)} requires an approximation, such as tapering f ( x ) f(x) or truncating the number of terms. $$ f ( x ) {\displaystyle f(x)} $$ $$ f P ( x ) {\displaystyle f_{P}(x)} $$ $$ f ( x ) {\displaystyle f(x)} $$ $$ f P ( x ) {\displaystyle f_{P}(x)} $$ $$ f ( x ) {\displaystyle f(x)} $$ The following figures provide a visual illustration of how the Fourier transform's integral measures whether a frequency is present in a particular function. The first image depicts the function f ( t ) = cos ⁡ ( 2 π 3 t ) e − π t 2 , f(t)=\cos(2\pi \ 3t)\ e^{-\pi t^{2},} which is a 3 Hz cosine wave (the first term) shaped by a Gaussian envelope function (the second term) that smoothly turns the wave on and off. The next 2 images show the product f ( t ) e − i 2 π 3 t , f(t)e^{-i2\pi 3t,} which must be integrated to calculate the Fourier transform at +3 Hz. The real part of the integrand has a non-negative average value, because the alternating signs of f ( t ) f(t) and Re ⁡ ( e − i 2 π 3 t ) \operatorname {Re (e^{-i2\pi 3t})} oscillate at the same rate and in phase, whereas f ( t ) f(t) and Im ⁡ ( e − i 2 π 3 t ) \operatorname {Im (e^{-i2\pi 3t})} oscillate at the same rate but with orthogonal phase. The absolute value of the Fourier transform at +3 Hz is 0.5, which is relatively large. When added to the Fourier transform at -3 Hz (which is identical because we started with a real signal), we find that the amplitude of the 3 Hz frequency component is 1. $$ f ( t ) = cos ⁡ ( 2 π 3 t ) e − π t 2 , {\displaystyle f(t)=\cos(2\pi \ 3t)\ e^{-\pi t^{2}},} $$ $$ f ( t ) e − i 2 π 3 t , {\displaystyle f(t)e^{-i2\pi 3t},} $$ $$ f ( t ) {\displaystyle f(t)} $$ $$ Re ⁡ ( e − i 2 π 3 t ) {\displaystyle \operatorname {Re} (e^{-i2\pi 3t})} $$ $$ f ( t ) {\displaystyle f(t)} $$ $$ Im ⁡ ( e − i 2 π 3 t ) {\displaystyle \operatorname {Im} (e^{-i2\pi 3t})} $$ However, when you try to measure a frequency that is not present, both the real and imaginary component of the integral vary rapidly between positive and negative values. For instance, the red curve is looking for 5 Hz. The absolute value of its integral is nearly zero, indicating that almost no 5 Hz component was in the signal. The general situation is usually more complicated than this, but heuristically this is how the Fourier transform measures how much of an individual frequency is present in a function f ( t ) . f(t). $$ f ( t ) . {\displaystyle f(t).} $$ To re-enforce an earlier point, the reason for the response at ξ = − 3 \xi =-3 Hz is because cos ⁡ ( 2 π 3 t ) \cos(2\pi 3t) and cos ⁡ ( 2 π ( − 3 ) t ) \cos(2\pi (-3)t) are indistinguishable. The transform of e i 2 π 3 t ⋅ e − π t 2 e^{i2\pi 3t\cdot e^{-\pi t^{2}}} would have just one response, whose amplitude is the integral of the smooth envelope: e − π t 2 , e^{-\pi t^{2},} whereas Re ⁡ ( f ( t ) ⋅ e − i 2 π 3 t ) \operatorname {Re (f(t)\cdot e^{-i2\pi 3t})} is e − π t 2 ( 1 + cos ⁡ ( 2 π 6 t ) ) / 2. e^{-\pi t^{2}(1+\cos(2\pi 6t))/2.} $$ ξ = − 3 {\displaystyle \xi =-3} $$ $$ cos ⁡ ( 2 π 3 t ) {\displaystyle \cos(2\pi 3t)} $$ $$ cos ⁡ ( 2 π ( − 3 ) t ) {\displaystyle \cos(2\pi (-3)t)} $$ $$ e i 2 π 3 t ⋅ e − π t 2 {\displaystyle e^{i2\pi 3t}\cdot e^{-\pi t^{2}}} $$ $$ e − π t 2 , {\displaystyle e^{-\pi t^{2}},} $$ $$ Re ⁡ ( f ( t ) ⋅ e − i 2 π 3 t ) {\displaystyle \operatorname {Re} (f(t)\cdot e^{-i2\pi 3t})} $$ $$ e − π t 2 ( 1 + cos ⁡ ( 2 π 6 t ) ) / 2. {\displaystyle e^{-\pi t^{2}}(1+\cos(2\pi 6t))/2.} $$ Let f ( x ) f(x) and h ( x ) h(x) represent integrable functions Lebesgue-measurable on the real line satisfying: ∫ − ∞ ∞ | f ( x ) | d x < ∞ . \int _{-\infty ^{\infty }|f(x)|\,dx<\infty .} We denote the Fourier transforms of these functions as f ^ ( ξ ) {\hat {f}(\xi )} and h ^ ( ξ ) {\hat {h}(\xi )} respectively. $$ f ( x ) {\displaystyle f(x)} $$ $$ h ( x ) {\displaystyle h(x)} $$ $$ ∫ − ∞ ∞ | f ( x ) | d x < ∞ . {\displaystyle \int _{-\infty }^{\infty }|f(x)|\,dx<\infty .} $$ $$ f ^ ( ξ ) {\displaystyle {\hat {f}}(\xi )} $$ $$ h ^ ( ξ ) {\displaystyle {\hat {h}}(\xi )} $$ The Fourier transform has the following basic properties: a f ( x ) + b h ( x ) ⟺ F a f ^ ( ξ ) + b h ^ ( ξ ) ; a , b ∈ C a\ f(x)+b\ h(x)\ \ {\stackrel {\mathcal {F}{\Longleftrightarrow }}\ \ a\ {\widehat {f}}(\xi )+b\ {\widehat {h}}(\xi );\quad \ a,b\in \mathbb {C} } $$ a f ( x ) + b h ( x ) ⟺ F a f ^ ( ξ ) + b h ^ ( ξ ) ; a , b ∈ C {\displaystyle a\ f(x)+b\ h(x)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ a\ {\widehat {f}}(\xi )+b\ {\widehat {h}}(\xi );\quad \ a,b\in \mathbb {C} } $$ f ( x − x 0 ) ⟺ F e − i 2 π x 0 ξ f ^ ( ξ ) ; x 0 ∈ R f(x-x_{0)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ e^{-i2\pi x_{0}\xi }\ {\widehat {f}}(\xi );\quad \ x_{0}\in \mathbb {R} } $$ f ( x − x 0 ) ⟺ F e − i 2 π x 0 ξ f ^ ( ξ ) ; x 0 ∈ R {\displaystyle f(x-x_{0})\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ e^{-i2\pi x_{0}\xi }\ {\widehat {f}}(\xi );\quad \ x_{0}\in \mathbb {R} } $$ e i 2 π ξ 0 x f ( x ) ⟺ F f ^ ( ξ − ξ 0 ) ; ξ 0 ∈ R e^{i2\pi \xi _{0x}f(x)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\widehat {f}}(\xi -\xi _{0});\quad \ \xi _{0}\in \mathbb {R} } $$ e i 2 π ξ 0 x f ( x ) ⟺ F f ^ ( ξ − ξ 0 ) ; ξ 0 ∈ R {\displaystyle e^{i2\pi \xi _{0}x}f(x)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\widehat {f}}(\xi -\xi _{0});\quad \ \xi _{0}\in \mathbb {R} } $$ f ( a x ) ⟺ F 1 | a | f ^ ( ξ a ) ; a ≠ 0 f(ax)\ \ {\stackrel {\mathcal {F}{\Longleftrightarrow }}\ \ {\frac {1}{|a|}}{\widehat {f}}\left({\frac {\xi }{a}}\right);\quad \ a\neq 0} The case a = − 1 a=-1 leads to the time-reversal property: f ( − x ) ⟺ F f ^ ( − ξ ) f(-x)\ \ {\stackrel {\mathcal {F}{\Longleftrightarrow }}\ \ {\widehat {f}}(-\xi )} $$ f ( a x ) ⟺ F 1 | a | f ^ ( ξ a ) ; a ≠ 0 {\displaystyle f(ax)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\frac {1}{|a|}}{\widehat {f}}\left({\frac {\xi }{a}}\right);\quad \ a\neq 0} $$ $$ a = − 1 {\displaystyle a=-1} $$ $$ f ( − x ) ⟺ F f ^ ( − ξ ) {\displaystyle f(-x)\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\widehat {f}}(-\xi )} $$ $$ f ( t ) {\displaystyle \scriptstyle f(t)} $$ $$ f ^ ( ω ) {\displaystyle \scriptstyle {\widehat {f}}(\omega )} $$ $$ g ( t ) {\displaystyle \scriptstyle g(t)} $$ $$ g ^ ( ω ) {\displaystyle \scriptstyle {\widehat {g}}(\omega )} $$ $$ t {\displaystyle \scriptstyle t} $$ $$ ω {\displaystyle \scriptstyle \omega } $$ $$ t {\displaystyle \scriptstyle t} $$ $$ ω {\displaystyle \scriptstyle \omega } $$ $$ ( f ( t ) = f R E ) {\displaystyle (f(t)=f_{RE})} $$ $$ ( f ^ R E ) . {\displaystyle ({\hat {f}}_{RE}).} $$ $$ ( g ( t ) = g R E + g R O ) , {\displaystyle (g(t)=g_{RE}+g_{RO}),} $$ $$ i ⋅ g ^ I O . {\displaystyle i\cdot {\hat {g}}_{IO}.} $$ When the real and imaginary parts of a complex function are decomposed into their even and odd parts, there are four components, denoted below by the subscripts RE, RO, IE, and IO. And there is a one-to-one mapping between the four components of a complex time function and the four components of its complex frequency transform: T i m e d o m a i n f = f R E + f R O + i f I E + i f I O ⏟ ⇕ F ⇕ F ⇕ F ⇕ F ⇕ F F r e q u e n c y d o m a i n f ^ = f ^ R E + i f ^ I O ⏞ + i f ^ I E + f ^ R O {\begin{aligned{\mathsf {Time\ domain}}\quad &\ f\quad &=\quad &f_{_{RE}}\quad &+\quad &f_{_{RO}}\quad &+\quad i\ &f_{_{IE}}\quad &+\quad &\underbrace {i\ f_{_{IO}}} \\&{\Bigg \Updownarrow }{\mathcal {F}}&&{\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}\\{\mathsf {Frequency\ domain}}\quad &{\widehat {f}}\quad &=\quad &{\widehat {f}}_{RE}\quad &+\quad &\overbrace {i\ {\widehat {f}}_{IO}} \quad &+\quad i\ &{\widehat {f}}_{IE}\quad &+\quad &{\widehat {f}}_{RO}\end{aligned}}} $$ T i m e d o m a i n f = f R E + f R O + i f I E + i f I O ⏟ ⇕ F ⇕ F ⇕ F ⇕ F ⇕ F F r e q u e n c y d o m a i n f ^ = f ^ R E + i f ^ I O ⏞ + i f ^ I E + f ^ R O {\displaystyle {\begin{aligned}{\mathsf {Time\ domain}}\quad &\ f\quad &=\quad &f_{_{RE}}\quad &+\quad &f_{_{RO}}\quad &+\quad i\ &f_{_{IE}}\quad &+\quad &\underbrace {i\ f_{_{IO}}} \\&{\Bigg \Updownarrow }{\mathcal {F}}&&{\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}&&\ \ {\Bigg \Updownarrow }{\mathcal {F}}\\{\mathsf {Frequency\ domain}}\quad &{\widehat {f}}\quad &=\quad &{\widehat {f}}_{RE}\quad &+\quad &\overbrace {i\ {\widehat {f}}_{IO}} \quad &+\quad i\ &{\widehat {f}}_{IE}\quad &+\quad &{\widehat {f}}_{RO}\end{aligned}}} $$ From this, various relationships are apparent, for example: $$ ( f R E + f R O ) {\displaystyle (f_{_{RE}}+f_{_{RO}})} $$ $$ f ^ R E + i f ^ I O . {\displaystyle {\hat {f}}_{RE}+i\ {\hat {f}}_{IO}.} $$ $$ ( i f I E + i f I O ) {\displaystyle (i\ f_{_{IE}}+i\ f_{_{IO}})} $$ $$ f ^ R O + i f ^ I E , {\displaystyle {\hat {f}}_{RO}+i\ {\hat {f}}_{IE},} $$ $$ ( f R E + i f I O ) {\displaystyle (f_{_{RE}}+i\ f_{_{IO}})} $$ $$ f ^ R E + f ^ R O , {\displaystyle {\hat {f}}_{RE}+{\hat {f}}_{RO},} $$ $$ ( f R O + i f I E ) {\displaystyle (f_{_{RO}}+i\ f_{_{IE}})} $$ $$ i f ^ I E + i f ^ I O , {\displaystyle i\ {\hat {f}}_{IE}+i{\hat {f}}_{IO},} $$ ( f ( x ) ) ∗ ⟺ F ( f ^ ( − ξ ) ) ∗ {\bigl (f(x){\bigr )}^{*}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ \left({\widehat {f}}(-\xi )\right)^{*}} (Note: the ∗ denotes complex conjugation.) $$ ( f ( x ) ) ∗ ⟺ F ( f ^ ( − ξ ) ) ∗ {\displaystyle {\bigl (}f(x){\bigr )}^{*}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ \left({\widehat {f}}(-\xi )\right)^{*}} $$ In particular, if f f is real, then f ^ {\widehat {f}} is even symmetric (aka Hermitian function): f ^ ( − ξ ) = ( f ^ ( ξ ) ) ∗ . {\widehat {f}(-\xi )={\bigl (}{\widehat {f}}(\xi ){\bigr )}^{*}.} $$ f {\displaystyle f} $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ $$ f ^ ( − ξ ) = ( f ^ ( ξ ) ) ∗ . {\displaystyle {\widehat {f}}(-\xi )={\bigl (}{\widehat {f}}(\xi ){\bigr )}^{*}.} $$ And if f f is purely imaginary, then f ^ {\widehat {f}} is odd symmetric: f ^ ( − ξ ) = − ( f ^ ( ξ ) ) ∗ . {\widehat {f}(-\xi )=-({\widehat {f}}(\xi ))^{*}.} $$ f {\displaystyle f} $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ $$ f ^ ( − ξ ) = − ( f ^ ( ξ ) ) ∗ . {\displaystyle {\widehat {f}}(-\xi )=-({\widehat {f}}(\xi ))^{*}.} $$ Re ⁡ { f ( x ) } ⟺ F 1 2 ( f ^ ( ξ ) + ( f ^ ( − ξ ) ) ∗ ) \operatorname {Re \{f(x)\}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\tfrac {1}{2}}\left({\widehat {f}}(\xi )+{\bigl (}{\widehat {f}}(-\xi ){\bigr )}^{*}\right)} Im ⁡ { f ( x ) } ⟺ F 1 2 i ( f ^ ( ξ ) − ( f ^ ( − ξ ) ) ∗ ) \operatorname {Im \{f(x)\}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\tfrac {1}{2i}}\left({\widehat {f}}(\xi )-{\bigl (}{\widehat {f}}(-\xi ){\bigr )}^{*}\right)} $$ Re ⁡ { f ( x ) } ⟺ F 1 2 ( f ^ ( ξ ) + ( f ^ ( − ξ ) ) ∗ ) {\displaystyle \operatorname {Re} \{f(x)\}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\tfrac {1}{2}}\left({\widehat {f}}(\xi )+{\bigl (}{\widehat {f}}(-\xi ){\bigr )}^{*}\right)} $$ $$ Im ⁡ { f ( x ) } ⟺ F 1 2 i ( f ^ ( ξ ) − ( f ^ ( − ξ ) ) ∗ ) {\displaystyle \operatorname {Im} \{f(x)\}\ \ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ \ {\tfrac {1}{2i}}\left({\widehat {f}}(\xi )-{\bigl (}{\widehat {f}}(-\xi ){\bigr )}^{*}\right)} $$ Substituting ξ = 0 \xi =0 in the definition, we obtain: f ^ ( 0 ) = ∫ − ∞ ∞ f ( x ) d x . {\widehat {f}(0)=\int _{-\infty }^{\infty }f(x)\,dx.} $$ ξ = 0 {\displaystyle \xi =0} $$ $$ f ^ ( 0 ) = ∫ − ∞ ∞ f ( x ) d x . {\displaystyle {\widehat {f}}(0)=\int _{-\infty }^{\infty }f(x)\,dx.} $$ The integral of f f over its domain is known as the average value or DC bias of the function. $$ f {\displaystyle f} $$ Under suitable conditions on the function f f , it can be recovered from its Fourier transform f ^ {\hat {f}} . Indeed, denoting the Fourier transform operator by F {\mathcal {F}} , so F f := f ^ {\mathcal {F}f:={\hat {f}}} , then for suitable functions, applying the Fourier transform twice simply flips the function: ( F 2 f ) ( x ) = f ( − x ) \left({\mathcal {F}^{2}f\right)(x)=f(-x)} , which can be interpreted as "reversing time". Since reversing time is two-periodic, applying this twice yields F 4 ( f ) = f {\mathcal {F}^{4}(f)=f} , so the Fourier transform operator is four-periodic, and similarly the inverse Fourier transform can be obtained by applying the Fourier transform three times: F 3 ( f ^ ) = f {\mathcal {F}^{3}\left({\hat {f}}\right)=f} . In particular the Fourier transform is invertible (under suitable conditions). $$ f {\displaystyle f} $$ $$ f ^ {\displaystyle {\hat {f}}} $$ $$ F {\displaystyle {\mathcal {F}}} $$ $$ F f := f ^ {\displaystyle {\mathcal {F}}f:={\hat {f}}} $$ $$ ( F 2 f ) ( x ) = f ( − x ) {\displaystyle \left({\mathcal {F}}^{2}f\right)(x)=f(-x)} $$ $$ F 4 ( f ) = f {\displaystyle {\mathcal {F}}^{4}(f)=f} $$ $$ F 3 ( f ^ ) = f {\displaystyle {\mathcal {F}}^{3}\left({\hat {f}}\right)=f} $$ More precisely, defining the parity operator P {\mathcal {P}} such that ( P f ) ( x ) = f ( − x ) ({\mathcal {P}f)(x)=f(-x)} , we have: F 0 = i d , F 1 = F , F 2 = P , F 3 = F − 1 = P ∘ F = F ∘ P , F 4 = i d {\begin{aligned{\mathcal {F}}^{0}&=\mathrm {id} ,\\{\mathcal {F}}^{1}&={\mathcal {F}},\\{\mathcal {F}}^{2}&={\mathcal {P}},\\{\mathcal {F}}^{3}&={\mathcal {F}}^{-1}={\mathcal {P}}\circ {\mathcal {F}}={\mathcal {F}}\circ {\mathcal {P}},\\{\mathcal {F}}^{4}&=\mathrm {id} \end{aligned}}} These equalities of operators require careful definition of the space of functions in question, defining equality of functions (equality at every point? equality almost everywhere?) and defining equality of operators – that is, defining the topology on the function space and operator space in question. These are not true for all functions, but are true under various conditions, which are the content of the various forms of the Fourier inversion theorem. $$ P {\displaystyle {\mathcal {P}}} $$ $$ ( P f ) ( x ) = f ( − x ) {\displaystyle ({\mathcal {P}}f)(x)=f(-x)} $$ $$ F 0 = i d , F 1 = F , F 2 = P , F 3 = F − 1 = P ∘ F = F ∘ P , F 4 = i d {\displaystyle {\begin{aligned}{\mathcal {F}}^{0}&=\mathrm {id} ,\\{\mathcal {F}}^{1}&={\mathcal {F}},\\{\mathcal {F}}^{2}&={\mathcal {P}},\\{\mathcal {F}}^{3}&={\mathcal {F}}^{-1}={\mathcal {P}}\circ {\mathcal {F}}={\mathcal {F}}\circ {\mathcal {P}},\\{\mathcal {F}}^{4}&=\mathrm {id} \end{aligned}}} $$ This fourfold periodicity of the Fourier transform is similar to a rotation of the plane by 90°, particularly as the two-fold iteration yields a reversal, and in fact this analogy can be made precise. While the Fourier transform can simply be interpreted as switching the time domain and the frequency domain, with the inverse Fourier transform switching them back, more geometrically it can be interpreted as a rotation by 90° in the time–frequency domain (considering time as the x-axis and frequency as the y-axis), and the Fourier transform can be generalized to the fractional Fourier transform, which involves rotations by other angles. This can be further generalized to linear canonical transformations, which can be visualized as the action of the special linear group SL2(R) on the time–frequency plane, with the preserved symplectic form corresponding to the uncertainty principle, below. This approach is particularly studied in signal processing, under time–frequency analysis. The frequency variable must have inverse units to the units of the original function's domain (typically named t or x). For example, if t is measured in seconds, ξ should be in cycles per second or hertz. If the scale of time is in units of 2π seconds, then another Greek letter ω typically is used instead to represent angular frequency (where ω = 2πξ) in units of radians per second. If using x for units of length, then ξ must be in inverse length, e.g., wavenumbers. That is to say, there are two versions of the real line: one which is the range of t and measured in units of t, and the other which is the range of ξ and measured in inverse units to the units of t. These two distinct versions of the real line cannot be equated with each other. Therefore, the Fourier transform goes from one space of functions to a different space of functions: functions which have a different domain of definition. In general, ξ must always be taken to be a linear form on the space of its domain, which is to say that the second real line is the dual space of the first real line. See the article on linear algebra for a more formal explanation and for more details. This point of view becomes essential in generalizations of the Fourier transform to general symmetry groups, including the case of Fourier series. That there is no one preferred way (often, one says "no canonical way") to compare the two versions of the real line which are involved in the Fourier transform—fixing the units on one line does not force the scale of the units on the other line—is the reason for the plethora of rival conventions on the definition of the Fourier transform. The various definitions resulting from different choices of units differ by various constants. In other conventions, the Fourier transform has i in the exponent instead of −i, and vice versa for the inversion formula. This convention is common in modern physics and is the default for Wolfram Alpha, and does not mean that the frequency has become negative, since there is no canonical definition of positivity for frequency of a complex wave. It simply means that f ^ ( ξ ) {\hat {f}(\xi )} is the amplitude of the wave e − i 2 π ξ x e^{-i2\pi \xi x} instead of the wave e i 2 π ξ x e^{i2\pi \xi x} (the former, with its minus sign, is often seen in the time dependence for Sinusoidal plane-wave solutions of the electromagnetic wave equation, or in the time dependence for quantum wave functions). Many of the identities involving the Fourier transform remain valid in those conventions, provided all terms that explicitly involve i have it replaced by −i. In Electrical engineering the letter j is typically used for the imaginary unit instead of i because i is used for current. $$ f ^ ( ξ ) {\displaystyle {\hat {f}}(\xi )} $$ $$ e − i 2 π ξ x {\displaystyle e^{-i2\pi \xi x}} $$ $$ e i 2 π ξ x {\displaystyle e^{i2\pi \xi x}} $$ When using dimensionless units, the constant factors might not even be written in the transform definition. For instance, in probability theory, the characteristic function Φ of the probability density function f of a random variable X of continuous type is defined without a negative sign in the exponential, and since the units of x are ignored, there is no 2π either: ϕ ( λ ) = ∫ − ∞ ∞ f ( x ) e i λ x d x . \phi (\lambda )=\int _{-\infty ^{\infty }f(x)e^{i\lambda x}\,dx.} $$ ϕ ( λ ) = ∫ − ∞ ∞ f ( x ) e i λ x d x . {\displaystyle \phi (\lambda )=\int _{-\infty }^{\infty }f(x)e^{i\lambda x}\,dx.} $$ (In probability theory, and in mathematical statistics, the use of the Fourier—Stieltjes transform is preferred, because so many random variables are not of continuous type, and do not possess a density function, and one must treat not functions but distributions, i.e., measures which possess "atoms".) From the higher point of view of group characters, which is much more abstract, all these arbitrary choices disappear, as will be explained in the later section of this article, which treats the notion of the Fourier transform of a function on a locally compact Abelian group. The Fourier transform may be defined in some cases for non-integrable functions, but the Fourier transforms of integrable functions have several strong properties. The Fourier transform f̂ of any integrable function f is uniformly continuous and ‖ f ^ ‖ ∞ ≤ ‖ f ‖ 1 \left\|{\hat {f}\right\|_{\infty }\leq \left\|f\right\|_{1}} $$ ‖ f ^ ‖ ∞ ≤ ‖ f ‖ 1 {\displaystyle \left\|{\hat {f}}\right\|_{\infty }\leq \left\|f\right\|_{1}} $$ By the Riemann–Lebesgue lemma, f ^ ( ξ ) → 0 as | ξ | → ∞ . {\hat {f}(\xi )\to 0{\text{ as }}|\xi |\to \infty .} $$ f ^ ( ξ ) → 0 as | ξ | → ∞ . {\displaystyle {\hat {f}}(\xi )\to 0{\text{ as }}|\xi |\to \infty .} $$ However, f ^ {\hat {f}} need not be integrable. For example, the Fourier transform of the rectangular function, which is integrable, is the sinc function, which is not Lebesgue integrable, because its improper integrals behave analogously to the alternating harmonic series, in converging to a sum without being absolutely convergent. $$ f ^ {\displaystyle {\hat {f}}} $$ It is not generally possible to write the inverse transform as a Lebesgue integral. However, when both f and f ^ {\hat {f}} are integrable, the inverse equality f ( x ) = ∫ − ∞ ∞ f ^ ( ξ ) e i 2 π x ξ d ξ f(x)=\int _{-\infty ^{\infty }{\hat {f}}(\xi )e^{i2\pi x\xi }\,d\xi } holds for almost every x. As a result, the Fourier transform is injective on L1(R). $$ f ^ {\displaystyle {\hat {f}}} $$ $$ f ( x ) = ∫ − ∞ ∞ f ^ ( ξ ) e i 2 π x ξ d ξ {\displaystyle f(x)=\int _{-\infty }^{\infty }{\hat {f}}(\xi )e^{i2\pi x\xi }\,d\xi } $$ Main page: Plancherel theorem Let f(x) and g(x) be integrable, and let f̂(ξ) and ĝ(ξ) be their Fourier transforms. If f(x) and g(x) are also square-integrable, then the Parseval formula follows: ⟨ f , g ⟩ L 2 = ∫ − ∞ ∞ f ( x ) g ( x ) ¯ d x = ∫ − ∞ ∞ f ^ ( ξ ) g ^ ( ξ ) ¯ d ξ , \langle f,g\rangle _{L^{2}=\int _{-\infty }^{\infty }f(x){\overline {g(x)}}\,dx=\int _{-\infty }^{\infty }{\hat {f}}(\xi ){\overline {{\hat {g}}(\xi )}}\,d\xi ,} where the bar denotes complex conjugation. $$ ⟨ f , g ⟩ L 2 = ∫ − ∞ ∞ f ( x ) g ( x ) ¯ d x = ∫ − ∞ ∞ f ^ ( ξ ) g ^ ( ξ ) ¯ d ξ , {\displaystyle \langle f,g\rangle _{L^{2}}=\int _{-\infty }^{\infty }f(x){\overline {g(x)}}\,dx=\int _{-\infty }^{\infty }{\hat {f}}(\xi ){\overline {{\hat {g}}(\xi )}}\,d\xi ,} $$ The Plancherel theorem, which follows from the above, states that ‖ f ‖ L 2 2 = ∫ − ∞ ∞ | f ( x ) | 2 d x = ∫ − ∞ ∞ | f ^ ( ξ ) | 2 d ξ . \|f\|_{L^{2}^{2}=\int _{-\infty }^{\infty }\left|f(x)\right|^{2}\,dx=\int _{-\infty }^{\infty }\left|{\hat {f}}(\xi )\right|^{2}\,d\xi .} $$ ‖ f ‖ L 2 2 = ∫ − ∞ ∞ | f ( x ) | 2 d x = ∫ − ∞ ∞ | f ^ ( ξ ) | 2 d ξ . {\displaystyle \|f\|_{L^{2}}^{2}=\int _{-\infty }^{\infty }\left|f(x)\right|^{2}\,dx=\int _{-\infty }^{\infty }\left|{\hat {f}}(\xi )\right|^{2}\,d\xi .} $$ Plancherel's theorem makes it possible to extend the Fourier transform, by a continuity argument, to a unitary operator on L2(R). On L1(R) ∩ L2(R), this extension agrees with original Fourier transform defined on L1(R), thus enlarging the domain of the Fourier transform to L1(R) + L2(R) (and consequently to Lp(R) for 1 ≤ p ≤ 2). Plancherel's theorem has the interpretation in the sciences that the Fourier transform preserves the energy of the original quantity. The terminology of these formulas is not quite standardised. Parseval's theorem was proved only for Fourier series, and was first proved by Lyapunov. But Parseval's formula makes sense for the Fourier transform as well, and so even though in the context of the Fourier transform it was proved by Plancherel, it is still often referred to as Parseval's formula, or Parseval's relation, or even Parseval's theorem. See Pontryagin duality for a general formulation of this concept in the context of locally compact abelian groups. The Poisson summation formula (PSF) is an equation that relates the Fourier series coefficients of the periodic summation of a function to values of the function's continuous Fourier transform. The Poisson summation formula says that for sufficiently regular functions f, ∑ n f ^ ( n ) = ∑ n f ( n ) . \sum _{n{\hat {f}}(n)=\sum _{n}f(n).} $$ ∑ n f ^ ( n ) = ∑ n f ( n ) . {\displaystyle \sum _{n}{\hat {f}}(n)=\sum _{n}f(n).} $$ It has a variety of useful forms that are derived from the basic one by application of the Fourier transform's scaling and time-shifting properties. The formula has applications in engineering, physics, and number theory. The frequency-domain dual of the standard Poisson summation formula is also called the discrete-time Fourier transform. Poisson summation is generally associated with the physics of periodic media, such as heat conduction on a circle. The fundamental solution of the heat equation on a circle is called a theta function. It is used in number theory to prove the transformation properties of theta functions, which turn out to be a type of modular form, and it is connected more generally to the theory of automorphic forms where it appears on one side of the Selberg trace formula. Suppose f(x) is an absolutely continuous differentiable function, and both f and its derivative f′ are integrable. Then the Fourier transform of the derivative is given by f ′ ^ ( ξ ) = F { d d x f ( x ) } = i 2 π ξ f ^ ( ξ ) . {\widehat {f'\,}(\xi )={\mathcal {F}}\left\{{\frac {d}{dx}}f(x)\right\}=i2\pi \xi {\hat {f}}(\xi ).} More generally, the Fourier transformation of the nth derivative f(n) is given by f ( n ) ^ ( ξ ) = F { d n d x n f ( x ) } = ( i 2 π ξ ) n f ^ ( ξ ) . {\widehat {f^{(n)}}(\xi )={\mathcal {F}}\left\{{\frac {d^{n}}{dx^{n}}}f(x)\right\}=(i2\pi \xi )^{n}{\hat {f}}(\xi ).} $$ f ′ ^ ( ξ ) = F { d d x f ( x ) } = i 2 π ξ f ^ ( ξ ) . {\displaystyle {\widehat {f'\,}}(\xi )={\mathcal {F}}\left\{{\frac {d}{dx}}f(x)\right\}=i2\pi \xi {\hat {f}}(\xi ).} $$ $$ f ( n ) ^ ( ξ ) = F { d n d x n f ( x ) } = ( i 2 π ξ ) n f ^ ( ξ ) . {\displaystyle {\widehat {f^{(n)}}}(\xi )={\mathcal {F}}\left\{{\frac {d^{n}}{dx^{n}}}f(x)\right\}=(i2\pi \xi )^{n}{\hat {f}}(\xi ).} $$ Analogously, F { d n d ξ n f ^ ( ξ ) } = ( i 2 π x ) n f ( x ) {\mathcal {F}\left\{{\frac {d^{n}}{d\xi ^{n}}}{\hat {f}}(\xi )\right\}=(i2\pi x)^{n}f(x)} , so F { x n f ( x ) } = ( i 2 π ) n d n d ξ n f ^ ( ξ ) . {\mathcal {F}\left\{x^{n}f(x)\right\}=\left({\frac {i}{2\pi }}\right)^{n}{\frac {d^{n}}{d\xi ^{n}}}{\hat {f}}(\xi ).} $$ F { d n d ξ n f ^ ( ξ ) } = ( i 2 π x ) n f ( x ) {\displaystyle {\mathcal {F}}\left\{{\frac {d^{n}}{d\xi ^{n}}}{\hat {f}}(\xi )\right\}=(i2\pi x)^{n}f(x)} $$ $$ F { x n f ( x ) } = ( i 2 π ) n d n d ξ n f ^ ( ξ ) . {\displaystyle {\mathcal {F}}\left\{x^{n}f(x)\right\}=\left({\frac {i}{2\pi }}\right)^{n}{\frac {d^{n}}{d\xi ^{n}}}{\hat {f}}(\xi ).} $$ By applying the Fourier transform and using these formulas, some ordinary differential equations can be transformed into algebraic equations, which are much easier to solve. These formulas also give rise to the rule of thumb "f(x) is smooth if and only if f̂(ξ) quickly falls to 0 for |ξ| → ∞." By using the analogous rules for the inverse Fourier transform, one can also say "f(x) quickly falls to 0 for |x| → ∞ if and only if f̂(ξ) is smooth." The Fourier transform translates between convolution and multiplication of functions. If f(x) and g(x) are integrable functions with Fourier transforms f̂(ξ) and ĝ(ξ) respectively, then the Fourier transform of the convolution is given by the product of the Fourier transforms f̂(ξ) and ĝ(ξ) (under other conventions for the definition of the Fourier transform a constant factor may appear). This means that if: h ( x ) = ( f ∗ g ) ( x ) = ∫ − ∞ ∞ f ( y ) g ( x − y ) d y , h(x)=(f*g)(x)=\int _{-\infty ^{\infty }f(y)g(x-y)\,dy,} where ∗ denotes the convolution operation, then: h ^ ( ξ ) = f ^ ( ξ ) g ^ ( ξ ) . {\hat {h}(\xi )={\hat {f}}(\xi )\,{\hat {g}}(\xi ).} $$ h ( x ) = ( f ∗ g ) ( x ) = ∫ − ∞ ∞ f ( y ) g ( x − y ) d y , {\displaystyle h(x)=(f*g)(x)=\int _{-\infty }^{\infty }f(y)g(x-y)\,dy,} $$ $$ h ^ ( ξ ) = f ^ ( ξ ) g ^ ( ξ ) . {\displaystyle {\hat {h}}(\xi )={\hat {f}}(\xi )\,{\hat {g}}(\xi ).} $$ In linear time invariant (LTI) system theory, it is common to interpret g(x) as the impulse response of an LTI system with input f(x) and output h(x), since substituting the unit impulse for f(x) yields h(x) = g(x). In this case, ĝ(ξ) represents the frequency response of the system. Conversely, if f(x) can be decomposed as the product of two square integrable functions p(x) and q(x), then the Fourier transform of f(x) is given by the convolution of the respective Fourier transforms p̂(ξ) and q̂(ξ). In an analogous manner, it can be shown that if h(x) is the cross-correlation of f(x) and g(x): h ( x ) = ( f ⋆ g ) ( x ) = ∫ − ∞ ∞ f ( y ) ¯ g ( x + y ) d y h(x)=(f\star g)(x)=\int _{-\infty ^{\infty }{\overline {f(y)}}g(x+y)\,dy} then the Fourier transform of h(x) is: h ^ ( ξ ) = f ^ ( ξ ) ¯ g ^ ( ξ ) . {\hat {h}(\xi )={\overline {{\hat {f}}(\xi )}}\,{\hat {g}}(\xi ).} $$ h ( x ) = ( f ⋆ g ) ( x ) = ∫ − ∞ ∞ f ( y ) ¯ g ( x + y ) d y {\displaystyle h(x)=(f\star g)(x)=\int _{-\infty }^{\infty }{\overline {f(y)}}g(x+y)\,dy} $$ $$ h ^ ( ξ ) = f ^ ( ξ ) ¯ g ^ ( ξ ) . {\displaystyle {\hat {h}}(\xi )={\overline {{\hat {f}}(\xi )}}\,{\hat {g}}(\xi ).} $$ As a special case, the autocorrelation of function f(x) is: h ( x ) = ( f ⋆ f ) ( x ) = ∫ − ∞ ∞ f ( y ) ¯ f ( x + y ) d y h(x)=(f\star f)(x)=\int _{-\infty ^{\infty }{\overline {f(y)}}f(x+y)\,dy} for which h ^ ( ξ ) = f ^ ( ξ ) ¯ f ^ ( ξ ) = | f ^ ( ξ ) | 2 . {\hat {h}(\xi )={\overline {{\hat {f}}(\xi )}}{\hat {f}}(\xi )=\left|{\hat {f}}(\xi )\right|^{2}.} $$ h ( x ) = ( f ⋆ f ) ( x ) = ∫ − ∞ ∞ f ( y ) ¯ f ( x + y ) d y {\displaystyle h(x)=(f\star f)(x)=\int _{-\infty }^{\infty }{\overline {f(y)}}f(x+y)\,dy} $$ $$ h ^ ( ξ ) = f ^ ( ξ ) ¯ f ^ ( ξ ) = | f ^ ( ξ ) | 2 . {\displaystyle {\hat {h}}(\xi )={\overline {{\hat {f}}(\xi )}}{\hat {f}}(\xi )=\left|{\hat {f}}(\xi )\right|^{2}.} $$ The Fourier transform is a linear transform which has eigenfunctions obeying F [ ψ ] = λ ψ , {\mathcal {F}[\psi ]=\lambda \psi ,} with λ ∈ C . \lambda \in \mathbb {C .} $$ F [ ψ ] = λ ψ , {\displaystyle {\mathcal {F}}[\psi ]=\lambda \psi ,} $$ $$ λ ∈ C . {\displaystyle \lambda \in \mathbb {C} .} $$ A set of eigenfunctions is found by noting that the homogeneous differential equation [ U ( 1 2 π d d x ) + U ( x ) ] ψ ( x ) = 0 \left[U\left({\frac {1{2\pi }}{\frac {d}{dx}}\right)+U(x)\right]\psi (x)=0} leads to eigenfunctions ψ ( x ) \psi (x) of the Fourier transform F {\mathcal {F}} as long as the form of the equation remains invariant under Fourier transform.[note 5] In other words, every solution ψ ( x ) \psi (x) and its Fourier transform ψ ^ ( ξ ) {\hat {\psi }(\xi )} obey the same equation. Assuming uniqueness of the solutions, every solution ψ ( x ) \psi (x) must therefore be an eigenfunction of the Fourier transform. The form of the equation remains unchanged under Fourier transform if U ( x ) U(x) can be expanded in a power series in which for all terms the same factor of either one of ± 1 , ± i \pm 1,\pm i arises from the factors i n i^{n} introduced by the differentiation rules upon Fourier transforming the homogeneous differential equation because this factor may then be cancelled. The simplest allowable U ( x ) = x U(x)=x leads to the standard normal distribution. $$ [ U ( 1 2 π d d x ) + U ( x ) ] ψ ( x ) = 0 {\displaystyle \left[U\left({\frac {1}{2\pi }}{\frac {d}{dx}}\right)+U(x)\right]\psi (x)=0} $$ $$ ψ ( x ) {\displaystyle \psi (x)} $$ $$ F {\displaystyle {\mathcal {F}}} $$ $$ ψ ( x ) {\displaystyle \psi (x)} $$ $$ ψ ^ ( ξ ) {\displaystyle {\hat {\psi }}(\xi )} $$ $$ ψ ( x ) {\displaystyle \psi (x)} $$ $$ U ( x ) {\displaystyle U(x)} $$ $$ ± 1 , ± i {\displaystyle \pm 1,\pm i} $$ $$ i n {\displaystyle i^{n}} $$ $$ U ( x ) = x {\displaystyle U(x)=x} $$ More generally, a set of eigenfunctions is also found by noting that the differentiation rules imply that the ordinary differential equation [ W ( i 2 π d d x ) + W ( x ) ] ψ ( x ) = C ψ ( x ) \left[W\left({\frac {i{2\pi }}{\frac {d}{dx}}\right)+W(x)\right]\psi (x)=C\psi (x)} with C C constant and W ( x ) W(x) being a non-constant even function remains invariant in form when applying the Fourier transform F {\mathcal {F}} to both sides of the equation. The simplest example is provided by W ( x ) = x 2 W(x)=x^{2} which is equivalent to considering the Schrödinger equation for the quantum harmonic oscillator. The corresponding solutions provide an important choice of an orthonormal basis for L2(R) and are given by the "physicist's" Hermite functions. Equivalently one may use ψ n ( x ) = 2 4 n ! e − π x 2 H e n ( 2 x π ) , \psi _{n(x)={\frac {\sqrt[{4}]{2}}{\sqrt {n!}}}e^{-\pi x^{2}}\mathrm {He} _{n}\left(2x{\sqrt {\pi }}\right),} where Hen(x) are the "probabilist's" Hermite polynomials, defined as H e n ( x ) = ( − 1 ) n e 1 2 x 2 ( d d x ) n e − 1 2 x 2 . \mathrm {He _{n}(x)=(-1)^{n}e^{{\frac {1}{2}}x^{2}}\left({\frac {d}{dx}}\right)^{n}e^{-{\frac {1}{2}}x^{2}}.} $$ [ W ( i 2 π d d x ) + W ( x ) ] ψ ( x ) = C ψ ( x ) {\displaystyle \left[W\left({\frac {i}{2\pi }}{\frac {d}{dx}}\right)+W(x)\right]\psi (x)=C\psi (x)} $$ $$ C {\displaystyle C} $$ $$ W ( x ) {\displaystyle W(x)} $$ $$ F {\displaystyle {\mathcal {F}}} $$ $$ W ( x ) = x 2 {\displaystyle W(x)=x^{2}} $$ $$ ψ n ( x ) = 2 4 n ! e − π x 2 H e n ( 2 x π ) , {\displaystyle \psi _{n}(x)={\frac {\sqrt[{4}]{2}}{\sqrt {n!}}}e^{-\pi x^{2}}\mathrm {He} _{n}\left(2x{\sqrt {\pi }}\right),} $$ $$ H e n ( x ) = ( − 1 ) n e 1 2 x 2 ( d d x ) n e − 1 2 x 2 . {\displaystyle \mathrm {He} _{n}(x)=(-1)^{n}e^{{\frac {1}{2}}x^{2}}\left({\frac {d}{dx}}\right)^{n}e^{-{\frac {1}{2}}x^{2}}.} $$ Under this convention for the Fourier transform, we have that ψ ^ n ( ξ ) = ( − i ) n ψ n ( ξ ) . {\hat {\psi }_{n}(\xi )=(-i)^{n}\psi _{n}(\xi ).} $$ ψ ^ n ( ξ ) = ( − i ) n ψ n ( ξ ) . {\displaystyle {\hat {\psi }}_{n}(\xi )=(-i)^{n}\psi _{n}(\xi ).} $$ In other words, the Hermite functions form a complete orthonormal system of eigenfunctions for the Fourier transform on L2(R). However, this choice of eigenfunctions is not unique. Because of F 4 = i d {\mathcal {F}^{4}=\mathrm {id} } there are only four different eigenvalues of the Fourier transform (the fourth roots of unity ±1 and ±i) and any linear combination of eigenfunctions with the same eigenvalue gives another eigenfunction. As a consequence of this, it is possible to decompose L2(R) as a direct sum of four spaces H0, H1, H2, and H3 where the Fourier transform acts on Hek simply by multiplication by ik. $$ F 4 = i d {\displaystyle {\mathcal {F}}^{4}=\mathrm {id} } $$ Since the complete set of Hermite functions ψn provides a resolution of the identity they diagonalize the Fourier operator, i.e. the Fourier transform can be represented by such a sum of terms weighted by the above eigenvalues, and these sums can be explicitly summed: F [ f ] ( ξ ) = ∫ d x f ( x ) ∑ n ≥ 0 ( − i ) n ψ n ( x ) ψ n ( ξ ) . {\mathcal {F}[f](\xi )=\int dxf(x)\sum _{n\geq 0}(-i)^{n}\psi _{n}(x)\psi _{n}(\xi )~.} $$ F [ f ] ( ξ ) = ∫ d x f ( x ) ∑ n ≥ 0 ( − i ) n ψ n ( x ) ψ n ( ξ ) . {\displaystyle {\mathcal {F}}[f](\xi )=\int dxf(x)\sum _{n\geq 0}(-i)^{n}\psi _{n}(x)\psi _{n}(\xi )~.} $$ This approach to define the Fourier transform was first proposed by Norbert Wiener. Among other properties, Hermite functions decrease exponentially fast in both frequency and time domains, and they are thus used to define a generalization of the Fourier transform, namely the fractional Fourier transform used in time–frequency analysis. In physics, this transform was introduced by Edward Condon. This change of basis functions becomes possible because the Fourier transform is a unitary transform when using the right conventions. Consequently, under the proper conditions it may be expected to result from a self-adjoint generator N N via F [ ψ ] = e − i t N ψ . {\mathcal {F}[\psi ]=e^{-itN}\psi .} $$ N {\displaystyle N} $$ $$ F [ ψ ] = e − i t N ψ . {\displaystyle {\mathcal {F}}[\psi ]=e^{-itN}\psi .} $$ The operator N N is the number operator of the quantum harmonic oscillator written as N ≡ 1 2 ( x − ∂ ∂ x ) ( x + ∂ ∂ x ) = 1 2 ( − ∂ 2 ∂ x 2 + x 2 − 1 ) . N\equiv {\frac {1{2}}\left(x-{\frac {\partial }{\partial x}}\right)\left(x+{\frac {\partial }{\partial x}}\right)={\frac {1}{2}}\left(-{\frac {\partial ^{2}}{\partial x^{2}}}+x^{2}-1\right).} $$ N {\displaystyle N} $$ $$ N ≡ 1 2 ( x − ∂ ∂ x ) ( x + ∂ ∂ x ) = 1 2 ( − ∂ 2 ∂ x 2 + x 2 − 1 ) . {\displaystyle N\equiv {\frac {1}{2}}\left(x-{\frac {\partial }{\partial x}}\right)\left(x+{\frac {\partial }{\partial x}}\right)={\frac {1}{2}}\left(-{\frac {\partial ^{2}}{\partial x^{2}}}+x^{2}-1\right).} $$ It can be interpreted as the generator of fractional Fourier transforms for arbitrary values of t, and of the conventional continuous Fourier transform F {\mathcal {F}} for the particular value t = π / 2 , t=\pi /2, with the Mehler kernel implementing the corresponding active transform. The eigenfunctions of N N are the Hermite functions ψ n ( x ) \psi _{n(x)} which are therefore also eigenfunctions of F . {\mathcal {F}.} $$ F {\displaystyle {\mathcal {F}}} $$ $$ t = π / 2 , {\displaystyle t=\pi /2,} $$ $$ N {\displaystyle N} $$ $$ ψ n ( x ) {\displaystyle \psi _{n}(x)} $$ $$ F . {\displaystyle {\mathcal {F}}.} $$ Upon extending the Fourier transform to distributions the Dirac comb is also an eigenfunction of the Fourier transform. The Heisenberg group is a certain group of unitary operators on the Hilbert space L2(R) of square integrable complex valued functions f on the real line, generated by the translations (Ty f)(x) = f (x + y) and multiplication by ei2πξx, (Mξ f)(x) = ei2πξx f (x). These operators do not commute, as their (group) commutator is ( M ξ − 1 T y − 1 M ξ T y f ) ( x ) = e i 2 π ξ y f ( x ) \left(M_{\xi ^{-1}T_{y}^{-1}M_{\xi }T_{y}f\right)(x)=e^{i2\pi \xi y}f(x)} which is multiplication by the constant (independent of x) ei2πξy ∈ U(1) (the circle group of unit modulus complex numbers). As an abstract group, the Heisenberg group is the three-dimensional Lie group of triples (x, ξ, z) ∈ R2 × U(1), with the group law ( x 1 , ξ 1 , t 1 ) ⋅ ( x 2 , ξ 2 , t 2 ) = ( x 1 + x 2 , ξ 1 + ξ 2 , t 1 t 2 e i 2 π ( x 1 ξ 1 + x 2 ξ 2 + x 1 ξ 2 ) ) . \left(x_{1,\xi _{1},t_{1}\right)\cdot \left(x_{2},\xi _{2},t_{2}\right)=\left(x_{1}+x_{2},\xi _{1}+\xi _{2},t_{1}t_{2}e^{i2\pi \left(x_{1}\xi _{1}+x_{2}\xi _{2}+x_{1}\xi _{2}\right)}\right).} $$ ( M ξ − 1 T y − 1 M ξ T y f ) ( x ) = e i 2 π ξ y f ( x ) {\displaystyle \left(M_{\xi }^{-1}T_{y}^{-1}M_{\xi }T_{y}f\right)(x)=e^{i2\pi \xi y}f(x)} $$ $$ ( x 1 , ξ 1 , t 1 ) ⋅ ( x 2 , ξ 2 , t 2 ) = ( x 1 + x 2 , ξ 1 + ξ 2 , t 1 t 2 e i 2 π ( x 1 ξ 1 + x 2 ξ 2 + x 1 ξ 2 ) ) . {\displaystyle \left(x_{1},\xi _{1},t_{1}\right)\cdot \left(x_{2},\xi _{2},t_{2}\right)=\left(x_{1}+x_{2},\xi _{1}+\xi _{2},t_{1}t_{2}e^{i2\pi \left(x_{1}\xi _{1}+x_{2}\xi _{2}+x_{1}\xi _{2}\right)}\right).} $$ Denote the Heisenberg group by H1. The above procedure describes not only the group structure, but also a standard unitary representation of H1 on a Hilbert space, which we denote by ρ : H1 → B(L2(R)). Define the linear automorphism of R2 by J ( x ξ ) = ( − ξ x ) J{\begin{pmatrixx\\\xi \end{pmatrix}}={\begin{pmatrix}-\xi \\x\end{pmatrix}}} so that J2 = −I. This J can be extended to a unique automorphism of H1: j ( x , ξ , t ) = ( − ξ , x , t e − i 2 π ξ x ) . j\left(x,\xi ,t\right)=\left(-\xi ,x,te^{-i2\pi \xi x\right).} $$ J ( x ξ ) = ( − ξ x ) {\displaystyle J{\begin{pmatrix}x\\\xi \end{pmatrix}}={\begin{pmatrix}-\xi \\x\end{pmatrix}}} $$ $$ j ( x , ξ , t ) = ( − ξ , x , t e − i 2 π ξ x ) . {\displaystyle j\left(x,\xi ,t\right)=\left(-\xi ,x,te^{-i2\pi \xi x}\right).} $$ According to the Stone–von Neumann theorem, the unitary representations ρ and ρ ∘ j are unitarily equivalent, so there is a unique intertwiner W ∈ U(L2(R)) such that ρ ∘ j = W ρ W ∗ . \rho \circ j=W\rho W^{*.} This operator W is the Fourier transform. $$ ρ ∘ j = W ρ W ∗ . {\displaystyle \rho \circ j=W\rho W^{*}.} $$ Many of the standard properties of the Fourier transform are immediate consequences of this more general framework. For example, the square of the Fourier transform, W2, is an intertwiner associated with J2 = −I, and so we have (W2f)(x) = f (−x) is the reflection of the original function f. The integral for the Fourier transform f ^ ( ξ ) = ∫ − ∞ ∞ e − i 2 π ξ t f ( t ) d t {\hat {f}(\xi )=\int _{-\infty }^{\infty }e^{-i2\pi \xi t}f(t)\,dt} can be studied for complex values of its argument ξ. Depending on the properties of f, this might not converge off the real axis at all, or it might converge to a complex analytic function for all values of ξ = σ + iτ, or something in between. $$ f ^ ( ξ ) = ∫ − ∞ ∞ e − i 2 π ξ t f ( t ) d t {\displaystyle {\hat {f}}(\xi )=\int _{-\infty }^{\infty }e^{-i2\pi \xi t}f(t)\,dt} $$ The Paley–Wiener theorem says that f is smooth (i.e., n-times differentiable for all positive integers n) and compactly supported if and only if f̂ (σ + iτ) is a holomorphic function for which there exists a constant a > 0 such that for any integer n ≥ 0, | ξ n f ^ ( ξ ) | ≤ C e a | τ | \left\vert \xi ^{n{\hat {f}}(\xi )\right\vert \leq Ce^{a\vert \tau \vert }} for some constant C. (In this case, f is supported on [−a, a].) This can be expressed by saying that f̂ is an entire function which is rapidly decreasing in σ (for fixed τ) and of exponential growth in τ (uniformly in σ). $$ | ξ n f ^ ( ξ ) | ≤ C e a | τ | {\displaystyle \left\vert \xi ^{n}{\hat {f}}(\xi )\right\vert \leq Ce^{a\vert \tau \vert }} $$ (If f is not smooth, but only L2, the statement still holds provided n = 0.) The space of such functions of a complex variable is called the Paley—Wiener space. This theorem has been generalised to semisimple Lie groups. If f is supported on the half-line t ≥ 0, then f is said to be "causal" because the impulse response function of a physically realisable filter must have this property, as no effect can precede its cause. Paley and Wiener showed that then f̂ extends to a holomorphic function on the complex lower half-plane τ < 0 which tends to zero as τ goes to infinity. The converse is false and it is not known how to characterise the Fourier transform of a causal function. The Fourier transform f̂(ξ) is related to the Laplace transform F(s), which is also used for the solution of differential equations and the analysis of filters. It may happen that a function f for which the Fourier integral does not converge on the real axis at all, nevertheless has a complex Fourier transform defined in some region of the complex plane. For example, if f(t) is of exponential growth, i.e., | f ( t ) | < C e a | t | \vert f(t)\vert <Ce^{a\vert t\vert } for some constants C, a ≥ 0, then f ^ ( i τ ) = ∫ − ∞ ∞ e 2 π τ t f ( t ) d t , {\hat {f}(i\tau )=\int _{-\infty }^{\infty }e^{2\pi \tau t}f(t)\,dt,} convergent for all 2πτ < −a, is the two-sided Laplace transform of f. $$ | f ( t ) | < C e a | t | {\displaystyle \vert f(t)\vert <Ce^{a\vert t\vert }} $$ $$ f ^ ( i τ ) = ∫ − ∞ ∞ e 2 π τ t f ( t ) d t , {\displaystyle {\hat {f}}(i\tau )=\int _{-\infty }^{\infty }e^{2\pi \tau t}f(t)\,dt,} $$ The more usual version ("one-sided") of the Laplace transform is F ( s ) = ∫ 0 ∞ f ( t ) e − s t d t . F(s)=\int _{0^{\infty }f(t)e^{-st}\,dt.} $$ F ( s ) = ∫ 0 ∞ f ( t ) e − s t d t . {\displaystyle F(s)=\int _{0}^{\infty }f(t)e^{-st}\,dt.} $$ If f is also causal, and analytical, then: f ^ ( i τ ) = F ( − 2 π τ ) . {\hat {f}(i\tau )=F(-2\pi \tau ).} Thus, extending the Fourier transform to the complex domain means it includes the Laplace transform as a special case in the case of causal functions—but with the change of variable s = i2πξ. $$ f ^ ( i τ ) = F ( − 2 π τ ) . {\displaystyle {\hat {f}}(i\tau )=F(-2\pi \tau ).} $$ From another, perhaps more classical viewpoint, the Laplace transform by its form involves an additional exponential regulating term which lets it converge outside of the imaginary line where the Fourier transform is defined. As such it can converge for at most exponentially divergent series and integrals, whereas the original Fourier decomposition cannot, enabling analysis of systems with divergent or critical elements. Two particular examples from linear signal processing are the construction of allpass filter networks from critical comb and mitigating filters via exact pole-zero cancellation on the unit circle. Such designs are common in audio processing, where highly nonlinear phase response is sought for, as in reverb. Furthermore, when extended pulselike impulse responses are sought for signal processing work, the easiest way to produce them is to have one circuit which produces a divergent time response, and then to cancel its divergence through a delayed opposite and compensatory response. There, only the delay circuit in-between admits a classical Fourier description, which is critical. Both the circuits to the side are unstable, and do not admit a convergent Fourier decomposition. However, they do admit a Laplace domain description, with identical half-planes of convergence in the complex plane (or in the discrete case, the Z-plane), wherein their effects cancel. In modern mathematics the Laplace transform is conventionally subsumed under the aegis Fourier methods. Both of them are subsumed by the far more general, and more abstract, idea of harmonic analysis. Still with ξ = σ + i τ \xi =\sigma +i\tau , if f ^ {\widehat {f}} is complex analytic for a ≤ τ ≤ b, then $$ ξ = σ + i τ {\displaystyle \xi =\sigma +i\tau } $$ $$ f ^ {\displaystyle {\widehat {f}}} $$ ∫ − ∞ ∞ f ^ ( σ + i a ) e i 2 π ξ t d σ = ∫ − ∞ ∞ f ^ ( σ + i b ) e i 2 π ξ t d σ \int _{-\infty ^{\infty }{\hat {f}}(\sigma +ia)e^{i2\pi \xi t}\,d\sigma =\int _{-\infty }^{\infty }{\hat {f}}(\sigma +ib)e^{i2\pi \xi t}\,d\sigma } by Cauchy's integral theorem. Therefore, the Fourier inversion formula can use integration along different lines, parallel to the real axis. $$ ∫ − ∞ ∞ f ^ ( σ + i a ) e i 2 π ξ t d σ = ∫ − ∞ ∞ f ^ ( σ + i b ) e i 2 π ξ t d σ {\displaystyle \int _{-\infty }^{\infty }{\hat {f}}(\sigma +ia)e^{i2\pi \xi t}\,d\sigma =\int _{-\infty }^{\infty }{\hat {f}}(\sigma +ib)e^{i2\pi \xi t}\,d\sigma } $$ Theorem: If f(t) = 0 for t < 0, and |f(t)| < Cea|t| for some constants C, a > 0, then f ( t ) = ∫ − ∞ ∞ f ^ ( σ + i τ ) e i 2 π ξ t d σ , f(t)=\int _{-\infty ^{\infty }{\hat {f}}(\sigma +i\tau )e^{i2\pi \xi t}\,d\sigma ,} for any τ < −⁠a/2π⁠. $$ f ( t ) = ∫ − ∞ ∞ f ^ ( σ + i τ ) e i 2 π ξ t d σ , {\displaystyle f(t)=\int _{-\infty }^{\infty }{\hat {f}}(\sigma +i\tau )e^{i2\pi \xi t}\,d\sigma ,} $$ This theorem implies the Mellin inversion formula for the Laplace transformation, f ( t ) = 1 i 2 π ∫ b − i ∞ b + i ∞ F ( s ) e s t d s f(t)={\frac {1{i2\pi }}\int _{b-i\infty }^{b+i\infty }F(s)e^{st}\,ds} for any b > a, where F(s) is the Laplace transform of f(t). $$ f ( t ) = 1 i 2 π ∫ b − i ∞ b + i ∞ F ( s ) e s t d s {\displaystyle f(t)={\frac {1}{i2\pi }}\int _{b-i\infty }^{b+i\infty }F(s)e^{st}\,ds} $$ The hypotheses can be weakened, as in the results of Carleson and Hunt, to f(t) e−at being L1, provided that f be of bounded variation in a closed neighborhood of t (cf. Dini test), the value of f at t be taken to be the arithmetic mean of the left and right limits, and that the integrals be taken in the sense of Cauchy principal values. L2 versions of these inversion formulas are also available. The Fourier transform can be defined in any arbitrary number of dimensions n. As with the one-dimensional case, there are many conventions. For an integrable function f(x), this article takes the definition: f ^ ( ξ ) = F ( f ) ( ξ ) = ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x {\hat {f}({\boldsymbol {\xi }})={\mathcal {F}}(f)({\boldsymbol {\xi }})=\int _{\mathbb {R} ^{n}}f(\mathbf {x} )e^{-i2\pi {\boldsymbol {\xi }}\cdot \mathbf {x} }\,d\mathbf {x} } where x and ξ are n-dimensional vectors, and x · ξ is the dot product of the vectors. Alternatively, ξ can be viewed as belonging to the dual vector space R n ⋆ \mathbb {R ^{n\star }} , in which case the dot product becomes the contraction of x and ξ, usually written as ⟨x, ξ⟩. $$ f ^ ( ξ ) = F ( f ) ( ξ ) = ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x {\displaystyle {\hat {f}}({\boldsymbol {\xi }})={\mathcal {F}}(f)({\boldsymbol {\xi }})=\int _{\mathbb {R} ^{n}}f(\mathbf {x} )e^{-i2\pi {\boldsymbol {\xi }}\cdot \mathbf {x} }\,d\mathbf {x} } $$ $$ R n ⋆ {\displaystyle \mathbb {R} ^{n\star }} $$ All of the basic properties listed above hold for the n-dimensional Fourier transform, as do Plancherel's and Parseval's theorem. When the function is integrable, the Fourier transform is still uniformly continuous and the Riemann–Lebesgue lemma holds. Generally speaking, the more concentrated f(x) is, the more spread out its Fourier transform f̂(ξ) must be. In particular, the scaling property of the Fourier transform may be seen as saying: if we squeeze a function in x, its Fourier transform stretches out in ξ. It is not possible to arbitrarily concentrate both a function and its Fourier transform. The trade-off between the compaction of a function and its Fourier transform can be formalized in the form of an uncertainty principle by viewing a function and its Fourier transform as conjugate variables with respect to the symplectic form on the time–frequency domain: from the point of view of the linear canonical transformation, the Fourier transform is rotation by 90° in the time–frequency domain, and preserves the symplectic form. Suppose f(x) is an integrable and square-integrable function. Without loss of generality, assume that f(x) is normalized: ∫ − ∞ ∞ | f ( x ) | 2 d x = 1. \int _{-\infty ^{\infty }|f(x)|^{2}\,dx=1.} $$ ∫ − ∞ ∞ | f ( x ) | 2 d x = 1. {\displaystyle \int _{-\infty }^{\infty }|f(x)|^{2}\,dx=1.} $$ It follows from the Plancherel theorem that f̂(ξ) is also normalized. The spread around x = 0 may be measured by the dispersion about zero defined by D 0 ( f ) = ∫ − ∞ ∞ x 2 | f ( x ) | 2 d x . D_{0(f)=\int _{-\infty }^{\infty }x^{2}|f(x)|^{2}\,dx.} $$ D 0 ( f ) = ∫ − ∞ ∞ x 2 | f ( x ) | 2 d x . {\displaystyle D_{0}(f)=\int _{-\infty }^{\infty }x^{2}|f(x)|^{2}\,dx.} $$ In probability terms, this is the second moment of |f(x)|2 about zero. The uncertainty principle states that, if f(x) is absolutely continuous and the functions x·f(x) and f′(x) are square integrable, then D 0 ( f ) D 0 ( f ^ ) ≥ 1 16 π 2 . D_{0(f)D_{0}({\hat {f}})\geq {\frac {1}{16\pi ^{2}}}.} $$ D 0 ( f ) D 0 ( f ^ ) ≥ 1 16 π 2 . {\displaystyle D_{0}(f)D_{0}({\hat {f}})\geq {\frac {1}{16\pi ^{2}}}.} $$ The equality is attained only in the case f ( x ) = C 1 e − π x 2 σ 2 ∴ f ^ ( ξ ) = σ C 1 e − π σ 2 ξ 2 {\begin{alignedf(x)&=C_{1}\,e^{-\pi {\frac {x^{2}}{\sigma ^{2}}}}\\\therefore {\hat {f}}(\xi )&=\sigma C_{1}\,e^{-\pi \sigma ^{2}\xi ^{2}}\end{aligned}}} where σ > 0 is arbitrary and C1 = ⁠4√2/√σ⁠ so that f is L2-normalized. In other words, where f is a (normalized) Gaussian function with variance σ2/2π, centered at zero, and its Fourier transform is a Gaussian function with variance σ−2/2π. $$ f ( x ) = C 1 e − π x 2 σ 2 ∴ f ^ ( ξ ) = σ C 1 e − π σ 2 ξ 2 {\displaystyle {\begin{aligned}f(x)&=C_{1}\,e^{-\pi {\frac {x^{2}}{\sigma ^{2}}}}\\\therefore {\hat {f}}(\xi )&=\sigma C_{1}\,e^{-\pi \sigma ^{2}\xi ^{2}}\end{aligned}}} $$ In fact, this inequality implies that: ( ∫ − ∞ ∞ ( x − x 0 ) 2 | f ( x ) | 2 d x ) ( ∫ − ∞ ∞ ( ξ − ξ 0 ) 2 | f ^ ( ξ ) | 2 d ξ ) ≥ 1 16 π 2 \left(\int _{-\infty ^{\infty }(x-x_{0})^{2}|f(x)|^{2}\,dx\right)\left(\int _{-\infty }^{\infty }(\xi -\xi _{0})^{2}\left|{\hat {f}}(\xi )\right|^{2}\,d\xi \right)\geq {\frac {1}{16\pi ^{2}}}} for any x0, ξ0 ∈ R. $$ ( ∫ − ∞ ∞ ( x − x 0 ) 2 | f ( x ) | 2 d x ) ( ∫ − ∞ ∞ ( ξ − ξ 0 ) 2 | f ^ ( ξ ) | 2 d ξ ) ≥ 1 16 π 2 {\displaystyle \left(\int _{-\infty }^{\infty }(x-x_{0})^{2}|f(x)|^{2}\,dx\right)\left(\int _{-\infty }^{\infty }(\xi -\xi _{0})^{2}\left|{\hat {f}}(\xi )\right|^{2}\,d\xi \right)\geq {\frac {1}{16\pi ^{2}}}} $$ In quantum mechanics, the momentum and position wave functions are Fourier transform pairs, up to a factor of the Planck constant. With this constant properly taken into account, the inequality above becomes the statement of the Heisenberg uncertainty principle. A stronger uncertainty principle is the Hirschman uncertainty principle, which is expressed as: H ( | f | 2 ) + H ( | f ^ | 2 ) ≥ log ⁡ ( e 2 ) H\left(\left|f\right|^{2\right)+H\left(\left|{\hat {f}}\right|^{2}\right)\geq \log \left({\frac {e}{2}}\right)} where H(p) is the differential entropy of the probability density function p(x): H ( p ) = − ∫ − ∞ ∞ p ( x ) log ⁡ ( p ( x ) ) d x H(p)=-\int _{-\infty ^{\infty }p(x)\log {\bigl (}p(x){\bigr )}\,dx} where the logarithms may be in any base that is consistent. The equality is attained for a Gaussian, as in the previous case. $$ H ( | f | 2 ) + H ( | f ^ | 2 ) ≥ log ⁡ ( e 2 ) {\displaystyle H\left(\left|f\right|^{2}\right)+H\left(\left|{\hat {f}}\right|^{2}\right)\geq \log \left({\frac {e}{2}}\right)} $$ $$ H ( p ) = − ∫ − ∞ ∞ p ( x ) log ⁡ ( p ( x ) ) d x {\displaystyle H(p)=-\int _{-\infty }^{\infty }p(x)\log {\bigl (}p(x){\bigr )}\,dx} $$ Fourier's original formulation of the transform did not use complex numbers, but rather sines and cosines. Statisticians and others still use this form. An absolutely integrable function f for which Fourier inversion holds can be expanded in terms of genuine frequencies (avoiding negative frequencies, which are sometimes considered hard to interpret physically) λ by f ( t ) = ∫ 0 ∞ ( a ( λ ) cos ⁡ ( 2 π λ t ) + b ( λ ) sin ⁡ ( 2 π λ t ) ) d λ . f(t)=\int _{0^{\infty }{\bigl (}a(\lambda )\cos(2\pi \lambda t)+b(\lambda )\sin(2\pi \lambda t){\bigr )}\,d\lambda .} $$ f ( t ) = ∫ 0 ∞ ( a ( λ ) cos ⁡ ( 2 π λ t ) + b ( λ ) sin ⁡ ( 2 π λ t ) ) d λ . {\displaystyle f(t)=\int _{0}^{\infty }{\bigl (}a(\lambda )\cos(2\pi \lambda t)+b(\lambda )\sin(2\pi \lambda t){\bigr )}\,d\lambda .} $$ This is called an expansion as a trigonometric integral, or a Fourier integral expansion. The coefficient functions a and b can be found by using variants of the Fourier cosine transform and the Fourier sine transform (the normalisations are, again, not standardised): a ( λ ) = 2 ∫ − ∞ ∞ f ( t ) cos ⁡ ( 2 π λ t ) d t a(\lambda )=2\int _{-\infty ^{\infty }f(t)\cos(2\pi \lambda t)\,dt} and b ( λ ) = 2 ∫ − ∞ ∞ f ( t ) sin ⁡ ( 2 π λ t ) d t . b(\lambda )=2\int _{-\infty ^{\infty }f(t)\sin(2\pi \lambda t)\,dt.} $$ a ( λ ) = 2 ∫ − ∞ ∞ f ( t ) cos ⁡ ( 2 π λ t ) d t {\displaystyle a(\lambda )=2\int _{-\infty }^{\infty }f(t)\cos(2\pi \lambda t)\,dt} $$ $$ b ( λ ) = 2 ∫ − ∞ ∞ f ( t ) sin ⁡ ( 2 π λ t ) d t . {\displaystyle b(\lambda )=2\int _{-\infty }^{\infty }f(t)\sin(2\pi \lambda t)\,dt.} $$ Older literature refers to the two transform functions, the Fourier cosine transform, a, and the Fourier sine transform, b. The function f can be recovered from the sine and cosine transform using f ( t ) = 2 ∫ 0 ∞ ∫ − ∞ ∞ f ( τ ) cos ⁡ ( 2 π λ ( τ − t ) ) d τ d λ . f(t)=2\int _{0^{\infty }\int _{-\infty }^{\infty }f(\tau )\cos {\bigl (}2\pi \lambda (\tau -t){\bigr )}\,d\tau \,d\lambda .} together with trigonometric identities. This is referred to as Fourier's integral formula. $$ f ( t ) = 2 ∫ 0 ∞ ∫ − ∞ ∞ f ( τ ) cos ⁡ ( 2 π λ ( τ − t ) ) d τ d λ . {\displaystyle f(t)=2\int _{0}^{\infty }\int _{-\infty }^{\infty }f(\tau )\cos {\bigl (}2\pi \lambda (\tau -t){\bigr )}\,d\tau \,d\lambda .} $$ Let the set of homogeneous harmonic polynomials of degree k on Rn be denoted by Ak. The set Ak consists of the solid spherical harmonics of degree k. The solid spherical harmonics play a similar role in higher dimensions to the Hermite polynomials in dimension one. Specifically, if f(x) = e−π|x|2P(x) for some P(x) in Ak, then f̂(ξ) = i−k f(ξ). Let the set Hk be the closure in L2(Rn) of linear combinations of functions of the form f(|x|)P(x) where P(x) is in Ak. The space L2(Rn) is then a direct sum of the spaces Hk and the Fourier transform maps each space Hk to itself and is possible to characterize the action of the Fourier transform on each space Hk. Let f(x) = f0(|x|)P(x) (with P(x) in Ak), then f ^ ( ξ ) = F 0 ( | ξ | ) P ( ξ ) {\hat {f}(\xi )=F_{0}(|\xi |)P(\xi )} where F 0 ( r ) = 2 π i − k r − n + 2 k − 2 2 ∫ 0 ∞ f 0 ( s ) J n + 2 k − 2 2 ( 2 π r s ) s n + 2 k 2 d s . F_{0(r)=2\pi i^{-k}r^{-{\frac {n+2k-2}{2}}}\int _{0}^{\infty }f_{0}(s)J_{\frac {n+2k-2}{2}}(2\pi rs)s^{\frac {n+2k}{2}}\,ds.} $$ f ^ ( ξ ) = F 0 ( | ξ | ) P ( ξ ) {\displaystyle {\hat {f}}(\xi )=F_{0}(|\xi |)P(\xi )} $$ $$ F 0 ( r ) = 2 π i − k r − n + 2 k − 2 2 ∫ 0 ∞ f 0 ( s ) J n + 2 k − 2 2 ( 2 π r s ) s n + 2 k 2 d s . {\displaystyle F_{0}(r)=2\pi i^{-k}r^{-{\frac {n+2k-2}{2}}}\int _{0}^{\infty }f_{0}(s)J_{\frac {n+2k-2}{2}}(2\pi rs)s^{\frac {n+2k}{2}}\,ds.} $$ Here J(n + 2k − 2)/2 denotes the Bessel function of the first kind with order ⁠n + 2k − 2/2⁠. When k = 0 this gives a useful formula for the Fourier transform of a radial function. This is essentially the Hankel transform. Moreover, there is a simple recursion relating the cases n + 2 and n allowing to compute, e.g., the three-dimensional Fourier transform of a radial function from the one-dimensional one. In higher dimensions it becomes interesting to study restriction problems for the Fourier transform. The Fourier transform of an integrable function is continuous and the restriction of this function to any set is defined. But for a square-integrable function the Fourier transform could be a general class of square integrable functions. As such, the restriction of the Fourier transform of an L2(Rn) function cannot be defined on sets of measure 0. It is still an active area of study to understand restriction problems in Lp for 1 < p < 2. It is possible in some cases to define the restriction of a Fourier transform to a set S, provided S has non-zero curvature. The case when S is the unit sphere in Rn is of particular interest. In this case the Tomas–Stein restriction theorem states that the restriction of the Fourier transform to the unit sphere in Rn is a bounded operator on Lp provided 1 ≤ p ≤ ⁠2n + 2/n + 3⁠. One notable difference between the Fourier transform in 1 dimension versus higher dimensions concerns the partial sum operator. Consider an increasing collection of measurable sets ER indexed by R ∈ (0,∞): such as balls of radius R centered at the origin, or cubes of side 2R. For a given integrable function f, consider the function fR defined by: f R ( x ) = ∫ E R f ^ ( ξ ) e i 2 π x ⋅ ξ d ξ , x ∈ R n . f_{R(x)=\int _{E_{R}}{\hat {f}}(\xi )e^{i2\pi x\cdot \xi }\,d\xi ,\quad x\in \mathbb {R} ^{n}.} $$ f R ( x ) = ∫ E R f ^ ( ξ ) e i 2 π x ⋅ ξ d ξ , x ∈ R n . {\displaystyle f_{R}(x)=\int _{E_{R}}{\hat {f}}(\xi )e^{i2\pi x\cdot \xi }\,d\xi ,\quad x\in \mathbb {R} ^{n}.} $$ Suppose in addition that f ∈ Lp(Rn). For n = 1 and 1 < p < ∞, if one takes ER = (−R, R), then fR converges to f in Lp as R tends to infinity, by the boundedness of the Hilbert transform. Naively one may hope the same holds true for n > 1. In the case that ER is taken to be a cube with side length R, then convergence still holds. Another natural candidate is the Euclidean ball ER = {ξ : |ξ| < R}. In order for this partial sum operator to converge, it is necessary that the multiplier for the unit ball be bounded in Lp(Rn). For n ≥ 2 it is a celebrated theorem of Charles Fefferman that the multiplier for the unit ball is never bounded unless p = 2. In fact, when p ≠ 2, this shows that not only may fR fail to converge to f in Lp, but for some functions f ∈ Lp(Rn), fR is not even an element of Lp. The definition of the Fourier transform by the integral formula f ^ ( ξ ) = ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x {\hat {f}(\xi )=\int _{\mathbb {R} ^{n}}f(x)e^{-i2\pi \xi \cdot x}\,dx} is valid for Lebesgue integrable functions f; that is, f ∈ L1(Rn). $$ f ^ ( ξ ) = ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x {\displaystyle {\hat {f}}(\xi )=\int _{\mathbb {R} ^{n}}f(x)e^{-i2\pi \xi \cdot x}\,dx} $$ The Fourier transform F : L1(Rn) → L∞(Rn) is a bounded operator. This follows from the observation that | f ^ ( ξ ) | ≤ ∫ R n | f ( x ) | d x , \left\vert {\hat {f}(\xi )\right\vert \leq \int _{\mathbb {R} ^{n}}\vert f(x)\vert \,dx,} which shows that its operator norm is bounded by 1. Indeed, it equals 1, which can be seen, for example, from the transform of the rect function. The image of L1 is a subset of the space C0(Rn) of continuous functions that tend to zero at infinity (the Riemann–Lebesgue lemma), although it is not the entire space. Indeed, there is no simple characterization of the image. $$ | f ^ ( ξ ) | ≤ ∫ R n | f ( x ) | d x , {\displaystyle \left\vert {\hat {f}}(\xi )\right\vert \leq \int _{\mathbb {R} ^{n}}\vert f(x)\vert \,dx,} $$ Since compactly supported smooth functions are integrable and dense in L2(Rn), the Plancherel theorem allows one to extend the definition of the Fourier transform to general functions in L2(Rn) by continuity arguments. The Fourier transform in L2(Rn) is no longer given by an ordinary Lebesgue integral, although it can be computed by an improper integral, here meaning that for an L2 function f, f ^ ( ξ ) = lim R → ∞ ∫ | x | ≤ R f ( x ) e − i 2 π ξ ⋅ x d x {\hat {f}(\xi )=\lim _{R\to \infty }\int _{|x|\leq R}f(x)e^{-i2\pi \xi \cdot x}\,dx} where the limit is taken in the L2 sense.) $$ f ^ ( ξ ) = lim R → ∞ ∫ | x | ≤ R f ( x ) e − i 2 π ξ ⋅ x d x {\displaystyle {\hat {f}}(\xi )=\lim _{R\to \infty }\int _{|x|\leq R}f(x)e^{-i2\pi \xi \cdot x}\,dx} $$ Many of the properties of the Fourier transform in L1 carry over to L2, by a suitable limiting argument. Furthermore, F : L2(Rn) → L2(Rn) is a unitary operator. For an operator to be unitary it is sufficient to show that it is bijective and preserves the inner product, so in this case these follow from the Fourier inversion theorem combined with the fact that for any f, g ∈ L2(Rn) we have ∫ R n f ( x ) F g ( x ) d x = ∫ R n F f ( x ) g ( x ) d x . \int _{\mathbb {R ^{n}}f(x){\mathcal {F}}g(x)\,dx=\int _{\mathbb {R} ^{n}}{\mathcal {F}}f(x)g(x)\,dx.} $$ ∫ R n f ( x ) F g ( x ) d x = ∫ R n F f ( x ) g ( x ) d x . {\displaystyle \int _{\mathbb {R} ^{n}}f(x){\mathcal {F}}g(x)\,dx=\int _{\mathbb {R} ^{n}}{\mathcal {F}}f(x)g(x)\,dx.} $$ In particular, the image of L2(Rn) is itself under the Fourier transform. The definition of the Fourier transform can be extended to functions in Lp(Rn) for 1 ≤ p ≤ 2 by decomposing such functions into a fat tail part in L2 plus a fat body part in L1. In each of these spaces, the Fourier transform of a function in Lp(Rn) is in Lq(Rn), where q = ⁠p/p − 1⁠ is the Hölder conjugate of p (by the Hausdorff–Young inequality). However, except for p = 2, the image is not easily characterized. Further extensions become more technical. The Fourier transform of functions in Lp for the range 2 < p < ∞ requires the study of distributions. In fact, it can be shown that there are functions in Lp with p > 2 so that the Fourier transform is not defined as a function. One might consider enlarging the domain of the Fourier transform from L1 + L2 by considering generalized functions, or distributions. A distribution on Rn is a continuous linear functional on the space Cc(Rn) of compactly supported smooth functions, equipped with a suitable topology. The strategy is then to consider the action of the Fourier transform on Cc(Rn) and pass to distributions by duality. The obstruction to doing this is that the Fourier transform does not map Cc(Rn) to Cc(Rn). In fact the Fourier transform of an element in Cc(Rn) can not vanish on an open set; see the above discussion on the uncertainty principle. The right space here is the slightly larger space of Schwartz functions. The Fourier transform is an automorphism on the Schwartz space, as a topological vector space, and thus induces an automorphism on its dual, the space of tempered distributions. The tempered distributions include all the integrable functions mentioned above, as well as well-behaved functions of polynomial growth and distributions of compact support. For the definition of the Fourier transform of a tempered distribution, let f and g be integrable functions, and let f̂ and ĝ be their Fourier transforms respectively. Then the Fourier transform obeys the following multiplication formula, ∫ R n f ^ ( x ) g ( x ) d x = ∫ R n f ( x ) g ^ ( x ) d x . \int _{\mathbb {R ^{n}}{\hat {f}}(x)g(x)\,dx=\int _{\mathbb {R} ^{n}}f(x){\hat {g}}(x)\,dx.} $$ ∫ R n f ^ ( x ) g ( x ) d x = ∫ R n f ( x ) g ^ ( x ) d x . {\displaystyle \int _{\mathbb {R} ^{n}}{\hat {f}}(x)g(x)\,dx=\int _{\mathbb {R} ^{n}}f(x){\hat {g}}(x)\,dx.} $$ Every integrable function f defines (induces) a distribution Tf by the relation T f ( φ ) = ∫ R n f ( x ) φ ( x ) d x T_{f(\varphi )=\int _{\mathbb {R} ^{n}}f(x)\varphi (x)\,dx} for all Schwartz functions φ. So it makes sense to define Fourier transform T̂f of Tf by T ^ f ( φ ) = T f ( φ ^ ) {\hat {T}_{f}(\varphi )=T_{f}\left({\hat {\varphi }}\right)} for all Schwartz functions φ. Extending this to all tempered distributions T gives the general definition of the Fourier transform. $$ T f ( φ ) = ∫ R n f ( x ) φ ( x ) d x {\displaystyle T_{f}(\varphi )=\int _{\mathbb {R} ^{n}}f(x)\varphi (x)\,dx} $$ $$ T ^ f ( φ ) = T f ( φ ^ ) {\displaystyle {\hat {T}}_{f}(\varphi )=T_{f}\left({\hat {\varphi }}\right)} $$ Distributions can be differentiated and the above-mentioned compatibility of the Fourier transform with differentiation and convolution remains true for tempered distributions. The Fourier transform of a finite Borel measure μ on Rn is given by: μ ^ ( ξ ) = ∫ R n e − i 2 π x ⋅ ξ d μ . {\hat {\mu }(\xi )=\int _{\mathbb {R} ^{n}}e^{-i2\pi x\cdot \xi }\,d\mu .} $$ μ ^ ( ξ ) = ∫ R n e − i 2 π x ⋅ ξ d μ . {\displaystyle {\hat {\mu }}(\xi )=\int _{\mathbb {R} ^{n}}e^{-i2\pi x\cdot \xi }\,d\mu .} $$ This transform continues to enjoy many of the properties of the Fourier transform of integrable functions. One notable difference is that the Riemann–Lebesgue lemma fails for measures. In the case that dμ = f(x) dx, then the formula above reduces to the usual definition for the Fourier transform of f. In the case that μ is the probability distribution associated to a random variable X, the Fourier–Stieltjes transform is closely related to the characteristic function, but the typical conventions in probability theory take eiξx instead of e−i2πξx. In the case when the distribution has a probability density function this definition reduces to the Fourier transform applied to the probability density function, again with a different choice of constants. The Fourier transform may be used to give a characterization of measures. Bochner's theorem characterizes which functions may arise as the Fourier–Stieltjes transform of a positive measure on the circle. Furthermore, the Dirac delta function, although not a function, is a finite Borel measure. Its Fourier transform is a constant function (whose specific value depends upon the form of the Fourier transform used). The Fourier transform may be generalized to any locally compact abelian group. A locally compact abelian group is an abelian group that is at the same time a locally compact Hausdorff topological space so that the group operation is continuous. If G is a locally compact abelian group, it has a translation invariant measure μ, called Haar measure. For a locally compact abelian group G, the set of irreducible, i.e. one-dimensional, unitary representations are called its characters. With its natural group structure and the topology of uniform convergence on compact sets (that is, the topology induced by the compact-open topology on the space of all continuous functions from G G to the circle group), the set of characters Ĝ is itself a locally compact abelian group, called the Pontryagin dual of G. For a function f in L1(G), its Fourier transform is defined by f ^ ( ξ ) = ∫ G ξ ( x ) f ( x ) d μ for any ξ ∈ G ^ . {\hat {f}(\xi )=\int _{G}\xi (x)f(x)\,d\mu \quad {\text{for any }}\xi \in {\hat {G}}.} $$ G {\displaystyle G} $$ $$ f ^ ( ξ ) = ∫ G ξ ( x ) f ( x ) d μ for any ξ ∈ G ^ . {\displaystyle {\hat {f}}(\xi )=\int _{G}\xi (x)f(x)\,d\mu \quad {\text{for any }}\xi \in {\hat {G}}.} $$ The Riemann–Lebesgue lemma holds in this case; f̂(ξ) is a function vanishing at infinity on Ĝ. The Fourier transform on T = R/Z is an example; here T is a locally compact abelian group, and the Haar measure μ on T can be thought of as the Lebesgue measure on [0,1). Consider the representation of T on the complex plane C that is a 1-dimensional complex vector space. There are a group of representations (which are irreducible since C is 1-dim) { e k : T → G L 1 ( C ) = C ∗ ∣ k ∈ Z } \{e_{k:T\rightarrow GL_{1}(C)=C^{*}\mid k\in Z\}} where e k ( x ) = e i 2 π k x e_{k(x)=e^{i2\pi kx}} for x ∈ T x\in T . $$ { e k : T → G L 1 ( C ) = C ∗ ∣ k ∈ Z } {\displaystyle \{e_{k}:T\rightarrow GL_{1}(C)=C^{*}\mid k\in Z\}} $$ $$ e k ( x ) = e i 2 π k x {\displaystyle e_{k}(x)=e^{i2\pi kx}} $$ $$ x ∈ T {\displaystyle x\in T} $$ The character of such representation, that is the trace of e k ( x ) e_{k(x)} for each x ∈ T x\in T and k ∈ Z k\in Z , is e i 2 π k x e^{i2\pi kx} itself. In the case of representation of finite group, the character table of the group G are rows of vectors such that each row is the character of one irreducible representation of G, and these vectors form an orthonormal basis of the space of class functions that map from G to C by Schur's lemma. Now the group T is no longer finite but still compact, and it preserves the orthonormality of character table. Each row of the table is the function e k ( x ) e_{k(x)} of x ∈ T , x\in T, and the inner product between two class functions (all functions being class functions since T is abelian) f , g ∈ L 2 ( T , d μ ) f,g\in L^{2(T,d\mu )} is defined as ⟨ f , g ⟩ = 1 | T | ∫ [ 0 , 1 ) f ( y ) g ¯ ( y ) d μ ( y ) {\textstyle \langle f,g\rangle ={\frac {1}{|T|}}\int _{[0,1)}f(y){\overline {g}}(y)d\mu (y)} with the normalizing factor | T | = 1 |T|=1 . The sequence { e k ∣ k ∈ Z } \{e_{k\mid k\in Z\}} is an orthonormal basis of the space of class functions L 2 ( T , d μ ) L^{2(T,d\mu )} . $$ e k ( x ) {\displaystyle e_{k}(x)} $$ $$ x ∈ T {\displaystyle x\in T} $$ $$ k ∈ Z {\displaystyle k\in Z} $$ $$ e i 2 π k x {\displaystyle e^{i2\pi kx}} $$ $$ e k ( x ) {\displaystyle e_{k}(x)} $$ $$ x ∈ T , {\displaystyle x\in T,} $$ $$ f , g ∈ L 2 ( T , d μ ) {\displaystyle f,g\in L^{2}(T,d\mu )} $$ $$ ⟨ f , g ⟩ = 1 | T | ∫ [ 0 , 1 ) f ( y ) g ¯ ( y ) d μ ( y ) {\textstyle \langle f,g\rangle ={\frac {1}{|T|}}\int _{[0,1)}f(y){\overline {g}}(y)d\mu (y)} $$ $$ | T | = 1 {\displaystyle |T|=1} $$ $$ { e k ∣ k ∈ Z } {\displaystyle \{e_{k}\mid k\in Z\}} $$ $$ L 2 ( T , d μ ) {\displaystyle L^{2}(T,d\mu )} $$ For any representation V of a finite group G, χ v \chi _{v} can be expressed as the span ∑ i ⟨ χ v , χ v i ⟩ χ v i {\textstyle \sum _{i}\left\langle \chi _{v},\chi _{v_{i}}\right\rangle \chi _{v_{i}}} ( V i V_{i} are the irreps of G), such that ⟨ χ v , χ v i ⟩ = 1 | G | ∑ g ∈ G χ v ( g ) χ ¯ v i ( g ) {\textstyle \left\langle \chi _{v},\chi _{v_{i}}\right\rangle ={\frac {1}{|G|}}\sum _{g\in G}\chi _{v}(g){\overline {\chi }}_{v_{i}}(g)} . Similarly for G = T G=T and f ∈ L 2 ( T , d μ ) f\in L^{2(T,d\mu )} , f ( x ) = ∑ k ∈ Z f ^ ( k ) e k {\textstyle f(x)=\sum _{k\in Z}{\hat {f}}(k)e_{k}} . The Pontriagin dual T ^ {\hat {T}} is { e k } ( k ∈ Z ) \{e_{k\}(k\in Z)} and for f ∈ L 2 ( T , d μ ) f\in L^{2(T,d\mu )} , f ^ ( k ) = 1 | T | ∫ [ 0 , 1 ) f ( y ) e − i 2 π k y d y {\textstyle {\hat {f}}(k)={\frac {1}{|T|}}\int _{[0,1)}f(y)e^{-i2\pi ky}dy} is its Fourier transform for e k ∈ T ^ e_{k\in {\hat {T}}} . $$ χ v {\displaystyle \chi _{v}} $$ $$ ∑ i ⟨ χ v , χ v i ⟩ χ v i {\textstyle \sum _{i}\left\langle \chi _{v},\chi _{v_{i}}\right\rangle \chi _{v_{i}}} $$ $$ V i {\displaystyle V_{i}} $$ $$ ⟨ χ v , χ v i ⟩ = 1 | G | ∑ g ∈ G χ v ( g ) χ ¯ v i ( g ) {\textstyle \left\langle \chi _{v},\chi _{v_{i}}\right\rangle ={\frac {1}{|G|}}\sum _{g\in G}\chi _{v}(g){\overline {\chi }}_{v_{i}}(g)} $$ $$ G = T {\displaystyle G=T} $$ $$ f ∈ L 2 ( T , d μ ) {\displaystyle f\in L^{2}(T,d\mu )} $$ $$ f ( x ) = ∑ k ∈ Z f ^ ( k ) e k {\textstyle f(x)=\sum _{k\in Z}{\hat {f}}(k)e_{k}} $$ $$ T ^ {\displaystyle {\hat {T}}} $$ $$ { e k } ( k ∈ Z ) {\displaystyle \{e_{k}\}(k\in Z)} $$ $$ f ∈ L 2 ( T , d μ ) {\displaystyle f\in L^{2}(T,d\mu )} $$ $$ f ^ ( k ) = 1 | T | ∫ [ 0 , 1 ) f ( y ) e − i 2 π k y d y {\textstyle {\hat {f}}(k)={\frac {1}{|T|}}\int _{[0,1)}f(y)e^{-i2\pi ky}dy} $$ $$ e k ∈ T ^ {\displaystyle e_{k}\in {\hat {T}}} $$ The Fourier transform is also a special case of Gelfand transform. In this particular context, it is closely related to the Pontryagin duality map defined above. Given an abelian locally compact Hausdorff topological group G, as before we consider space L1(G), defined using a Haar measure. With convolution as multiplication, L1(G) is an abelian Banach algebra. It also has an involution * given by f ∗ ( g ) = f ( g − 1 ) ¯ . f^{*(g)={\overline {f\left(g^{-1}\right)}}.} $$ f ∗ ( g ) = f ( g − 1 ) ¯ . {\displaystyle f^{*}(g)={\overline {f\left(g^{-1}\right)}}.} $$ Taking the completion with respect to the largest possibly C*-norm gives its enveloping C*-algebra, called the group C*-algebra C*(G) of G. (Any C*-norm on L1(G) is bounded by the L1 norm, therefore their supremum exists.) Given any abelian C*-algebra A, the Gelfand transform gives an isomorphism between A and C0(A^), where A^ is the multiplicative linear functionals, i.e. one-dimensional representations, on A with the weak-* topology. The map is simply given by a ↦ ( φ ↦ φ ( a ) ) a\mapsto {\bigl (\varphi \mapsto \varphi (a){\bigr )}} It turns out that the multiplicative linear functionals of C*(G), after suitable identification, are exactly the characters of G, and the Gelfand transform, when restricted to the dense subset L1(G) is the Fourier–Pontryagin transform. $$ a ↦ ( φ ↦ φ ( a ) ) {\displaystyle a\mapsto {\bigl (}\varphi \mapsto \varphi (a){\bigr )}} $$ The Fourier transform can also be defined for functions on a non-abelian group, provided that the group is compact. Removing the assumption that the underlying group is abelian, irreducible unitary representations need not always be one-dimensional. This means the Fourier transform on a non-abelian group takes values as Hilbert space operators. The Fourier transform on compact groups is a major tool in representation theory and non-commutative harmonic analysis. Let G be a compact Hausdorff topological group. Let Σ denote the collection of all isomorphism classes of finite-dimensional irreducible unitary representations, along with a definite choice of representation U(σ) on the Hilbert space Hσ of finite dimension dσ for each σ ∈ Σ. If μ is a finite Borel measure on G, then the Fourier–Stieltjes transform of μ is the operator on Hσ defined by ⟨ μ ^ ξ , η ⟩ H σ = ∫ G ⟨ U ¯ g ( σ ) ξ , η ⟩ d μ ( g ) \left\langle {\hat {\mu }\xi ,\eta \right\rangle _{H_{\sigma }}=\int _{G}\left\langle {\overline {U}}_{g}^{(\sigma )}\xi ,\eta \right\rangle \,d\mu (g)} where U(σ) is the complex-conjugate representation of U(σ) acting on Hσ. If μ is absolutely continuous with respect to the left-invariant probability measure λ on G, represented as d μ = f d λ d\mu =f\,d\lambda for some f ∈ L1(λ), one identifies the Fourier transform of f with the Fourier–Stieltjes transform of μ. $$ ⟨ μ ^ ξ , η ⟩ H σ = ∫ G ⟨ U ¯ g ( σ ) ξ , η ⟩ d μ ( g ) {\displaystyle \left\langle {\hat {\mu }}\xi ,\eta \right\rangle _{H_{\sigma }}=\int _{G}\left\langle {\overline {U}}_{g}^{(\sigma )}\xi ,\eta \right\rangle \,d\mu (g)} $$ $$ d μ = f d λ {\displaystyle d\mu =f\,d\lambda } $$ The mapping μ ↦ μ ^ \mu \mapsto {\hat {\mu }} defines an isomorphism between the Banach space M(G) of finite Borel measures (see rca space) and a closed subspace of the Banach space C∞(Σ) consisting of all sequences E = (Eσ) indexed by Σ of (bounded) linear operators Eσ : Hσ → Hσ for which the norm ‖ E ‖ = sup σ ∈ Σ ‖ E σ ‖ \|E\|=\sup _{\sigma \in \Sigma \left\|E_{\sigma }\right\|} is finite. The "convolution theorem" asserts that, furthermore, this isomorphism of Banach spaces is in fact an isometric isomorphism of C*-algebras into a subspace of C∞(Σ). Multiplication on M(G) is given by convolution of measures and the involution * defined by f ∗ ( g ) = f ( g − 1 ) ¯ , f^{*(g)={\overline {f\left(g^{-1}\right)}},} and C∞(Σ) has a natural C*-algebra structure as Hilbert space operators. $$ μ ↦ μ ^ {\displaystyle \mu \mapsto {\hat {\mu }}} $$ $$ ‖ E ‖ = sup σ ∈ Σ ‖ E σ ‖ {\displaystyle \|E\|=\sup _{\sigma \in \Sigma }\left\|E_{\sigma }\right\|} $$ $$ f ∗ ( g ) = f ( g − 1 ) ¯ , {\displaystyle f^{*}(g)={\overline {f\left(g^{-1}\right)}},} $$ The Peter–Weyl theorem holds, and a version of the Fourier inversion formula (Plancherel's theorem) follows: if f ∈ L2(G), then f ( g ) = ∑ σ ∈ Σ d σ tr ⁡ ( f ^ ( σ ) U g ( σ ) ) f(g)=\sum _{\sigma \in \Sigma d_{\sigma }\operatorname {tr} \left({\hat {f}}(\sigma )U_{g}^{(\sigma )}\right)} where the summation is understood as convergent in the L2 sense. $$ f ( g ) = ∑ σ ∈ Σ d σ tr ⁡ ( f ^ ( σ ) U g ( σ ) ) {\displaystyle f(g)=\sum _{\sigma \in \Sigma }d_{\sigma }\operatorname {tr} \left({\hat {f}}(\sigma )U_{g}^{(\sigma )}\right)} $$ The generalization of the Fourier transform to the noncommutative situation has also in part contributed to the development of noncommutative geometry.[citation needed] In this context, a categorical generalization of the Fourier transform to noncommutative groups is Tannaka–Krein duality, which replaces the group of characters with the category of representations. However, this loses the connection with harmonic functions. In signal processing terms, a function (of time) is a representation of a signal with perfect time resolution, but no frequency information, while the Fourier transform has perfect frequency resolution, but no time information: the magnitude of the Fourier transform at a point is how much frequency content there is, but location is only given by phase (argument of the Fourier transform at a point), and standing waves are not localized in time – a sine wave continues out to infinity, without decaying. This limits the usefulness of the Fourier transform for analyzing signals that are localized in time, notably transients, or any signal of finite extent. As alternatives to the Fourier transform, in time–frequency analysis, one uses time–frequency transforms or time–frequency distributions to represent signals in a form that has some time information and some frequency information – by the uncertainty principle, there is a trade-off between these. These can be generalizations of the Fourier transform, such as the short-time Fourier transform, fractional Fourier transform, Synchrosqueezing Fourier transform, or other functions to represent signals, as in wavelet transforms and chirplet transforms, with the wavelet analog of the (continuous) Fourier transform being the continuous wavelet transform. Linear operations performed in one domain (time or frequency) have corresponding operations in the other domain, which are sometimes easier to perform. The operation of differentiation in the time domain corresponds to multiplication by the frequency,[note 6] so some differential equations are easier to analyze in the frequency domain. Also, convolution in the time domain corresponds to ordinary multiplication in the frequency domain (see Convolution theorem). After performing the desired operations, transformation of the result can be made back to the time domain. Harmonic analysis is the systematic study of the relationship between the frequency and time domains, including the kinds of functions or operations that are "simpler" in one or the other, and has deep connections to many areas of modern mathematics. Perhaps the most important use of the Fourier transformation is to solve partial differential equations. Many of the equations of the mathematical physics of the nineteenth century can be treated this way. Fourier studied the heat equation, which in one dimension and in dimensionless units is ∂ 2 y ( x , t ) ∂ 2 x = ∂ y ( x , t ) ∂ t . {\frac {\partial ^{2y(x,t)}{\partial ^{2}x}}={\frac {\partial y(x,t)}{\partial t}}.} The example we will give, a slightly more difficult one, is the wave equation in one dimension, ∂ 2 y ( x , t ) ∂ 2 x = ∂ 2 y ( x , t ) ∂ 2 t . {\frac {\partial ^{2y(x,t)}{\partial ^{2}x}}={\frac {\partial ^{2}y(x,t)}{\partial ^{2}t}}.} $$ ∂ 2 y ( x , t ) ∂ 2 x = ∂ y ( x , t ) ∂ t . {\displaystyle {\frac {\partial ^{2}y(x,t)}{\partial ^{2}x}}={\frac {\partial y(x,t)}{\partial t}}.} $$ $$ ∂ 2 y ( x , t ) ∂ 2 x = ∂ 2 y ( x , t ) ∂ 2 t . {\displaystyle {\frac {\partial ^{2}y(x,t)}{\partial ^{2}x}}={\frac {\partial ^{2}y(x,t)}{\partial ^{2}t}}.} $$ As usual, the problem is not to find a solution: there are infinitely many. The problem is that of the so-called "boundary problem": find a solution which satisfies the "boundary conditions" y ( x , 0 ) = f ( x ) , ∂ y ( x , 0 ) ∂ t = g ( x ) . y(x,0)=f(x),\qquad {\frac {\partial y(x,0){\partial t}}=g(x).} $$ y ( x , 0 ) = f ( x ) , ∂ y ( x , 0 ) ∂ t = g ( x ) . {\displaystyle y(x,0)=f(x),\qquad {\frac {\partial y(x,0)}{\partial t}}=g(x).} $$ Here, f and g are given functions. For the heat equation, only one boundary condition can be required (usually the first one). But for the wave equation, there are still infinitely many solutions y which satisfy the first boundary condition. But when one imposes both conditions, there is only one possible solution. It is easier to find the Fourier transform ŷ of the solution than to find the solution directly. This is because the Fourier transformation takes differentiation into multiplication by the Fourier-dual variable, and so a partial differential equation applied to the original function is transformed into multiplication by polynomial functions of the dual variables applied to the transformed function. After ŷ is determined, we can apply the inverse Fourier transformation to find y. Fourier's method is as follows. First, note that any function of the forms cos ⁡ ( 2 π ξ ( x ± t ) ) or sin ⁡ ( 2 π ξ ( x ± t ) ) \cos {\bigl (2\pi \xi (x\pm t){\bigr )}{\text{ or }}\sin {\bigl (}2\pi \xi (x\pm t){\bigr )}} satisfies the wave equation. These are called the elementary solutions. $$ cos ⁡ ( 2 π ξ ( x ± t ) ) or sin ⁡ ( 2 π ξ ( x ± t ) ) {\displaystyle \cos {\bigl (}2\pi \xi (x\pm t){\bigr )}{\text{ or }}\sin {\bigl (}2\pi \xi (x\pm t){\bigr )}} $$ Second, note that therefore any integral y ( x , t ) = ∫ 0 ∞ d ξ [ a + ( ξ ) cos ⁡ ( 2 π ξ ( x + t ) ) + a − ( ξ ) cos ⁡ ( 2 π ξ ( x − t ) ) + b + ( ξ ) sin ⁡ ( 2 π ξ ( x + t ) ) + b − ( ξ ) sin ⁡ ( 2 π ξ ( x − t ) ) ] {\begin{alignedy(x,t)=\int _{0}^{\infty }d\xi {\Bigl [}&a_{+}(\xi )\cos {\bigl (}2\pi \xi (x+t){\bigr )}+a_{-}(\xi )\cos {\bigl (}2\pi \xi (x-t){\bigr )}+{}\\&b_{+}(\xi )\sin {\bigl (}2\pi \xi (x+t){\bigr )}+b_{-}(\xi )\sin \left(2\pi \xi (x-t)\right){\Bigr ]}\end{aligned}}} satisfies the wave equation for arbitrary a+, a−, b+, b−. This integral may be interpreted as a continuous linear combination of solutions for the linear equation. $$ y ( x , t ) = ∫ 0 ∞ d ξ [ a + ( ξ ) cos ⁡ ( 2 π ξ ( x + t ) ) + a − ( ξ ) cos ⁡ ( 2 π ξ ( x − t ) ) + b + ( ξ ) sin ⁡ ( 2 π ξ ( x + t ) ) + b − ( ξ ) sin ⁡ ( 2 π ξ ( x − t ) ) ] {\displaystyle {\begin{aligned}y(x,t)=\int _{0}^{\infty }d\xi {\Bigl [}&a_{+}(\xi )\cos {\bigl (}2\pi \xi (x+t){\bigr )}+a_{-}(\xi )\cos {\bigl (}2\pi \xi (x-t){\bigr )}+{}\\&b_{+}(\xi )\sin {\bigl (}2\pi \xi (x+t){\bigr )}+b_{-}(\xi )\sin \left(2\pi \xi (x-t)\right){\Bigr ]}\end{aligned}}} $$ Now this resembles the formula for the Fourier synthesis of a function. In fact, this is the real inverse Fourier transform of a± and b± in the variable x. The third step is to examine how to find the specific unknown coefficient functions a± and b± that will lead to y satisfying the boundary conditions. We are interested in the values of these solutions at t = 0. So we will set t = 0. Assuming that the conditions needed for Fourier inversion are satisfied, we can then find the Fourier sine and cosine transforms (in the variable x) of both sides and obtain 2 ∫ − ∞ ∞ y ( x , 0 ) cos ⁡ ( 2 π ξ x ) d x = a + + a − 2\int _{-\infty ^{\infty }y(x,0)\cos(2\pi \xi x)\,dx=a_{+}+a_{-}} and 2 ∫ − ∞ ∞ y ( x , 0 ) sin ⁡ ( 2 π ξ x ) d x = b + + b − . 2\int _{-\infty ^{\infty }y(x,0)\sin(2\pi \xi x)\,dx=b_{+}+b_{-}.} $$ 2 ∫ − ∞ ∞ y ( x , 0 ) cos ⁡ ( 2 π ξ x ) d x = a + + a − {\displaystyle 2\int _{-\infty }^{\infty }y(x,0)\cos(2\pi \xi x)\,dx=a_{+}+a_{-}} $$ $$ 2 ∫ − ∞ ∞ y ( x , 0 ) sin ⁡ ( 2 π ξ x ) d x = b + + b − . {\displaystyle 2\int _{-\infty }^{\infty }y(x,0)\sin(2\pi \xi x)\,dx=b_{+}+b_{-}.} $$ Similarly, taking the derivative of y with respect to t and then applying the Fourier sine and cosine transformations yields 2 ∫ − ∞ ∞ ∂ y ( u , 0 ) ∂ t sin ⁡ ( 2 π ξ x ) d x = ( 2 π ξ ) ( − a + + a − ) 2\int _{-\infty ^{\infty }{\frac {\partial y(u,0)}{\partial t}}\sin(2\pi \xi x)\,dx=(2\pi \xi )\left(-a_{+}+a_{-}\right)} and 2 ∫ − ∞ ∞ ∂ y ( u , 0 ) ∂ t cos ⁡ ( 2 π ξ x ) d x = ( 2 π ξ ) ( b + − b − ) . 2\int _{-\infty ^{\infty }{\frac {\partial y(u,0)}{\partial t}}\cos(2\pi \xi x)\,dx=(2\pi \xi )\left(b_{+}-b_{-}\right).} $$ 2 ∫ − ∞ ∞ ∂ y ( u , 0 ) ∂ t sin ⁡ ( 2 π ξ x ) d x = ( 2 π ξ ) ( − a + + a − ) {\displaystyle 2\int _{-\infty }^{\infty }{\frac {\partial y(u,0)}{\partial t}}\sin(2\pi \xi x)\,dx=(2\pi \xi )\left(-a_{+}+a_{-}\right)} $$ $$ 2 ∫ − ∞ ∞ ∂ y ( u , 0 ) ∂ t cos ⁡ ( 2 π ξ x ) d x = ( 2 π ξ ) ( b + − b − ) . {\displaystyle 2\int _{-\infty }^{\infty }{\frac {\partial y(u,0)}{\partial t}}\cos(2\pi \xi x)\,dx=(2\pi \xi )\left(b_{+}-b_{-}\right).} $$ These are four linear equations for the four unknowns a± and b±, in terms of the Fourier sine and cosine transforms of the boundary conditions, which are easily solved by elementary algebra, provided that these transforms can be found. In summary, we chose a set of elementary solutions, parametrized by ξ, of which the general solution would be a (continuous) linear combination in the form of an integral over the parameter ξ. But this integral was in the form of a Fourier integral. The next step was to express the boundary conditions in terms of these integrals, and set them equal to the given functions f and g. But these expressions also took the form of a Fourier integral because of the properties of the Fourier transform of a derivative. The last step was to exploit Fourier inversion by applying the Fourier transformation to both sides, thus obtaining expressions for the coefficient functions a± and b± in terms of the given boundary conditions f and g. From a higher point of view, Fourier's procedure can be reformulated more conceptually. Since there are two variables, we will use the Fourier transformation in both x and t rather than operate as Fourier did, who only transformed in the spatial variables. Note that ŷ must be considered in the sense of a distribution since y(x, t) is not going to be L1: as a wave, it will persist through time and thus is not a transient phenomenon. But it will be bounded and so its Fourier transform can be defined as a distribution. The operational properties of the Fourier transformation that are relevant to this equation are that it takes differentiation in x to multiplication by i2πξ and differentiation with respect to t to multiplication by i2πf where f is the frequency. Then the wave equation becomes an algebraic equation in ŷ: ξ 2 y ^ ( ξ , f ) = f 2 y ^ ( ξ , f ) . \xi ^{2{\hat {y}}(\xi ,f)=f^{2}{\hat {y}}(\xi ,f).} This is equivalent to requiring ŷ(ξ, f) = 0 unless ξ = ±f. Right away, this explains why the choice of elementary solutions we made earlier worked so well: obviously f̂ = δ(ξ ± f) will be solutions. Applying Fourier inversion to these delta functions, we obtain the elementary solutions we picked earlier. But from the higher point of view, one does not pick elementary solutions, but rather considers the space of all distributions which are supported on the (degenerate) conic ξ2 − f2 = 0. $$ ξ 2 y ^ ( ξ , f ) = f 2 y ^ ( ξ , f ) . {\displaystyle \xi ^{2}{\hat {y}}(\xi ,f)=f^{2}{\hat {y}}(\xi ,f).} $$ We may as well consider the distributions supported on the conic that are given by distributions of one variable on the line ξ = f plus distributions on the line ξ = −f as follows: if Φ is any test function, ∬ y ^ ϕ ( ξ , f ) d ξ d f = ∫ s + ϕ ( ξ , ξ ) d ξ + ∫ s − ϕ ( ξ , − ξ ) d ξ , \iint {\hat {y}\phi (\xi ,f)\,d\xi \,df=\int s_{+}\phi (\xi ,\xi )\,d\xi +\int s_{-}\phi (\xi ,-\xi )\,d\xi ,} where s+, and s−, are distributions of one variable. $$ ∬ y ^ ϕ ( ξ , f ) d ξ d f = ∫ s + ϕ ( ξ , ξ ) d ξ + ∫ s − ϕ ( ξ , − ξ ) d ξ , {\displaystyle \iint {\hat {y}}\phi (\xi ,f)\,d\xi \,df=\int s_{+}\phi (\xi ,\xi )\,d\xi +\int s_{-}\phi (\xi ,-\xi )\,d\xi ,} $$ Then Fourier inversion gives, for the boundary conditions, something very similar to what we had more concretely above (put Φ(ξ, f) = ei2π(xξ+tf), which is clearly of polynomial growth): y ( x , 0 ) = ∫ { s + ( ξ ) + s − ( ξ ) } e i 2 π ξ x + 0 d ξ y(x,0)=\int {\bigl \{s_{+}(\xi )+s_{-}(\xi ){\bigr \}}e^{i2\pi \xi x+0}\,d\xi } and ∂ y ( x , 0 ) ∂ t = ∫ { s + ( ξ ) − s − ( ξ ) } i 2 π ξ e i 2 π ξ x + 0 d ξ . {\frac {\partial y(x,0){\partial t}}=\int {\bigl \{}s_{+}(\xi )-s_{-}(\xi ){\bigr \}}i2\pi \xi e^{i2\pi \xi x+0}\,d\xi .} $$ y ( x , 0 ) = ∫ { s + ( ξ ) + s − ( ξ ) } e i 2 π ξ x + 0 d ξ {\displaystyle y(x,0)=\int {\bigl \{}s_{+}(\xi )+s_{-}(\xi ){\bigr \}}e^{i2\pi \xi x+0}\,d\xi } $$ $$ ∂ y ( x , 0 ) ∂ t = ∫ { s + ( ξ ) − s − ( ξ ) } i 2 π ξ e i 2 π ξ x + 0 d ξ . {\displaystyle {\frac {\partial y(x,0)}{\partial t}}=\int {\bigl \{}s_{+}(\xi )-s_{-}(\xi ){\bigr \}}i2\pi \xi e^{i2\pi \xi x+0}\,d\xi .} $$ Now, as before, applying the one-variable Fourier transformation in the variable x to these functions of x yields two equations in the two unknown distributions s± (which can be taken to be ordinary functions if the boundary conditions are L1 or L2). From a calculational point of view, the drawback of course is that one must first calculate the Fourier transforms of the boundary conditions, then assemble the solution from these, and then calculate an inverse Fourier transform. Closed form formulas are rare, except when there is some geometric symmetry that can be exploited, and the numerical calculations are difficult because of the oscillatory nature of the integrals, which makes convergence slow and hard to estimate. For practical calculations, other methods are often used. The twentieth century has seen the extension of these methods to all linear partial differential equations with polynomial coefficients, and by extending the notion of Fourier transformation to include Fourier integral operators, some non-linear equations as well. The Fourier transform is also used in nuclear magnetic resonance (NMR) and in other kinds of spectroscopy, e.g. infrared (FTIR). In NMR an exponentially shaped free induction decay (FID) signal is acquired in the time domain and Fourier-transformed to a Lorentzian line-shape in the frequency domain. The Fourier transform is also used in magnetic resonance imaging (MRI) and mass spectrometry. The Fourier transform is useful in quantum mechanics in at least two different ways. To begin with, the basic conceptual structure of quantum mechanics postulates the existence of pairs of complementary variables, connected by the Heisenberg uncertainty principle. For example, in one dimension, the spatial variable q of, say, a particle, can only be measured by the quantum mechanical "position operator" at the cost of losing information about the momentum p of the particle. Therefore, the physical state of the particle can either be described by a function, called "the wave function", of q or by a function of p but not by a function of both variables. The variable p is called the conjugate variable to q. In classical mechanics, the physical state of a particle (existing in one dimension, for simplicity of exposition) would be given by assigning definite values to both p and q simultaneously. Thus, the set of all possible physical states is the two-dimensional real vector space with a p-axis and a q-axis called the phase space. In contrast, quantum mechanics chooses a polarisation of this space in the sense that it picks a subspace of one-half the dimension, for example, the q-axis alone, but instead of considering only points, takes the set of all complex-valued "wave functions" on this axis. Nevertheless, choosing the p-axis is an equally valid polarisation, yielding a different representation of the set of possible physical states of the particle. Both representations of the wavefunction are related by a Fourier transform, such that ϕ ( p ) = ∫ d q ψ ( q ) e − i p q / h , \phi (p)=\int dq\,\psi (q)e^{-ipq/h,} or, equivalently, ψ ( q ) = ∫ d p ϕ ( p ) e i p q / h . \psi (q)=\int dp\,\phi (p)e^{ipq/h.} $$ ϕ ( p ) = ∫ d q ψ ( q ) e − i p q / h , {\displaystyle \phi (p)=\int dq\,\psi (q)e^{-ipq/h},} $$ $$ ψ ( q ) = ∫ d p ϕ ( p ) e i p q / h . {\displaystyle \psi (q)=\int dp\,\phi (p)e^{ipq/h}.} $$ Physically realisable states are L2, and so by the Plancherel theorem, their Fourier transforms are also L2. (Note that since q is in units of distance and p is in units of momentum, the presence of the Planck constant in the exponent makes the exponent dimensionless, as it should be.) Therefore, the Fourier transform can be used to pass from one way of representing the state of the particle, by a wave function of position, to another way of representing the state of the particle: by a wave function of momentum. Infinitely many different polarisations are possible, and all are equally valid. Being able to transform states from one representation to another by the Fourier transform is not only convenient but also the underlying reason of the Heisenberg uncertainty principle. The other use of the Fourier transform in both quantum mechanics and quantum field theory is to solve the applicable wave equation. In non-relativistic quantum mechanics, Schrödinger's equation for a time-varying wave function in one-dimension, not subject to external forces, is − ∂ 2 ∂ x 2 ψ ( x , t ) = i h 2 π ∂ ∂ t ψ ( x , t ) . -{\frac {\partial ^{2}{\partial x^{2}}}\psi (x,t)=i{\frac {h}{2\pi }}{\frac {\partial }{\partial t}}\psi (x,t).} $$ − ∂ 2 ∂ x 2 ψ ( x , t ) = i h 2 π ∂ ∂ t ψ ( x , t ) . {\displaystyle -{\frac {\partial ^{2}}{\partial x^{2}}}\psi (x,t)=i{\frac {h}{2\pi }}{\frac {\partial }{\partial t}}\psi (x,t).} $$ This is the same as the heat equation except for the presence of the imaginary unit i. Fourier methods can be used to solve this equation. In the presence of a potential, given by the potential energy function V(x), the equation becomes − ∂ 2 ∂ x 2 ψ ( x , t ) + V ( x ) ψ ( x , t ) = i h 2 π ∂ ∂ t ψ ( x , t ) . -{\frac {\partial ^{2}{\partial x^{2}}}\psi (x,t)+V(x)\psi (x,t)=i{\frac {h}{2\pi }}{\frac {\partial }{\partial t}}\psi (x,t).} $$ − ∂ 2 ∂ x 2 ψ ( x , t ) + V ( x ) ψ ( x , t ) = i h 2 π ∂ ∂ t ψ ( x , t ) . {\displaystyle -{\frac {\partial ^{2}}{\partial x^{2}}}\psi (x,t)+V(x)\psi (x,t)=i{\frac {h}{2\pi }}{\frac {\partial }{\partial t}}\psi (x,t).} $$ The "elementary solutions", as we referred to them above, are the so-called "stationary states" of the particle, and Fourier's algorithm, as described above, can still be used to solve the boundary value problem of the future evolution of ψ given its values for t = 0. Neither of these approaches is of much practical use in quantum mechanics. Boundary value problems and the time-evolution of the wave function is not of much practical interest: it is the stationary states that are most important. In relativistic quantum mechanics, Schrödinger's equation becomes a wave equation as was usual in classical physics, except that complex-valued waves are considered. A simple example, in the absence of interactions with other particles or fields, is the free one-dimensional Klein–Gordon–Schrödinger–Fock equation, this time in dimensionless units, ( ∂ 2 ∂ x 2 + 1 ) ψ ( x , t ) = ∂ 2 ∂ t 2 ψ ( x , t ) . \left({\frac {\partial ^{2}{\partial x^{2}}}+1\right)\psi (x,t)={\frac {\partial ^{2}}{\partial t^{2}}}\psi (x,t).} $$ ( ∂ 2 ∂ x 2 + 1 ) ψ ( x , t ) = ∂ 2 ∂ t 2 ψ ( x , t ) . {\displaystyle \left({\frac {\partial ^{2}}{\partial x^{2}}}+1\right)\psi (x,t)={\frac {\partial ^{2}}{\partial t^{2}}}\psi (x,t).} $$ This is, from the mathematical point of view, the same as the wave equation of classical physics solved above (but with a complex-valued wave, which makes no difference in the methods). This is of great use in quantum field theory: each separate Fourier component of a wave can be treated as a separate harmonic oscillator and then quantized, a procedure known as "second quantization". Fourier methods have been adapted to also deal with non-trivial interactions. Finally, the number operator of the quantum harmonic oscillator can be interpreted, for example via the Mehler kernel, as the generator of the Fourier transform F {\mathcal {F}} . $$ F {\displaystyle {\mathcal {F}}} $$ The Fourier transform is used for the spectral analysis of time-series. The subject of statistical signal processing does not, however, usually apply the Fourier transformation to the signal itself. Even if a real signal is indeed transient, it has been found in practice advisable to model a signal by a function (or, alternatively, a stochastic process) which is stationary in the sense that its characteristic properties are constant over all time. The Fourier transform of such a function does not exist in the usual sense, and it has been found more useful for the analysis of signals to instead take the Fourier transform of its autocorrelation function. The autocorrelation function R of a function f is defined by R f ( τ ) = lim T → ∞ 1 2 T ∫ − T T f ( t ) f ( t + τ ) d t . R_{f(\tau )=\lim _{T\rightarrow \infty }{\frac {1}{2T}}\int _{-T}^{T}f(t)f(t+\tau )\,dt.} $$ R f ( τ ) = lim T → ∞ 1 2 T ∫ − T T f ( t ) f ( t + τ ) d t . {\displaystyle R_{f}(\tau )=\lim _{T\rightarrow \infty }{\frac {1}{2T}}\int _{-T}^{T}f(t)f(t+\tau )\,dt.} $$ This function is a function of the time-lag τ elapsing between the values of f to be correlated. For most functions f that occur in practice, R is a bounded even function of the time-lag τ and for typical noisy signals it turns out to be uniformly continuous with a maximum at τ = 0. The autocorrelation function, more properly called the autocovariance function unless it is normalized in some appropriate fashion, measures the strength of the correlation between the values of f separated by a time lag. This is a way of searching for the correlation of f with its own past. It is useful even for other statistical tasks besides the analysis of signals. For example, if f(t) represents the temperature at time t, one expects a strong correlation with the temperature at a time lag of 24 hours. It possesses a Fourier transform, P f ( ξ ) = ∫ − ∞ ∞ R f ( τ ) e − i 2 π ξ τ d τ . P_{f(\xi )=\int _{-\infty }^{\infty }R_{f}(\tau )e^{-i2\pi \xi \tau }\,d\tau .} $$ P f ( ξ ) = ∫ − ∞ ∞ R f ( τ ) e − i 2 π ξ τ d τ . {\displaystyle P_{f}(\xi )=\int _{-\infty }^{\infty }R_{f}(\tau )e^{-i2\pi \xi \tau }\,d\tau .} $$ This Fourier transform is called the power spectral density function of f. (Unless all periodic components are first filtered out from f, this integral will diverge, but it is easy to filter out such periodicities.) The power spectrum, as indicated by this density function P, measures the amount of variance contributed to the data by the frequency ξ. In electrical signals, the variance is proportional to the average power (energy per unit time), and so the power spectrum describes how much the different frequencies contribute to the average power of the signal. This process is called the spectral analysis of time-series and is analogous to the usual analysis of variance of data that is not a time-series (ANOVA). Knowledge of which frequencies are "important" in this sense is crucial for the proper design of filters and for the proper evaluation of measuring apparatuses. It can also be useful for the scientific analysis of the phenomena responsible for producing the data. The power spectrum of a signal can also be approximately measured directly by measuring the average power that remains in a signal after all the frequencies outside a narrow band have been filtered out. Spectral analysis is carried out for visual signals as well. The power spectrum ignores all phase relations, which is good enough for many purposes, but for video signals other types of spectral analysis must also be employed, still using the Fourier transform as a tool. Other common notations for f ^ ( ξ ) {\hat {f}(\xi )} include: f ~ ( ξ ) , F ( ξ ) , F ( f ) ( ξ ) , ( F f ) ( ξ ) , F ( f ) , F { f } , F ( f ( t ) ) , F { f ( t ) } . {\tilde {f}(\xi ),\ F(\xi ),\ {\mathcal {F}}\left(f\right)(\xi ),\ \left({\mathcal {F}}f\right)(\xi ),\ {\mathcal {F}}(f),\ {\mathcal {F}}\{f\},\ {\mathcal {F}}{\bigl (}f(t){\bigr )},\ {\mathcal {F}}{\bigl \{}f(t){\bigr \}}.} $$ f ^ ( ξ ) {\displaystyle {\hat {f}}(\xi )} $$ $$ f ~ ( ξ ) , F ( ξ ) , F ( f ) ( ξ ) , ( F f ) ( ξ ) , F ( f ) , F { f } , F ( f ( t ) ) , F { f ( t ) } . {\displaystyle {\tilde {f}}(\xi ),\ F(\xi ),\ {\mathcal {F}}\left(f\right)(\xi ),\ \left({\mathcal {F}}f\right)(\xi ),\ {\mathcal {F}}(f),\ {\mathcal {F}}\{f\},\ {\mathcal {F}}{\bigl (}f(t){\bigr )},\ {\mathcal {F}}{\bigl \{}f(t){\bigr \}}.} $$ In the sciences and engineering it is also common to make substitutions like these: ξ → f , x → t , f → x , f ^ → X . \xi \rightarrow f,\quad x\rightarrow t,\quad f\rightarrow x,\quad {\hat {f}\rightarrow X.} $$ ξ → f , x → t , f → x , f ^ → X . {\displaystyle \xi \rightarrow f,\quad x\rightarrow t,\quad f\rightarrow x,\quad {\hat {f}}\rightarrow X.} $$ So the transform pair f ( x ) ⟺ F f ^ ( ξ ) f(x)\ {\stackrel {\mathcal {F}{\Longleftrightarrow }}\ {\hat {f}}(\xi )} can become x ( t ) ⟺ F X ( f ) x(t)\ {\stackrel {\mathcal {F}{\Longleftrightarrow }}\ X(f)} $$ f ( x ) ⟺ F f ^ ( ξ ) {\displaystyle f(x)\ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ {\hat {f}}(\xi )} $$ $$ x ( t ) ⟺ F X ( f ) {\displaystyle x(t)\ {\stackrel {\mathcal {F}}{\Longleftrightarrow }}\ X(f)} $$ A disadvantage of the capital letter notation is when expressing a transform such as f ⋅ g ^ {\widehat {f\cdot g}} or f ′ ^ , {\widehat {f'},} which become the more awkward F { f ⋅ g } {\mathcal {F}\{f\cdot g\}} and F { f ′ } . {\mathcal {F}\{f'\}.} $$ f ⋅ g ^ {\displaystyle {\widehat {f\cdot g}}} $$ $$ f ′ ^ , {\displaystyle {\widehat {f'}},} $$ $$ F { f ⋅ g } {\displaystyle {\mathcal {F}}\{f\cdot g\}} $$ $$ F { f ′ } . {\displaystyle {\mathcal {F}}\{f'\}.} $$ In some contexts such as particle physics, the same symbol f f may be used for both for a function as well as it Fourier transform, with the two only distinguished by their argument I.e. f ( k 1 + k 2 ) f(k_{1+k_{2})} would refer to the Fourier transform because of the momentum argument, while f ( x 0 + π r → ) f(x_{0+\pi {\vec {r}})} would refer to the original function because of the positional argument. Although tildes may be used as in f ~ {\tilde {f}} to indicate Fourier transforms, tildes may also be used to indicate a modification of a quantity with a more Lorentz invariant form, such as d k ~ = d k ( 2 π ) 3 2 ω {\tilde {dk}={\frac {dk}{(2\pi )^{3}2\omega }}} , so care must be taken. Similarly, f ^ {\hat {f}} often denotes the Hilbert transform of f f . $$ f {\displaystyle f} $$ $$ f ( k 1 + k 2 ) {\displaystyle f(k_{1}+k_{2})} $$ $$ f ( x 0 + π r → ) {\displaystyle f(x_{0}+\pi {\vec {r}})} $$ $$ f ~ {\displaystyle {\tilde {f}}} $$ $$ d k ~ = d k ( 2 π ) 3 2 ω {\displaystyle {\tilde {dk}}={\frac {dk}{(2\pi )^{3}2\omega }}} $$ $$ f ^ {\displaystyle {\hat {f}}} $$ $$ f {\displaystyle f} $$ The interpretation of the complex function f̂(ξ) may be aided by expressing it in polar coordinate form f ^ ( ξ ) = A ( ξ ) e i φ ( ξ ) {\hat {f}(\xi )=A(\xi )e^{i\varphi (\xi )}} in terms of the two real functions A(ξ) and φ(ξ) where: A ( ξ ) = | f ^ ( ξ ) | , A(\xi )=\left|{\hat {f}(\xi )\right|,} is the amplitude and φ ( ξ ) = arg ⁡ ( f ^ ( ξ ) ) , \varphi (\xi )=\arg \left({\hat {f}(\xi )\right),} is the phase (see arg function). $$ f ^ ( ξ ) = A ( ξ ) e i φ ( ξ ) {\displaystyle {\hat {f}}(\xi )=A(\xi )e^{i\varphi (\xi )}} $$ $$ A ( ξ ) = | f ^ ( ξ ) | , {\displaystyle A(\xi )=\left|{\hat {f}}(\xi )\right|,} $$ $$ φ ( ξ ) = arg ⁡ ( f ^ ( ξ ) ) , {\displaystyle \varphi (\xi )=\arg \left({\hat {f}}(\xi )\right),} $$ Then the inverse transform can be written: f ( x ) = ∫ − ∞ ∞ A ( ξ ) e i ( 2 π ξ x + φ ( ξ ) ) d ξ , f(x)=\int _{-\infty ^{\infty }A(\xi )\ e^{i{\bigl (}2\pi \xi x+\varphi (\xi ){\bigr )}}\,d\xi ,} which is a recombination of all the frequency components of f(x). Each component is a complex sinusoid of the form e2πixξ whose amplitude is A(ξ) and whose initial phase angle (at x = 0) is φ(ξ). $$ f ( x ) = ∫ − ∞ ∞ A ( ξ ) e i ( 2 π ξ x + φ ( ξ ) ) d ξ , {\displaystyle f(x)=\int _{-\infty }^{\infty }A(\xi )\ e^{i{\bigl (}2\pi \xi x+\varphi (\xi ){\bigr )}}\,d\xi ,} $$ The Fourier transform may be thought of as a mapping on function spaces. This mapping is here denoted F and F(f) is used to denote the Fourier transform of the function f. This mapping is linear, which means that F can also be seen as a linear transformation on the function space and implies that the standard notation in linear algebra of applying a linear transformation to a vector (here the function f) can be used to write F f instead of F(f). Since the result of applying the Fourier transform is again a function, we can be interested in the value of this function evaluated at the value ξ for its variable, and this is denoted either as F f(ξ) or as (F f)(ξ). Notice that in the former case, it is implicitly understood that F is applied first to f and then the resulting function is evaluated at ξ, not the other way around. In mathematics and various applied sciences, it is often necessary to distinguish between a function f and the value of f when its variable equals x, denoted f(x). This means that a notation like F(f(x)) formally can be interpreted as the Fourier transform of the values of f at x. Despite this flaw, the previous notation appears frequently, often when a particular function or a function of a particular variable is to be transformed. For example, F ( rect ⁡ ( x ) ) = sinc ⁡ ( ξ ) {\mathcal {F}{\bigl (}\operatorname {rect} (x){\bigr )}=\operatorname {sinc} (\xi )} is sometimes used to express that the Fourier transform of a rectangular function is a sinc function, or F ( f ( x + x 0 ) ) = F ( f ( x ) ) e i 2 π x 0 ξ {\mathcal {F}{\bigl (}f(x+x_{0}){\bigr )}={\mathcal {F}}{\bigl (}f(x){\bigr )}\,e^{i2\pi x_{0}\xi }} is used to express the shift property of the Fourier transform. $$ F ( rect ⁡ ( x ) ) = sinc ⁡ ( ξ ) {\displaystyle {\mathcal {F}}{\bigl (}\operatorname {rect} (x){\bigr )}=\operatorname {sinc} (\xi )} $$ $$ F ( f ( x + x 0 ) ) = F ( f ( x ) ) e i 2 π x 0 ξ {\displaystyle {\mathcal {F}}{\bigl (}f(x+x_{0}){\bigr )}={\mathcal {F}}{\bigl (}f(x){\bigr )}\,e^{i2\pi x_{0}\xi }} $$ Notice, that the last example is only correct under the assumption that the transformed function is a function of x, not of x0. As discussed above, the characteristic function of a random variable is the same as the Fourier–Stieltjes transform of its distribution measure, but in this context it is typical to take a different convention for the constants. Typically characteristic function is defined E ( e i t ⋅ X ) = ∫ e i t ⋅ x d μ X ( x ) . E\left(e^{it\cdot X\right)=\int e^{it\cdot x}\,d\mu _{X}(x).} $$ E ( e i t ⋅ X ) = ∫ e i t ⋅ x d μ X ( x ) . {\displaystyle E\left(e^{it\cdot X}\right)=\int e^{it\cdot x}\,d\mu _{X}(x).} $$ As in the case of the "non-unitary angular frequency" convention above, the factor of 2π appears in neither the normalizing constant nor the exponent. Unlike any of the conventions appearing above, this convention takes the opposite sign in the exponent. The appropriate computation method largely depends how the original mathematical function is represented and the desired form of the output function. In this section we consider both functions of a continuous variable, f ( x ) , f(x), and functions of a discrete variable (i.e. ordered pairs of x x and f f values). For discrete-valued x , x, the transform integral becomes a summation of sinusoids, which is still a continuous function of frequency ( ξ \xi or ω \omega ). When the sinusoids are harmonically-related (i.e. when the x x -values are spaced at integer multiples of an interval), the transform is called discrete-time Fourier transform (DTFT). $$ f ( x ) , {\displaystyle f(x),} $$ $$ x {\displaystyle x} $$ $$ f {\displaystyle f} $$ $$ x , {\displaystyle x,} $$ $$ ξ {\displaystyle \xi } $$ $$ ω {\displaystyle \omega } $$ $$ x {\displaystyle x} $$ Sampling the DTFT at equally-spaced values of frequency is the most common modern method of computation. Efficient procedures, depending on the frequency resolution needed, are described at Discrete-time Fourier transform § Sampling the DTFT. The discrete Fourier transform (DFT), used there, is usually computed by a fast Fourier transform (FFT) algorithm. Tables of closed-form Fourier transforms, such as § Square-integrable functions, one-dimensional and § Table of discrete-time Fourier transforms, are created by mathematically evaluating the Fourier analysis integral (or summation) into another closed-form function of frequency ( ξ \xi or ω \omega ). When mathematically possible, this provides a transform for a continuum of frequency values. $$ ξ {\displaystyle \xi } $$ $$ ω {\displaystyle \omega } $$ Many computer algebra systems such as Matlab and Mathematica that are capable of symbolic integration are capable of computing Fourier transforms analytically. For example, to compute the Fourier transform of cos(6πt) e−πt2 one might enter the command integrate cos(6*pi*t) exp(−pi*t^2) exp(-i*2*pi*f*t) from -inf to inf into Wolfram Alpha.[note 7] Discrete sampling of the Fourier transform can also be done by numerical integration of the definition at each value of frequency for which transform is desired. The numerical integration approach works on a much broader class of functions than the analytic approach. If the input function is a series of ordered pairs, numerical integration reduces to just a summation over the set of data pairs. The DTFT is a common subcase of this more general situation. The following tables record some closed-form Fourier transforms. For functions f(x) and g(x) denote their Fourier transforms by f̂ and ĝ. Only the three most common conventions are included. It may be useful to notice that entry 105 gives a relationship between the Fourier transform of a function and the original function, which can be seen as relating the Fourier transform and its inverse. The Fourier transforms in this table may be found in Erdélyi (1954) or Kammler (2000, appendix). $$ f ( x ) {\displaystyle f(x)\,} $$ $$ f ^ ( ξ ) ≜ f 1 ^ ( ξ ) = ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x {\displaystyle {\begin{aligned}&{\widehat {f}}(\xi )\triangleq {\widehat {f_{1}}}(\xi )\\&=\int _{-\infty }^{\infty }f(x)e^{-i2\pi \xi x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f 2 ^ ( ω ) = 1 2 π ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\widehat {f}}(\omega )\triangleq {\widehat {f_{2}}}(\omega )\\&={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f 3 ^ ( ω ) = ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\widehat {f}}(\omega )\triangleq {\widehat {f_{3}}}(\omega )\\&=\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ a f ( x ) + b g ( x ) {\displaystyle a\,f(x)+b\,g(x)\,} $$ $$ a f ^ ( ξ ) + b g ^ ( ξ ) {\displaystyle a\,{\widehat {f}}(\xi )+b\,{\widehat {g}}(\xi )\,} $$ $$ a f ^ ( ω ) + b g ^ ( ω ) {\displaystyle a\,{\widehat {f}}(\omega )+b\,{\widehat {g}}(\omega )\,} $$ $$ a f ^ ( ω ) + b g ^ ( ω ) {\displaystyle a\,{\widehat {f}}(\omega )+b\,{\widehat {g}}(\omega )\,} $$ $$ f ( x − a ) {\displaystyle f(x-a)\,} $$ $$ e − i 2 π ξ a f ^ ( ξ ) {\displaystyle e^{-i2\pi \xi a}{\widehat {f}}(\xi )\,} $$ $$ e − i a ω f ^ ( ω ) {\displaystyle e^{-ia\omega }{\widehat {f}}(\omega )\,} $$ $$ e − i a ω f ^ ( ω ) {\displaystyle e^{-ia\omega }{\widehat {f}}(\omega )\,} $$ $$ f ( x ) e i a x {\displaystyle f(x)e^{iax}\,} $$ $$ f ^ ( ξ − a 2 π ) {\displaystyle {\widehat {f}}\left(\xi -{\frac {a}{2\pi }}\right)\,} $$ $$ f ^ ( ω − a ) {\displaystyle {\widehat {f}}(\omega -a)\,} $$ $$ f ^ ( ω − a ) {\displaystyle {\widehat {f}}(\omega -a)\,} $$ $$ f ( a x ) {\displaystyle f(ax)\,} $$ $$ 1 | a | f ^ ( ξ a ) {\displaystyle {\frac {1}{|a|}}{\widehat {f}}\left({\frac {\xi }{a}}\right)\,} $$ $$ 1 | a | f ^ ( ω a ) {\displaystyle {\frac {1}{|a|}}{\widehat {f}}\left({\frac {\omega }{a}}\right)\,} $$ $$ 1 | a | f ^ ( ω a ) {\displaystyle {\frac {1}{|a|}}{\widehat {f}}\left({\frac {\omega }{a}}\right)\,} $$ $$ 1 | a | f ^ ( ω a ) {\displaystyle {\frac {1}{|a|}}{\hat {f}}\left({\frac {\omega }{a}}\right)\,} $$ $$ f n ^ ( x ) {\displaystyle {\widehat {f_{n}}}(x)\,} $$ $$ f 1 ^ ( x ) ⟷ F 1 f ( − ξ ) {\displaystyle {\widehat {f_{1}}}(x)\ {\stackrel {{\mathcal {F}}_{1}}{\longleftrightarrow }}\ f(-\xi )\,} $$ $$ f 2 ^ ( x ) ⟷ F 2 f ( − ω ) {\displaystyle {\widehat {f_{2}}}(x)\ {\stackrel {{\mathcal {F}}_{2}}{\longleftrightarrow }}\ f(-\omega )\,} $$ $$ f 3 ^ ( x ) ⟷ F 3 2 π f ( − ω ) {\displaystyle {\widehat {f_{3}}}(x)\ {\stackrel {{\mathcal {F}}_{3}}{\longleftrightarrow }}\ 2\pi f(-\omega )\,} $$ $$ d n f ( x ) d x n {\displaystyle {\frac {d^{n}f(x)}{dx^{n}}}\,} $$ $$ ( i 2 π ξ ) n f ^ ( ξ ) {\displaystyle (i2\pi \xi )^{n}{\widehat {f}}(\xi )\,} $$ $$ ( i ω ) n f ^ ( ω ) {\displaystyle (i\omega )^{n}{\widehat {f}}(\omega )\,} $$ $$ ( i ω ) n f ^ ( ω ) {\displaystyle (i\omega )^{n}{\widehat {f}}(\omega )\,} $$ As f is a Schwartz function $$ ∫ − ∞ x f ( τ ) d τ {\displaystyle \int _{-\infty }^{x}f(\tau )d\tau } $$ $$ f ^ ( ξ ) i 2 π ξ + C δ ( ξ ) {\displaystyle {\frac {{\widehat {f}}(\xi )}{i2\pi \xi }}+C\,\delta (\xi )} $$ $$ f ^ ( ω ) i ω + 2 π C δ ( ω ) {\displaystyle {\frac {{\widehat {f}}(\omega )}{i\omega }}+{\sqrt {2\pi }}C\delta (\omega )} $$ $$ f ^ ( ω ) i ω + 2 π C δ ( ω ) {\displaystyle {\frac {{\widehat {f}}(\omega )}{i\omega }}+2\pi C\delta (\omega )} $$ $$ δ {\displaystyle \delta } $$ $$ C {\displaystyle C} $$ $$ f ( x ) {\displaystyle f(x)} $$ $$ ∫ − ∞ ∞ ( f ( x ) − C ) d x = 0 {\displaystyle \int _{-\infty }^{\infty }(f(x)-C)\,dx=0} $$ $$ x n f ( x ) {\displaystyle x^{n}f(x)\,} $$ $$ ( i 2 π ) n d n f ^ ( ξ ) d ξ n {\displaystyle \left({\frac {i}{2\pi }}\right)^{n}{\frac {d^{n}{\widehat {f}}(\xi )}{d\xi ^{n}}}\,} $$ $$ i n d n f ^ ( ω ) d ω n {\displaystyle i^{n}{\frac {d^{n}{\widehat {f}}(\omega )}{d\omega ^{n}}}} $$ $$ i n d n f ^ ( ω ) d ω n {\displaystyle i^{n}{\frac {d^{n}{\widehat {f}}(\omega )}{d\omega ^{n}}}} $$ $$ ( f ∗ g ) ( x ) {\displaystyle (f*g)(x)\,} $$ $$ f ^ ( ξ ) g ^ ( ξ ) {\displaystyle {\widehat {f}}(\xi ){\widehat {g}}(\xi )\,} $$ $$ 2 π f ^ ( ω ) g ^ ( ω ) {\displaystyle {\sqrt {2\pi }}\ {\widehat {f}}(\omega ){\widehat {g}}(\omega )\,} $$ $$ f ^ ( ω ) g ^ ( ω ) {\displaystyle {\widehat {f}}(\omega ){\widehat {g}}(\omega )\,} $$ $$ f ( x ) g ( x ) {\displaystyle f(x)g(x)\,} $$ $$ ( f ^ ∗ g ^ ) ( ξ ) {\displaystyle \left({\widehat {f}}*{\widehat {g}}\right)(\xi )\,} $$ $$ 1 2 π ( f ^ ∗ g ^ ) ( ω ) {\displaystyle {\frac {1}{\sqrt {2\pi }}}\left({\widehat {f}}*{\widehat {g}}\right)(\omega )\,} $$ $$ 1 2 π ( f ^ ∗ g ^ ) ( ω ) {\displaystyle {\frac {1}{2\pi }}\left({\widehat {f}}*{\widehat {g}}\right)(\omega )\,} $$ $$ f ^ ( − ξ ) = f ^ ( ξ ) ¯ {\displaystyle {\widehat {f}}(-\xi )={\overline {{\widehat {f}}(\xi )}}\,} $$ $$ f ^ ( − ω ) = f ^ ( ω ) ¯ {\displaystyle {\widehat {f}}(-\omega )={\overline {{\widehat {f}}(\omega )}}\,} $$ $$ f ^ ( − ω ) = f ^ ( ω ) ¯ {\displaystyle {\widehat {f}}(-\omega )={\overline {{\widehat {f}}(\omega )}}\,} $$ $$ f ^ ( − ξ ) = − f ^ ( ξ ) ¯ {\displaystyle {\widehat {f}}(-\xi )=-{\overline {{\widehat {f}}(\xi )}}\,} $$ $$ f ^ ( − ω ) = − f ^ ( ω ) ¯ {\displaystyle {\widehat {f}}(-\omega )=-{\overline {{\widehat {f}}(\omega )}}\,} $$ $$ f ^ ( − ω ) = − f ^ ( ω ) ¯ {\displaystyle {\widehat {f}}(-\omega )=-{\overline {{\widehat {f}}(\omega )}}\,} $$ $$ f ( x ) ¯ {\displaystyle {\overline {f(x)}}} $$ $$ f ^ ( − ξ ) ¯ {\displaystyle {\overline {{\widehat {f}}(-\xi )}}} $$ $$ f ^ ( − ω ) ¯ {\displaystyle {\overline {{\widehat {f}}(-\omega )}}} $$ $$ f ^ ( − ω ) ¯ {\displaystyle {\overline {{\widehat {f}}(-\omega )}}} $$ $$ f ( x ) cos ⁡ ( a x ) {\displaystyle f(x)\cos(ax)} $$ $$ f ^ ( ξ − a 2 π ) + f ^ ( ξ + a 2 π ) 2 {\displaystyle {\frac {{\widehat {f}}\left(\xi -{\frac {a}{2\pi }}\right)+{\widehat {f}}\left(\xi +{\frac {a}{2\pi }}\right)}{2}}} $$ $$ f ^ ( ω − a ) + f ^ ( ω + a ) 2 {\displaystyle {\frac {{\widehat {f}}(\omega -a)+{\widehat {f}}(\omega +a)}{2}}\,} $$ $$ f ^ ( ω − a ) + f ^ ( ω + a ) 2 {\displaystyle {\frac {{\widehat {f}}(\omega -a)+{\widehat {f}}(\omega +a)}{2}}} $$ $$ cos ⁡ ( a x ) = e i a x + e − i a x 2 . {\displaystyle \cos(ax)={\frac {e^{iax}+e^{-iax}}{2}}.} $$ $$ f ( x ) sin ⁡ ( a x ) {\displaystyle f(x)\sin(ax)} $$ $$ f ^ ( ξ − a 2 π ) − f ^ ( ξ + a 2 π ) 2 i {\displaystyle {\frac {{\widehat {f}}\left(\xi -{\frac {a}{2\pi }}\right)-{\widehat {f}}\left(\xi +{\frac {a}{2\pi }}\right)}{2i}}} $$ $$ f ^ ( ω − a ) − f ^ ( ω + a ) 2 i {\displaystyle {\frac {{\widehat {f}}(\omega -a)-{\widehat {f}}(\omega +a)}{2i}}} $$ $$ f ^ ( ω − a ) − f ^ ( ω + a ) 2 i {\displaystyle {\frac {{\widehat {f}}(\omega -a)-{\widehat {f}}(\omega +a)}{2i}}} $$ $$ sin ⁡ ( a x ) = e i a x − e − i a x 2 i . {\displaystyle \sin(ax)={\frac {e^{iax}-e^{-iax}}{2i}}.} $$ The Fourier transforms in this table may be found in Campbell & Foster (1948), Erdélyi (1954), or Kammler (2000, appendix). $$ f ( x ) {\displaystyle f(x)\,} $$ $$ f ^ ( ξ ) ≜ f ^ 1 ( ξ ) = ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\xi )\triangleq {\hat {f}}_{1}(\xi )\\&=\int _{-\infty }^{\infty }f(x)e^{-i2\pi \xi x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f ^ 2 ( ω ) = 1 2 π ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\omega )\triangleq {\hat {f}}_{2}(\omega )\\&={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f ^ 3 ( ω ) = ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\omega )\triangleq {\hat {f}}_{3}(\omega )\\&=\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ rect ⁡ ( a x ) {\displaystyle \operatorname {rect} (ax)\,} $$ $$ 1 | a | sinc ⁡ ( ξ a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {sinc} \left({\frac {\xi }{a}}\right)} $$ $$ 1 2 π a 2 sinc ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{\sqrt {2\pi a^{2}}}}\,\operatorname {sinc} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ 1 | a | sinc ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {sinc} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ sinc ⁡ ( a x ) {\displaystyle \operatorname {sinc} (ax)\,} $$ $$ 1 | a | rect ⁡ ( ξ a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {rect} \left({\frac {\xi }{a}}\right)\,} $$ $$ 1 2 π a 2 rect ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{\sqrt {2\pi a^{2}}}}\,\operatorname {rect} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ 1 | a | rect ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {rect} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ sinc 2 ⁡ ( a x ) {\displaystyle \operatorname {sinc} ^{2}(ax)} $$ $$ 1 | a | tri ⁡ ( ξ a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {tri} \left({\frac {\xi }{a}}\right)} $$ $$ 1 2 π a 2 tri ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{\sqrt {2\pi a^{2}}}}\,\operatorname {tri} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ 1 | a | tri ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {tri} \left({\frac {\omega }{2\pi a}}\right)} $$ $$ tri ⁡ ( a x ) {\displaystyle \operatorname {tri} (ax)} $$ $$ 1 | a | sinc 2 ⁡ ( ξ a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {sinc} ^{2}\left({\frac {\xi }{a}}\right)\,} $$ $$ 1 2 π a 2 sinc 2 ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{\sqrt {2\pi a^{2}}}}\,\operatorname {sinc} ^{2}\left({\frac {\omega }{2\pi a}}\right)} $$ $$ 1 | a | sinc 2 ⁡ ( ω 2 π a ) {\displaystyle {\frac {1}{|a|}}\,\operatorname {sinc} ^{2}\left({\frac {\omega }{2\pi a}}\right)} $$ $$ e − a x u ( x ) {\displaystyle e^{-ax}u(x)\,} $$ $$ 1 a + i 2 π ξ {\displaystyle {\frac {1}{a+i2\pi \xi }}} $$ $$ 1 2 π ( a + i ω ) {\displaystyle {\frac {1}{{\sqrt {2\pi }}(a+i\omega )}}} $$ $$ 1 a + i ω {\displaystyle {\frac {1}{a+i\omega }}} $$ $$ e − α x 2 {\displaystyle e^{-\alpha x^{2}}\,} $$ $$ π α e − ( π ξ ) 2 α {\displaystyle {\sqrt {\frac {\pi }{\alpha }}}\,e^{-{\frac {(\pi \xi )^{2}}{\alpha }}}} $$ $$ 1 2 α e − ω 2 4 α {\displaystyle {\frac {1}{\sqrt {2\alpha }}}\,e^{-{\frac {\omega ^{2}}{4\alpha }}}} $$ $$ π α e − ω 2 4 α {\displaystyle {\sqrt {\frac {\pi }{\alpha }}}\,e^{-{\frac {\omega ^{2}}{4\alpha }}}} $$ $$ e − a | x | {\displaystyle e^{-a|x|}\,} $$ $$ 2 a a 2 + 4 π 2 ξ 2 {\displaystyle {\frac {2a}{a^{2}+4\pi ^{2}\xi ^{2}}}} $$ $$ 2 π a a 2 + ω 2 {\displaystyle {\sqrt {\frac {2}{\pi }}}\,{\frac {a}{a^{2}+\omega ^{2}}}} $$ $$ 2 a a 2 + ω 2 {\displaystyle {\frac {2a}{a^{2}+\omega ^{2}}}} $$ $$ sech ⁡ ( a x ) {\displaystyle \operatorname {sech} (ax)\,} $$ $$ π a sech ⁡ ( π 2 a ξ ) {\displaystyle {\frac {\pi }{a}}\operatorname {sech} \left({\frac {\pi ^{2}}{a}}\xi \right)} $$ $$ 1 a π 2 sech ⁡ ( π 2 a ω ) {\displaystyle {\frac {1}{a}}{\sqrt {\frac {\pi }{2}}}\operatorname {sech} \left({\frac {\pi }{2a}}\omega \right)} $$ $$ π a sech ⁡ ( π 2 a ω ) {\displaystyle {\frac {\pi }{a}}\operatorname {sech} \left({\frac {\pi }{2a}}\omega \right)} $$ $$ e − a 2 x 2 2 H n ( a x ) {\displaystyle e^{-{\frac {a^{2}x^{2}}{2}}}H_{n}(ax)\,} $$ $$ 2 π ( − i ) n a e − 2 π 2 ξ 2 a 2 H n ( 2 π ξ a ) {\displaystyle {\frac {{\sqrt {2\pi }}(-i)^{n}}{a}}e^{-{\frac {2\pi ^{2}\xi ^{2}}{a^{2}}}}H_{n}\left({\frac {2\pi \xi }{a}}\right)} $$ $$ ( − i ) n a e − ω 2 2 a 2 H n ( ω a ) {\displaystyle {\frac {(-i)^{n}}{a}}e^{-{\frac {\omega ^{2}}{2a^{2}}}}H_{n}\left({\frac {\omega }{a}}\right)} $$ $$ ( − i ) n 2 π a e − ω 2 2 a 2 H n ( ω a ) {\displaystyle {\frac {(-i)^{n}{\sqrt {2\pi }}}{a}}e^{-{\frac {\omega ^{2}}{2a^{2}}}}H_{n}\left({\frac {\omega }{a}}\right)} $$ The Fourier transforms in this table may be found in Erdélyi (1954) or Kammler (2000, appendix). $$ f ( x ) {\displaystyle f(x)\,} $$ $$ f ^ ( ξ ) ≜ f ^ 1 ( ξ ) = ∫ − ∞ ∞ f ( x ) e − i 2 π ξ x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\xi )\triangleq {\hat {f}}_{1}(\xi )\\&=\int _{-\infty }^{\infty }f(x)e^{-i2\pi \xi x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f ^ 2 ( ω ) = 1 2 π ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\omega )\triangleq {\hat {f}}_{2}(\omega )\\&={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ f ^ ( ω ) ≜ f ^ 3 ( ω ) = ∫ − ∞ ∞ f ( x ) e − i ω x d x {\displaystyle {\begin{aligned}&{\hat {f}}(\omega )\triangleq {\hat {f}}_{3}(\omega )\\&=\int _{-\infty }^{\infty }f(x)e^{-i\omega x}\,dx\end{aligned}}} $$ $$ 1 {\displaystyle 1} $$ $$ δ ( ξ ) {\displaystyle \delta (\xi )} $$ $$ 2 π δ ( ω ) {\displaystyle {\sqrt {2\pi }}\,\delta (\omega )} $$ $$ 2 π δ ( ω ) {\displaystyle 2\pi \delta (\omega )} $$ $$ δ ( x ) {\displaystyle \delta (x)\,} $$ $$ 1 {\displaystyle 1} $$ $$ 1 2 π {\displaystyle {\frac {1}{\sqrt {2\pi }}}\,} $$ $$ 1 {\displaystyle 1} $$ $$ e i a x {\displaystyle e^{iax}} $$ $$ δ ( ξ − a 2 π ) {\displaystyle \delta \left(\xi -{\frac {a}{2\pi }}\right)} $$ $$ 2 π δ ( ω − a ) {\displaystyle {\sqrt {2\pi }}\,\delta (\omega -a)} $$ $$ 2 π δ ( ω − a ) {\displaystyle 2\pi \delta (\omega -a)} $$ $$ cos ⁡ ( a x ) {\displaystyle \cos(ax)} $$ $$ δ ( ξ − a 2 π ) + δ ( ξ + a 2 π ) 2 {\displaystyle {\frac {\delta \left(\xi -{\frac {a}{2\pi }}\right)+\delta \left(\xi +{\frac {a}{2\pi }}\right)}{2}}} $$ $$ 2 π δ ( ω − a ) + δ ( ω + a ) 2 {\displaystyle {\sqrt {2\pi }}\,{\frac {\delta (\omega -a)+\delta (\omega +a)}{2}}} $$ $$ π ( δ ( ω − a ) + δ ( ω + a ) ) {\displaystyle \pi \left(\delta (\omega -a)+\delta (\omega +a)\right)} $$ $$ cos ⁡ ( a x ) = e i a x + e − i a x 2 . {\displaystyle \cos(ax)={\frac {e^{iax}+e^{-iax}}{2}}.} $$ $$ sin ⁡ ( a x ) {\displaystyle \sin(ax)} $$ $$ δ ( ξ − a 2 π ) − δ ( ξ + a 2 π ) 2 i {\displaystyle {\frac {\delta \left(\xi -{\frac {a}{2\pi }}\right)-\delta \left(\xi +{\frac {a}{2\pi }}\right)}{2i}}} $$ $$ 2 π δ ( ω − a ) − δ ( ω + a ) 2 i {\displaystyle {\sqrt {2\pi }}\,{\frac {\delta (\omega -a)-\delta (\omega +a)}{2i}}} $$ $$ − i π ( δ ( ω − a ) − δ ( ω + a ) ) {\displaystyle -i\pi {\bigl (}\delta (\omega -a)-\delta (\omega +a){\bigr )}} $$ $$ sin ⁡ ( a x ) = e i a x − e − i a x 2 i . {\displaystyle \sin(ax)={\frac {e^{iax}-e^{-iax}}{2i}}.} $$ $$ cos ⁡ ( a x 2 ) {\displaystyle \cos \left(ax^{2}\right)} $$ $$ π a cos ⁡ ( π 2 ξ 2 a − π 4 ) {\displaystyle {\sqrt {\frac {\pi }{a}}}\cos \left({\frac {\pi ^{2}\xi ^{2}}{a}}-{\frac {\pi }{4}}\right)} $$ $$ 1 2 a cos ⁡ ( ω 2 4 a − π 4 ) {\displaystyle {\frac {1}{\sqrt {2a}}}\cos \left({\frac {\omega ^{2}}{4a}}-{\frac {\pi }{4}}\right)} $$ $$ π a cos ⁡ ( ω 2 4 a − π 4 ) {\displaystyle {\sqrt {\frac {\pi }{a}}}\cos \left({\frac {\omega ^{2}}{4a}}-{\frac {\pi }{4}}\right)} $$ $$ cos ⁡ ( a x 2 ) = e i a x 2 + e − i a x 2 2 . {\displaystyle \cos(ax^{2})={\frac {e^{iax^{2}}+e^{-iax^{2}}}{2}}.} $$ $$ sin ⁡ ( a x 2 ) {\displaystyle \sin \left(ax^{2}\right)} $$ $$ − π a sin ⁡ ( π 2 ξ 2 a − π 4 ) {\displaystyle -{\sqrt {\frac {\pi }{a}}}\sin \left({\frac {\pi ^{2}\xi ^{2}}{a}}-{\frac {\pi }{4}}\right)} $$ $$ − 1 2 a sin ⁡ ( ω 2 4 a − π 4 ) {\displaystyle {\frac {-1}{\sqrt {2a}}}\sin \left({\frac {\omega ^{2}}{4a}}-{\frac {\pi }{4}}\right)} $$ $$ − π a sin ⁡ ( ω 2 4 a − π 4 ) {\displaystyle -{\sqrt {\frac {\pi }{a}}}\sin \left({\frac {\omega ^{2}}{4a}}-{\frac {\pi }{4}}\right)} $$ $$ sin ⁡ ( a x 2 ) = e i a x 2 − e − i a x 2 2 i . {\displaystyle \sin(ax^{2})={\frac {e^{iax^{2}}-e^{-iax^{2}}}{2i}}.} $$ $$ e − π i α x 2 {\displaystyle e^{-\pi i\alpha x^{2}}\,} $$ $$ 1 α e − i π 4 e i π ξ 2 α {\displaystyle {\frac {1}{\sqrt {\alpha }}}\,e^{-i{\frac {\pi }{4}}}e^{i{\frac {\pi \xi ^{2}}{\alpha }}}} $$ $$ 1 2 π α e − i π 4 e i ω 2 4 π α {\displaystyle {\frac {1}{\sqrt {2\pi \alpha }}}\,e^{-i{\frac {\pi }{4}}}e^{i{\frac {\omega ^{2}}{4\pi \alpha }}}} $$ $$ 1 α e − i π 4 e i ω 2 4 π α {\displaystyle {\frac {1}{\sqrt {\alpha }}}\,e^{-i{\frac {\pi }{4}}}e^{i{\frac {\omega ^{2}}{4\pi \alpha }}}} $$ $$ α {\displaystyle \alpha } $$ $$ x n {\displaystyle x^{n}\,} $$ $$ ( i 2 π ) n δ ( n ) ( ξ ) {\displaystyle \left({\frac {i}{2\pi }}\right)^{n}\delta ^{(n)}(\xi )} $$ $$ i n 2 π δ ( n ) ( ω ) {\displaystyle i^{n}{\sqrt {2\pi }}\delta ^{(n)}(\omega )} $$ $$ 2 π i n δ ( n ) ( ω ) {\displaystyle 2\pi i^{n}\delta ^{(n)}(\omega )} $$ $$ δ ( n ) ( x ) {\displaystyle \delta ^{(n)}(x)} $$ $$ ( i 2 π ξ ) n {\displaystyle (i2\pi \xi )^{n}} $$ $$ ( i ω ) n 2 π {\displaystyle {\frac {(i\omega )^{n}}{\sqrt {2\pi }}}} $$ $$ ( i ω ) n {\displaystyle (i\omega )^{n}} $$ $$ 1 x {\displaystyle {\frac {1}{x}}} $$ $$ − i π sgn ⁡ ( ξ ) {\displaystyle -i\pi \operatorname {sgn}(\xi )} $$ $$ − i π 2 sgn ⁡ ( ω ) {\displaystyle -i{\sqrt {\frac {\pi }{2}}}\operatorname {sgn}(\omega )} $$ $$ − i π sgn ⁡ ( ω ) {\displaystyle -i\pi \operatorname {sgn}(\omega )} $$ $$ 1 x n := ( − 1 ) n − 1 ( n − 1 ) ! d n d x n log ⁡ | x | {\displaystyle {\begin{aligned}&{\frac {1}{x^{n}}}\\&:={\frac {(-1)^{n-1}}{(n-1)!}}{\frac {d^{n}}{dx^{n}}}\log |x|\end{aligned}}} $$ $$ − i π ( − i 2 π ξ ) n − 1 ( n − 1 ) ! sgn ⁡ ( ξ ) {\displaystyle -i\pi {\frac {(-i2\pi \xi )^{n-1}}{(n-1)!}}\operatorname {sgn}(\xi )} $$ $$ − i π 2 ( − i ω ) n − 1 ( n − 1 ) ! sgn ⁡ ( ω ) {\displaystyle -i{\sqrt {\frac {\pi }{2}}}\,{\frac {(-i\omega )^{n-1}}{(n-1)!}}\operatorname {sgn}(\omega )} $$ $$ − i π ( − i ω ) n − 1 ( n − 1 ) ! sgn ⁡ ( ω ) {\displaystyle -i\pi {\frac {(-i\omega )^{n-1}}{(n-1)!}}\operatorname {sgn}(\omega )} $$ $$ ( − 1 ) n − 1 ( n − 1 ) ! d n d x n log ⁡ | x | {\displaystyle {\frac {(-1)^{n-1}}{(n-1)!}}{\frac {d^{n}}{dx^{n}}}\log |x|} $$ $$ | x | α {\displaystyle |x|^{\alpha }} $$ $$ − 2 sin ⁡ ( π α 2 ) Γ ( α + 1 ) | 2 π ξ | α + 1 {\displaystyle -{\frac {2\sin \left({\frac {\pi \alpha }{2}}\right)\Gamma (\alpha +1)}{|2\pi \xi |^{\alpha +1}}}} $$ $$ − 2 2 π sin ⁡ ( π α 2 ) Γ ( α + 1 ) | ω | α + 1 {\displaystyle {\frac {-2}{\sqrt {2\pi }}}\,{\frac {\sin \left({\frac {\pi \alpha }{2}}\right)\Gamma (\alpha +1)}{|\omega |^{\alpha +1}}}} $$ $$ − 2 sin ⁡ ( π α 2 ) Γ ( α + 1 ) | ω | α + 1 {\displaystyle -{\frac {2\sin \left({\frac {\pi \alpha }{2}}\right)\Gamma (\alpha +1)}{|\omega |^{\alpha +1}}}} $$ $$ 1 | x | {\displaystyle {\frac {1}{\sqrt {|x|}}}} $$ $$ 1 | ξ | {\displaystyle {\frac {1}{\sqrt {|\xi |}}}} $$ $$ 1 | ω | {\displaystyle {\frac {1}{\sqrt {|\omega |}}}} $$ $$ 2 π | ω | {\displaystyle {\frac {\sqrt {2\pi }}{\sqrt {|\omega |}}}} $$ $$ sgn ⁡ ( x ) {\displaystyle \operatorname {sgn}(x)} $$ $$ 1 i π ξ {\displaystyle {\frac {1}{i\pi \xi }}} $$ $$ 2 π 1 i ω {\displaystyle {\sqrt {\frac {2}{\pi }}}{\frac {1}{i\omega }}} $$ $$ 2 i ω {\displaystyle {\frac {2}{i\omega }}} $$ $$ u ( x ) {\displaystyle u(x)} $$ $$ 1 2 ( 1 i π ξ + δ ( ξ ) ) {\displaystyle {\frac {1}{2}}\left({\frac {1}{i\pi \xi }}+\delta (\xi )\right)} $$ $$ π 2 ( 1 i π ω + δ ( ω ) ) {\displaystyle {\sqrt {\frac {\pi }{2}}}\left({\frac {1}{i\pi \omega }}+\delta (\omega )\right)} $$ $$ π ( 1 i π ω + δ ( ω ) ) {\displaystyle \pi \left({\frac {1}{i\pi \omega }}+\delta (\omega )\right)} $$ $$ ∑ n = − ∞ ∞ δ ( x − n T ) {\displaystyle \sum _{n=-\infty }^{\infty }\delta (x-nT)} $$ $$ 1 T ∑ k = − ∞ ∞ δ ( ξ − k T ) {\displaystyle {\frac {1}{T}}\sum _{k=-\infty }^{\infty }\delta \left(\xi -{\frac {k}{T}}\right)} $$ $$ 2 π T ∑ k = − ∞ ∞ δ ( ω − 2 π k T ) {\displaystyle {\frac {\sqrt {2\pi }}{T}}\sum _{k=-\infty }^{\infty }\delta \left(\omega -{\frac {2\pi k}{T}}\right)} $$ $$ 2 π T ∑ k = − ∞ ∞ δ ( ω − 2 π k T ) {\displaystyle {\frac {2\pi }{T}}\sum _{k=-\infty }^{\infty }\delta \left(\omega -{\frac {2\pi k}{T}}\right)} $$ $$ ∑ n = − ∞ ∞ e i n x = 2 π ∑ k = − ∞ ∞ δ ( x + 2 π k ) {\displaystyle {\begin{aligned}&\sum _{n=-\infty }^{\infty }e^{inx}\\={}&2\pi \sum _{k=-\infty }^{\infty }\delta (x+2\pi k)\end{aligned}}} $$ $$ J 0 ( x ) {\displaystyle J_{0}(x)} $$ $$ 2 rect ⁡ ( π ξ ) 1 − 4 π 2 ξ 2 {\displaystyle {\frac {2\,\operatorname {rect} (\pi \xi )}{\sqrt {1-4\pi ^{2}\xi ^{2}}}}} $$ $$ 2 π rect ⁡ ( ω 2 ) 1 − ω 2 {\displaystyle {\sqrt {\frac {2}{\pi }}}\,{\frac {\operatorname {rect} \left({\frac {\omega }{2}}\right)}{\sqrt {1-\omega ^{2}}}}} $$ $$ 2 rect ⁡ ( ω 2 ) 1 − ω 2 {\displaystyle {\frac {2\,\operatorname {rect} \left({\frac {\omega }{2}}\right)}{\sqrt {1-\omega ^{2}}}}} $$ $$ J n ( x ) {\displaystyle J_{n}(x)} $$ $$ 2 ( − i ) n T n ( 2 π ξ ) rect ⁡ ( π ξ ) 1 − 4 π 2 ξ 2 {\displaystyle {\frac {2(-i)^{n}T_{n}(2\pi \xi )\operatorname {rect} (\pi \xi )}{\sqrt {1-4\pi ^{2}\xi ^{2}}}}} $$ $$ 2 π ( − i ) n T n ( ω ) rect ⁡ ( ω 2 ) 1 − ω 2 {\displaystyle {\sqrt {\frac {2}{\pi }}}{\frac {(-i)^{n}T_{n}(\omega )\operatorname {rect} \left({\frac {\omega }{2}}\right)}{\sqrt {1-\omega ^{2}}}}} $$ $$ 2 ( − i ) n T n ( ω ) rect ⁡ ( ω 2 ) 1 − ω 2 {\displaystyle {\frac {2(-i)^{n}T_{n}(\omega )\operatorname {rect} \left({\frac {\omega }{2}}\right)}{\sqrt {1-\omega ^{2}}}}} $$ $$ log ⁡ | x | {\displaystyle \log \left|x\right|} $$ $$ − 1 2 1 | ξ | − γ δ ( ξ ) {\displaystyle -{\frac {1}{2}}{\frac {1}{\left|\xi \right|}}-\gamma \delta \left(\xi \right)} $$ $$ − π 2 | ω | − 2 π γ δ ( ω ) {\displaystyle -{\frac {\sqrt {\frac {\pi }{2}}}{\left|\omega \right|}}-{\sqrt {2\pi }}\gamma \delta \left(\omega \right)} $$ $$ − π | ω | − 2 π γ δ ( ω ) {\displaystyle -{\frac {\pi }{\left|\omega \right|}}-2\pi \gamma \delta \left(\omega \right)} $$ $$ ( ∓ i x ) − α {\displaystyle \left(\mp ix\right)^{-\alpha }} $$ $$ ( 2 π ) α Γ ( α ) u ( ± ξ ) ( ± ξ ) α − 1 {\displaystyle {\frac {\left(2\pi \right)^{\alpha }}{\Gamma \left(\alpha \right)}}u\left(\pm \xi \right)\left(\pm \xi \right)^{\alpha -1}} $$ $$ 2 π Γ ( α ) u ( ± ω ) ( ± ω ) α − 1 {\displaystyle {\frac {\sqrt {2\pi }}{\Gamma \left(\alpha \right)}}u\left(\pm \omega \right)\left(\pm \omega \right)^{\alpha -1}} $$ $$ 2 π Γ ( α ) u ( ± ω ) ( ± ω ) α − 1 {\displaystyle {\frac {2\pi }{\Gamma \left(\alpha \right)}}u\left(\pm \omega \right)\left(\pm \omega \right)^{\alpha -1}} $$ $$ f ( x , y ) {\displaystyle f(x,y)} $$ $$ f ^ ( ξ x , ξ y ) ≜ ∬ f ( x , y ) e − i 2 π ( ξ x x + ξ y y ) d x d y {\displaystyle {\begin{aligned}&{\hat {f}}(\xi _{x},\xi _{y})\triangleq \\&\iint f(x,y)e^{-i2\pi (\xi _{x}x+\xi _{y}y)}\,dx\,dy\end{aligned}}} $$ $$ f ^ ( ω x , ω y ) ≜ 1 2 π ∬ f ( x , y ) e − i ( ω x x + ω y y ) d x d y {\displaystyle {\begin{aligned}&{\hat {f}}(\omega _{x},\omega _{y})\triangleq \\&{\frac {1}{2\pi }}\iint f(x,y)e^{-i(\omega _{x}x+\omega _{y}y)}\,dx\,dy\end{aligned}}} $$ $$ f ^ ( ω x , ω y ) ≜ ∬ f ( x , y ) e − i ( ω x x + ω y y ) d x d y {\displaystyle {\begin{aligned}&{\hat {f}}(\omega _{x},\omega _{y})\triangleq \\&\iint f(x,y)e^{-i(\omega _{x}x+\omega _{y}y)}\,dx\,dy\end{aligned}}} $$ $$ e − π ( a 2 x 2 + b 2 y 2 ) {\displaystyle e^{-\pi \left(a^{2}x^{2}+b^{2}y^{2}\right)}} $$ $$ 1 | a b | e − π ( ξ x 2 a 2 + ξ y 2 b 2 ) {\displaystyle {\frac {1}{|ab|}}e^{-\pi \left({\frac {\xi _{x}^{2}}{a^{2}}}+{\frac {\xi _{y}^{2}}{b^{2}}}\right)}} $$ $$ 1 2 π | a b | e − 1 4 π ( ω x 2 a 2 + ω y 2 b 2 ) {\displaystyle {\frac {1}{2\pi \,|ab|}}e^{-{\frac {1}{4\pi }}\left({\frac {\omega _{x}^{2}}{a^{2}}}+{\frac {\omega _{y}^{2}}{b^{2}}}\right)}} $$ $$ 1 | a b | e − 1 4 π ( ω x 2 a 2 + ω y 2 b 2 ) {\displaystyle {\frac {1}{|ab|}}e^{-{\frac {1}{4\pi }}\left({\frac {\omega _{x}^{2}}{a^{2}}}+{\frac {\omega _{y}^{2}}{b^{2}}}\right)}} $$ $$ circ ⁡ ( x 2 + y 2 ) {\displaystyle \operatorname {circ} \left({\sqrt {x^{2}+y^{2}}}\right)} $$ $$ J 1 ( 2 π ξ x 2 + ξ y 2 ) ξ x 2 + ξ y 2 {\displaystyle {\frac {J_{1}\left(2\pi {\sqrt {\xi _{x}^{2}+\xi _{y}^{2}}}\right)}{\sqrt {\xi _{x}^{2}+\xi _{y}^{2}}}}} $$ $$ J 1 ( ω x 2 + ω y 2 ) ω x 2 + ω y 2 {\displaystyle {\frac {J_{1}\left({\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}\right)}{\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}}} $$ $$ 2 π J 1 ( ω x 2 + ω y 2 ) ω x 2 + ω y 2 {\displaystyle {\frac {2\pi J_{1}\left({\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}\right)}{\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}}} $$ $$ 1 x 2 + y 2 {\displaystyle {\frac {1}{\sqrt {x^{2}+y^{2}}}}} $$ $$ 1 ξ x 2 + ξ y 2 {\displaystyle {\frac {1}{\sqrt {\xi _{x}^{2}+\xi _{y}^{2}}}}} $$ $$ 1 ω x 2 + ω y 2 {\displaystyle {\frac {1}{\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}}} $$ $$ 2 π ω x 2 + ω y 2 {\displaystyle {\frac {2\pi }{\sqrt {\omega _{x}^{2}+\omega _{y}^{2}}}}} $$ $$ i x + i y {\displaystyle {\frac {i}{x+iy}}} $$ $$ 1 ξ x + i ξ y {\displaystyle {\frac {1}{\xi _{x}+i\xi _{y}}}} $$ $$ 1 ω x + i ω y {\displaystyle {\frac {1}{\omega _{x}+i\omega _{y}}}} $$ $$ 2 π ω x + i ω y {\displaystyle {\frac {2\pi }{\omega _{x}+i\omega _{y}}}} $$ $$ f ( x ) {\displaystyle f(\mathbf {x} )\,} $$ $$ f 1 ^ ( ξ ) ≜ ∫ R n f ( x ) e − i 2 π ξ ⋅ x d x {\displaystyle {\begin{aligned}&{\hat {f_{1}}}({\boldsymbol {\xi }})\triangleq \\&\int _{\mathbb {R} ^{n}}f(\mathbf {x} )e^{-i2\pi {\boldsymbol {\xi }}\cdot \mathbf {x} }\,d\mathbf {x} \end{aligned}}} $$ $$ f 2 ^ ( ω ) ≜ 1 ( 2 π ) n 2 ∫ R n f ( x ) e − i ω ⋅ x d x {\displaystyle {\begin{aligned}&{\hat {f_{2}}}({\boldsymbol {\omega }})\triangleq \\&{\frac {1}{{(2\pi )}^{\frac {n}{2}}}}\int _{\mathbb {R} ^{n}}f(\mathbf {x} )e^{-i{\boldsymbol {\omega }}\cdot \mathbf {x} }\,d\mathbf {x} \end{aligned}}} $$ $$ f 3 ^ ( ω ) ≜ ∫ R n f ( x ) e − i ω ⋅ x d x {\displaystyle {\begin{aligned}&{\hat {f_{3}}}({\boldsymbol {\omega }})\triangleq \\&\int _{\mathbb {R} ^{n}}f(\mathbf {x} )e^{-i{\boldsymbol {\omega }}\cdot \mathbf {x} }\,d\mathbf {x} \end{aligned}}} $$ $$ χ [ 0 , 1 ] ( | x | ) ( 1 − | x | 2 ) δ {\displaystyle \chi _{[0,1]}(|\mathbf {x} |)\left(1-|\mathbf {x} |^{2}\right)^{\delta }} $$ $$ Γ ( δ + 1 ) π δ | ξ | n 2 + δ J n 2 + δ ( 2 π | ξ | ) {\displaystyle {\frac {\Gamma (\delta +1)}{\pi ^{\delta }\,|{\boldsymbol {\xi }}|^{{\frac {n}{2}}+\delta }}}J_{{\frac {n}{2}}+\delta }(2\pi |{\boldsymbol {\xi }}|)} $$ $$ 2 δ Γ ( δ + 1 ) | ω | n 2 + δ J n 2 + δ ( | ω | ) {\displaystyle 2^{\delta }\,{\frac {\Gamma (\delta +1)}{\left|{\boldsymbol {\omega }}\right|^{{\frac {n}{2}}+\delta }}}J_{{\frac {n}{2}}+\delta }(|{\boldsymbol {\omega }}|)} $$ $$ Γ ( δ + 1 ) π δ | ω 2 π | − n 2 − δ J n 2 + δ ( | ω | ) {\displaystyle {\frac {\Gamma (\delta +1)}{\pi ^{\delta }}}\left|{\frac {\boldsymbol {\omega }}{2\pi }}\right|^{-{\frac {n}{2}}-\delta }J_{{\frac {n}{2}}+\delta }(\!|{\boldsymbol {\omega }}|\!)} $$ $$ | x | − α , 0 < Re ⁡ α < n . {\displaystyle |\mathbf {x} |^{-\alpha },\quad 0<\operatorname {Re} \alpha <n.} $$ $$ ( 2 π ) α c n , α | ξ | − ( n − α ) {\displaystyle {\frac {(2\pi )^{\alpha }}{c_{n,\alpha }}}|{\boldsymbol {\xi }}|^{-(n-\alpha )}} $$ $$ ( 2 π ) n 2 c n , α | ω | − ( n − α ) {\displaystyle {\frac {(2\pi )^{\frac {n}{2}}}{c_{n,\alpha }}}|{\boldsymbol {\omega }}|^{-(n-\alpha )}} $$ $$ ( 2 π ) n c n , α | ω | − ( n − α ) {\displaystyle {\frac {(2\pi )^{n}}{c_{n,\alpha }}}|{\boldsymbol {\omega }}|^{-(n-\alpha )}} $$ $$ c n , α = π n 2 2 α Γ ( α 2 ) Γ ( n − α 2 ) . {\displaystyle c_{n,\alpha }=\pi ^{\frac {n}{2}}2^{\alpha }{\frac {\Gamma \left({\frac {\alpha }{2}}\right)}{\Gamma \left({\frac {n-\alpha }{2}}\right)}}.} $$ $$ 1 | σ | ( 2 π ) n 2 e − 1 2 x T σ − T σ − 1 x {\displaystyle {\frac {1}{\left|{\boldsymbol {\sigma }}\right|\left(2\pi \right)^{\frac {n}{2}}}}e^{-{\frac {1}{2}}\mathbf {x} ^{\mathrm {T} }{\boldsymbol {\sigma }}^{-\mathrm {T} }{\boldsymbol {\sigma }}^{-1}\mathbf {x} }} $$ $$ e − 2 π 2 ξ T σ σ T ξ {\displaystyle e^{-2\pi ^{2}{\boldsymbol {\xi }}^{\mathrm {T} }{\boldsymbol {\sigma }}{\boldsymbol {\sigma }}^{\mathrm {T} }{\boldsymbol {\xi }}}} $$ $$ ( 2 π ) − n 2 e − 1 2 ω T σ σ T ω {\displaystyle (2\pi )^{-{\frac {n}{2}}}e^{-{\frac {1}{2}}{\boldsymbol {\omega }}^{\mathrm {T} }{\boldsymbol {\sigma }}{\boldsymbol {\sigma }}^{\mathrm {T} }{\boldsymbol {\omega }}}} $$ $$ e − 1 2 ω T σ σ T ω {\displaystyle e^{-{\frac {1}{2}}{\boldsymbol {\omega }}^{\mathrm {T} }{\boldsymbol {\sigma }}{\boldsymbol {\sigma }}^{\mathrm {T} }{\boldsymbol {\omega }}}} $$ $$ e − 2 π α | x | {\displaystyle e^{-2\pi \alpha |\mathbf {x} |}} $$ $$ c n α ( α 2 + | ξ | 2 ) n + 1 2 {\displaystyle {\frac {c_{n}\alpha }{\left(\alpha ^{2}+|{\boldsymbol {\xi }}|^{2}\right)^{\frac {n+1}{2}}}}} $$ $$ c n ( 2 π ) n + 2 2 α ( 4 π 2 α 2 + | ω | 2 ) n + 1 2 {\displaystyle {\frac {c_{n}(2\pi )^{\frac {n+2}{2}}\alpha }{\left(4\pi ^{2}\alpha ^{2}+|{\boldsymbol {\omega }}|^{2}\right)^{\frac {n+1}{2}}}}} $$ $$ c n ( 2 π ) n + 1 α ( 4 π 2 α 2 + | ω | 2 ) n + 1 2 {\displaystyle {\frac {c_{n}(2\pi )^{n+1}\alpha }{\left(4\pi ^{2}\alpha ^{2}+|{\boldsymbol {\omega }}|^{2}\right)^{\frac {n+1}{2}}}}} $$ $$ c n = Γ ( n + 1 2 ) π n + 1 2 , {\displaystyle c_{n}={\frac {\Gamma \left({\frac {n+1}{2}}\right)}{\pi ^{\frac {n+1}{2}}}},} $$ $$ f ( x ) e − i 2 π ξ 0 x {\displaystyle f(x)e^{-i2\pi \xi _{0}x}} $$ $$ f ^ ( ξ + ξ 0 ) . {\displaystyle {\widehat {f}}(\xi +\xi _{0}).} $$ $$ ξ = 0 {\displaystyle \xi =0} $$ $$ f ^ ( ξ 0 ) , {\displaystyle {\widehat {f}}(\xi _{0}),} $$ $$ ξ 0 {\displaystyle \xi _{0}} $$ $$ U ( 1 2 π d d x ) {\displaystyle U\left({\frac {1}{2\pi }}{\frac {d}{dx}}\right)} $$ $$ x {\displaystyle x} $$ $$ 1 2 π d d x {\displaystyle {\frac {1}{2\pi }}{\frac {d}{dx}}} $$ $$ U ( x ) . {\displaystyle U(x).} $$ $$ | x | λ {\displaystyle |\mathbf {x} |^{\lambda }} $$ $$ 2 λ + n π 1 2 n Γ ( λ + n 2 ) Γ ( − λ 2 ) | ω | − λ − n {\displaystyle 2^{\lambda +n}\pi ^{{\tfrac {1}{2}}n}{\frac {\Gamma \left({\frac {\lambda +n}{2}}\right)}{\Gamma \left(-{\frac {\lambda }{2}}\right)}}|{\boldsymbol {\omega }}|^{-\lambda -n}} $$ $$ λ = − α {\displaystyle \lambda =-\alpha } $$

End of content from: https://en.wikipedia.org/wiki/Fourier_transform

--------------------------------------------------------------------------------

Content from: https://en.wikipedia.org/wiki/Navier–Stokes_equations

$$ J = − D d φ d x {\displaystyle J=-D{\frac {d\varphi }{dx}}} $$ The Navier–Stokes equations (/nævˈjeɪ stoʊks/ nav-YAY STOHKS) are partial differential equations which describe the motion of viscous fluid substances. They were named after French engineer and physicist Claude-Louis Navier and the Irish physicist and mathematician George Gabriel Stokes. They were developed over several decades of progressively building the theories, from 1822 (Navier) to 1842–1850 (Stokes). The Navier–Stokes equations mathematically express momentum balance for Newtonian fluids and make use of conservation of mass. They are sometimes accompanied by an equation of state relating pressure, temperature and density. They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable). The Navier–Stokes equations are useful because they describe the physics of many phenomena of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations, in their full and simplified forms, help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other problems. Coupled with Maxwell's equations, they can be used to model and study magnetohydrodynamics. The Navier–Stokes equations are also of great interest in a purely mathematical sense. Despite their wide range of practical uses, it has not yet been proven whether smooth solutions always exist in three dimensions—i.e., whether they are infinitely differentiable (or even just bounded) at all points in the domain. This is called the Navier–Stokes existence and smoothness problem. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1 million prize for a solution or a counterexample. The solution of the equations is a flow velocity. It is a vector field—to every point in a fluid, at any moment in a time interval, it gives a vector whose direction and magnitude are those of the velocity of the fluid at that point in space and at that moment in time. It is usually studied in three spatial dimensions and one time dimension, although two (spatial) dimensional and steady-state cases are often used as models, and higher-dimensional analogues are studied in both pure and applied mathematics. Once the velocity field is calculated, other quantities of interest such as pressure or temperature may be found using dynamical equations and relations. This is different from what one normally sees in classical mechanics, where solutions are typically trajectories of position of a particle or deflection of a continuum. Studying velocity instead of position makes more sense for a fluid, although for visualization purposes one can compute various trajectories. In particular, the streamlines of a vector field, interpreted as flow velocity, are the paths along which a massless fluid particle would travel. These paths are the integral curves whose derivative at each point is equal to the vector field, and they can represent visually the behavior of the vector field at a point in time. The Navier–Stokes momentum equation can be derived as a particular form of the Cauchy momentum equation, whose general convective form is: D u D t = 1 ρ ∇ ⋅ σ + g . {\frac {\mathrm {D \mathbf {u} }{\mathrm {D} t}}={\frac {1}{\rho }}\nabla \cdot {\boldsymbol {\sigma }}+\mathbf {g} .} By setting the Cauchy stress tensor σ {\textstyle {\boldsymbol {\sigma }}} to be the sum of a viscosity term τ {\textstyle {\boldsymbol {\tau }}} (the deviatoric stress) and a pressure term − p I {\textstyle -p\mathbf {I} } (volumetric stress), we arrive at: $$ D u D t = 1 ρ ∇ ⋅ σ + g . {\displaystyle {\frac {\mathrm {D} \mathbf {u} }{\mathrm {D} t}}={\frac {1}{\rho }}\nabla \cdot {\boldsymbol {\sigma }}+\mathbf {g} .} $$ $$ σ {\textstyle {\boldsymbol {\sigma }}} $$ $$ τ {\textstyle {\boldsymbol {\tau }}} $$ $$ − p I {\textstyle -p\mathbf {I} } $$ ρ D u D t = − ∇ p + ∇ ⋅ τ + ρ a \rho {\frac {\mathrm {D \mathbf {u} }{\mathrm {D} t}}=-\nabla p+\nabla \cdot {\boldsymbol {\tau }}+\rho \,\mathbf {a} } $$ ρ D u D t = − ∇ p + ∇ ⋅ τ + ρ a {\displaystyle \rho {\frac {\mathrm {D} \mathbf {u} }{\mathrm {D} t}}=-\nabla p+\nabla \cdot {\boldsymbol {\tau }}+\rho \,\mathbf {a} } $$ where $$ D D t {\textstyle {\frac {\mathrm {D} }{\mathrm {D} t}}} $$ $$ ∂ ∂ t + u ⋅ ∇ {\textstyle {\frac {\partial }{\partial t}}+\mathbf {u} \cdot \nabla } $$ $$ ρ {\textstyle \rho } $$ $$ u {\textstyle \mathbf {u} } $$ $$ ∇ ⋅ {\textstyle \nabla \cdot \,} $$ $$ p {\textstyle p} $$ $$ t {\textstyle t} $$ $$ τ {\textstyle {\boldsymbol {\tau }}} $$ $$ a {\textstyle \mathbf {a} } $$ In this form, it is apparent that in the assumption of an inviscid fluid – no deviatoric stress – Cauchy equations reduce to the Euler equations. Assuming conservation of mass, with the known properties of divergence and gradient we can use the mass continuity equation, which represents the mass per unit volume of a homogenous fluid with respect to space and time (i.e., material derivative D D t {\frac {\mathbf {D }{\mathbf {Dt} }}} ) of any finite volume (V) to represent the change of velocity in fluid media: D m D t = ∭ V ( D ρ D t + ρ ( ∇ ⋅ u ) ) d V D ρ D t + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ( ∇ ρ ) ⋅ u + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\begin{aligned{\frac {\mathbf {D} m}{\mathbf {Dt} }}&={\iiint \limits _{V}}\left({{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot \mathbf {u} )}\right)dV\\{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot {\mathbf {u} })&={\frac {\partial \rho }{\partial t}}+({\nabla \rho })\cdot {\mathbf {u} }+{\rho }(\nabla \cdot \mathbf {u} )={\frac {\partial \rho }{\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0\end{aligned}}} where $$ D D t {\displaystyle {\frac {\mathbf {D} }{\mathbf {Dt} }}} $$ $$ D m D t = ∭ V ( D ρ D t + ρ ( ∇ ⋅ u ) ) d V D ρ D t + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ( ∇ ρ ) ⋅ u + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\displaystyle {\begin{aligned}{\frac {\mathbf {D} m}{\mathbf {Dt} }}&={\iiint \limits _{V}}\left({{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot \mathbf {u} )}\right)dV\\{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot {\mathbf {u} })&={\frac {\partial \rho }{\partial t}}+({\nabla \rho })\cdot {\mathbf {u} }+{\rho }(\nabla \cdot \mathbf {u} )={\frac {\partial \rho }{\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0\end{aligned}}} $$ $$ D m D t {\textstyle {\frac {\mathrm {D} m}{\mathrm {D} t}}} $$ $$ ρ {\displaystyle \rho } $$ $$ ∭ V ( F ( x 1 , x 2 , x 3 , t ) ) d V {\textstyle {\iiint \limits _{V}}(F(x_{1},x_{2},x_{3},t))dV} $$ $$ ∂ ∂ t {\textstyle {\frac {\partial }{\partial t}}} $$ $$ ∇ ⋅ u {\textstyle \nabla \cdot \mathbf {u} \,} $$ $$ u {\displaystyle \mathbf {u} } $$ $$ ∇ ρ {\textstyle {\nabla \rho }\,} $$ $$ ρ {\displaystyle \rho } $$ Note 1 - Refer to the mathematical operator del represented by the nabla ( ∇ \nabla ) symbol. $$ ∇ {\displaystyle \nabla } $$ to arrive at the conservation form of the equations of motion. This is often written: ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u ) = − ∇ p + ∇ ⋅ τ + ρ a {\frac {\partial {\partial t}}(\rho \,\mathbf {u} )+\nabla \cdot (\rho \,\mathbf {u} \otimes \mathbf {u} )=-\nabla p+\nabla \cdot {\boldsymbol {\tau }}+\rho \,\mathbf {a} } $$ ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u ) = − ∇ p + ∇ ⋅ τ + ρ a {\displaystyle {\frac {\partial }{\partial t}}(\rho \,\mathbf {u} )+\nabla \cdot (\rho \,\mathbf {u} \otimes \mathbf {u} )=-\nabla p+\nabla \cdot {\boldsymbol {\tau }}+\rho \,\mathbf {a} } $$ where ⊗ {\textstyle \otimes } is the outer product of the flow velocity ( u \mathbf {u } ): u ⊗ u = u u T \mathbf {u \otimes \mathbf {u} =\mathbf {u} \mathbf {u} ^{\mathrm {T} }} $$ ⊗ {\textstyle \otimes } $$ $$ u {\displaystyle \mathbf {u} } $$ $$ u ⊗ u = u u T {\displaystyle \mathbf {u} \otimes \mathbf {u} =\mathbf {u} \mathbf {u} ^{\mathrm {T} }} $$ The left side of the equation describes acceleration, and may be composed of time-dependent and convective components (also the effects of non-inertial coordinates if present). The right side of the equation is in effect a summation of hydrostatic effects, the divergence of deviatoric stress and body forces (such as gravity). All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below. A significant feature of the Cauchy equation and consequently all other continuum equations (including Euler and Navier–Stokes) is the presence of convective acceleration: the effect of acceleration of a flow with respect to space. While individual fluid particles indeed experience time-dependent acceleration, the convective acceleration of the flow field is a spatial effect, one example being fluid speeding up in a nozzle. Remark: here, the deviatoric stress tensor is denoted τ {\textstyle {\boldsymbol {\tau }}} as it was in the general continuum equations and in the incompressible flow section. $$ τ {\textstyle {\boldsymbol {\tau }}} $$ The compressible momentum Navier–Stokes equation results from the following assumptions on the Cauchy stress tensor: $$ ∇ u {\textstyle \nabla \mathbf {u} } $$ $$ ε ( ∇ u ) ≡ 1 2 ∇ u + 1 2 ( ∇ u ) T {\textstyle {\boldsymbol {\varepsilon }}\left(\nabla \mathbf {u} \right)\equiv {\frac {1}{2}}\nabla \mathbf {u} +{\frac {1}{2}}\left(\nabla \mathbf {u} \right)^{T}} $$ $$ σ ( ε ) = − p I + C : ε {\textstyle {\boldsymbol {\sigma }}({\boldsymbol {\varepsilon }})=-p\mathbf {I} +\mathbf {C} :{\boldsymbol {\varepsilon }}} $$ $$ p {\textstyle p} $$ $$ C {\textstyle \mathbf {C} } $$ $$ C {\textstyle \mathbf {C} } $$ $$ λ {\textstyle \lambda } $$ $$ μ {\textstyle \mu } $$ σ ( ε ) = − p I + λ tr ⁡ ( ε ) I + 2 μ ε {\boldsymbol {\sigma }({\boldsymbol {\varepsilon }})=-p\mathbf {I} +\lambda \operatorname {tr} ({\boldsymbol {\varepsilon }})\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}} $$ σ ( ε ) = − p I + λ tr ⁡ ( ε ) I + 2 μ ε {\displaystyle {\boldsymbol {\sigma }}({\boldsymbol {\varepsilon }})=-p\mathbf {I} +\lambda \operatorname {tr} ({\boldsymbol {\varepsilon }})\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}} $$ where I {\textstyle \mathbf {I} } is the identity tensor, and tr ⁡ ( ε ) {\textstyle \operatorname {tr} ({\boldsymbol {\varepsilon }})} is the trace of the rate-of-strain tensor. So this decomposition can be explicitly defined as: σ = − p I + λ ( ∇ ⋅ u ) I + μ ( ∇ u + ( ∇ u ) T ) . {\boldsymbol {\sigma }=-p\mathbf {I} +\lambda (\nabla \cdot \mathbf {u} )\mathbf {I} +\mu \left(\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }\right).} $$ I {\textstyle \mathbf {I} } $$ $$ tr ⁡ ( ε ) {\textstyle \operatorname {tr} ({\boldsymbol {\varepsilon }})} $$ $$ σ = − p I + λ ( ∇ ⋅ u ) I + μ ( ∇ u + ( ∇ u ) T ) . {\displaystyle {\boldsymbol {\sigma }}=-p\mathbf {I} +\lambda (\nabla \cdot \mathbf {u} )\mathbf {I} +\mu \left(\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }\right).} $$ Since the trace of the rate-of-strain tensor in three dimensions is the divergence (i.e. rate of expansion) of the flow: tr ⁡ ( ε ) = ∇ ⋅ u . \operatorname {tr ({\boldsymbol {\varepsilon }})=\nabla \cdot \mathbf {u} .} $$ tr ⁡ ( ε ) = ∇ ⋅ u . {\displaystyle \operatorname {tr} ({\boldsymbol {\varepsilon }})=\nabla \cdot \mathbf {u} .} $$ Given this relation, and since the trace of the identity tensor in three dimensions is three: tr ⁡ ( I ) = 3. \operatorname {tr ({\boldsymbol {I}})=3.} $$ tr ⁡ ( I ) = 3. {\displaystyle \operatorname {tr} ({\boldsymbol {I}})=3.} $$ the trace of the stress tensor in three dimensions becomes: tr ⁡ ( σ ) = − 3 p + ( 3 λ + 2 μ ) ∇ ⋅ u . \operatorname {tr ({\boldsymbol {\sigma }})=-3p+(3\lambda +2\mu )\nabla \cdot \mathbf {u} .} $$ tr ⁡ ( σ ) = − 3 p + ( 3 λ + 2 μ ) ∇ ⋅ u . {\displaystyle \operatorname {tr} ({\boldsymbol {\sigma }})=-3p+(3\lambda +2\mu )\nabla \cdot \mathbf {u} .} $$ So by alternatively decomposing the stress tensor into isotropic and deviatoric parts, as usual in fluid dynamics: σ = − [ p − ( λ + 2 3 μ ) ( ∇ ⋅ u ) ] I + μ ( ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ) {\boldsymbol {\sigma }=-\left[p-\left(\lambda +{\tfrac {2}{3}}\mu \right)\left(\nabla \cdot \mathbf {u} \right)\right]\mathbf {I} +\mu \left(\nabla \mathbf {u} +\left(\nabla \mathbf {u} \right)^{\mathrm {T} }-{\tfrac {2}{3}}\left(\nabla \cdot \mathbf {u} \right)\mathbf {I} \right)} $$ σ = − [ p − ( λ + 2 3 μ ) ( ∇ ⋅ u ) ] I + μ ( ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ) {\displaystyle {\boldsymbol {\sigma }}=-\left[p-\left(\lambda +{\tfrac {2}{3}}\mu \right)\left(\nabla \cdot \mathbf {u} \right)\right]\mathbf {I} +\mu \left(\nabla \mathbf {u} +\left(\nabla \mathbf {u} \right)^{\mathrm {T} }-{\tfrac {2}{3}}\left(\nabla \cdot \mathbf {u} \right)\mathbf {I} \right)} $$ Introducing the bulk viscosity ζ {\textstyle \zeta } , ζ ≡ λ + 2 3 μ , \zeta \equiv \lambda +{\tfrac {2{3}}\mu ,} $$ ζ {\textstyle \zeta } $$ $$ ζ ≡ λ + 2 3 μ , {\displaystyle \zeta \equiv \lambda +{\tfrac {2}{3}}\mu ,} $$ we arrive to the linear constitutive equation in the form usually employed in thermal hydraulics: σ = − [ p − ζ ( ∇ ⋅ u ) ] I + μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] {\boldsymbol {\sigma }=-[p-\zeta (\nabla \cdot \mathbf {u} )]\mathbf {I} +\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]} $$ σ = − [ p − ζ ( ∇ ⋅ u ) ] I + μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] {\displaystyle {\boldsymbol {\sigma }}=-[p-\zeta (\nabla \cdot \mathbf {u} )]\mathbf {I} +\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]} $$ which can also be arranged in the other usual form: σ = − p I + μ ( ∇ u + ( ∇ u ) T ) + ( ζ − 2 3 μ ) ( ∇ ⋅ u ) I . {\boldsymbol {\sigma }=-p\mathbf {I} +\mu \left(\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }\right)+\left(\zeta -{\frac {2}{3}}\mu \right)(\nabla \cdot \mathbf {u} )\mathbf {I} .} $$ σ = − p I + μ ( ∇ u + ( ∇ u ) T ) + ( ζ − 2 3 μ ) ( ∇ ⋅ u ) I . {\displaystyle {\boldsymbol {\sigma }}=-p\mathbf {I} +\mu \left(\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }\right)+\left(\zeta -{\frac {2}{3}}\mu \right)(\nabla \cdot \mathbf {u} )\mathbf {I} .} $$ Note that in the compressible case the pressure is no more proportional to the isotropic stress term, since there is the additional bulk viscosity term: p = − 1 3 tr ⁡ ( σ ) + ζ ( ∇ ⋅ u ) p=-{\frac {1{3}}\operatorname {tr} ({\boldsymbol {\sigma }})+\zeta (\nabla \cdot \mathbf {u} )} $$ p = − 1 3 tr ⁡ ( σ ) + ζ ( ∇ ⋅ u ) {\displaystyle p=-{\frac {1}{3}}\operatorname {tr} ({\boldsymbol {\sigma }})+\zeta (\nabla \cdot \mathbf {u} )} $$ and the deviatoric stress tensor σ ′ {\boldsymbol {\sigma }'} is still coincident with the shear stress tensor τ {\boldsymbol {\tau }} (i.e. the deviatoric stress in a Newtonian fluid has no normal stress components), and it has a compressibility term in addition to the incompressible case, which is proportional to the shear viscosity: $$ σ ′ {\displaystyle {\boldsymbol {\sigma }}'} $$ $$ τ {\displaystyle {\boldsymbol {\tau }}} $$ σ ′ = τ = μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] {\boldsymbol {\sigma }'={\boldsymbol {\tau }}=\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]} $$ σ ′ = τ = μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] {\displaystyle {\boldsymbol {\sigma }}'={\boldsymbol {\tau }}=\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]} $$ Both bulk viscosity ζ {\textstyle \zeta } and dynamic viscosity μ {\textstyle \mu } need not be constant – in general, they depend on two thermodynamics variables if the fluid contains a single chemical species, say for example, pressure and temperature. Any equation that makes explicit one of these transport coefficient in the conservation variables is called an equation of state. $$ ζ {\textstyle \zeta } $$ $$ μ {\textstyle \mu } $$ The most general of the Navier–Stokes equations become ρ D u D t = ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ∇ [ ζ ( ∇ ⋅ u ) ] + ρ a . \rho {\frac {\mathrm {D \mathbf {u} }{\mathrm {D} t}}=\rho \left({\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\nabla [\zeta (\nabla \cdot \mathbf {u} )]+\rho \mathbf {a} .} $$ ρ D u D t = ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ∇ [ ζ ( ∇ ⋅ u ) ] + ρ a . {\displaystyle \rho {\frac {\mathrm {D} \mathbf {u} }{\mathrm {D} t}}=\rho \left({\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\nabla [\zeta (\nabla \cdot \mathbf {u} )]+\rho \mathbf {a} .} $$ in index notation, the equation can be written as ρ ( ∂ u i ∂ t + u k ∂ u i ∂ x k ) = − ∂ p ∂ x i + ∂ ∂ x k [ μ ( ∂ u i ∂ x k + ∂ u k ∂ x i − 2 3 δ i k ∂ u l ∂ x l ) ] + ∂ ∂ x i ( ζ ∂ u l ∂ x l ) + ρ a i . \rho \left({\frac {\partial u_{i}{\partial t}}+u_{k}{\frac {\partial u_{i}}{\partial x_{k}}}\right)=-{\frac {\partial p}{\partial x_{i}}}+{\frac {\partial }{\partial x_{k}}}\left[\mu \left({\frac {\partial u_{i}}{\partial x_{k}}}+{\frac {\partial u_{k}}{\partial x_{i}}}-{\frac {2}{3}}\delta _{ik}{\frac {\partial u_{l}}{\partial x_{l}}}\right)\right]+{\frac {\partial }{\partial x_{i}}}\left(\zeta {\frac {\partial u_{l}}{\partial x_{l}}}\right)+\rho a_{i}.} $$ ρ ( ∂ u i ∂ t + u k ∂ u i ∂ x k ) = − ∂ p ∂ x i + ∂ ∂ x k [ μ ( ∂ u i ∂ x k + ∂ u k ∂ x i − 2 3 δ i k ∂ u l ∂ x l ) ] + ∂ ∂ x i ( ζ ∂ u l ∂ x l ) + ρ a i . {\displaystyle \rho \left({\frac {\partial u_{i}}{\partial t}}+u_{k}{\frac {\partial u_{i}}{\partial x_{k}}}\right)=-{\frac {\partial p}{\partial x_{i}}}+{\frac {\partial }{\partial x_{k}}}\left[\mu \left({\frac {\partial u_{i}}{\partial x_{k}}}+{\frac {\partial u_{k}}{\partial x_{i}}}-{\frac {2}{3}}\delta _{ik}{\frac {\partial u_{l}}{\partial x_{l}}}\right)\right]+{\frac {\partial }{\partial x_{i}}}\left(\zeta {\frac {\partial u_{l}}{\partial x_{l}}}\right)+\rho a_{i}.} $$ The corresponding equation in conservation form can be obtained by considering that, given the mass continuity equation, the left side is equivalent to: ρ D u D t = ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u ) \rho {\frac {\mathrm {D \mathbf {u} }{\mathrm {D} t}}={\frac {\partial }{\partial t}}(\rho \mathbf {u} )+\nabla \cdot (\rho \mathbf {u} \otimes \mathbf {u} )} $$ ρ D u D t = ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u ) {\displaystyle \rho {\frac {\mathrm {D} \mathbf {u} }{\mathrm {D} t}}={\frac {\partial }{\partial t}}(\rho \mathbf {u} )+\nabla \cdot (\rho \mathbf {u} \otimes \mathbf {u} )} $$ To give finally: ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u + [ p − ζ ( ∇ ⋅ u ) ] I − μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] ) = ρ a . {\frac {\partial {\partial t}}(\rho \mathbf {u} )+\nabla \cdot \left(\rho \mathbf {u} \otimes \mathbf {u} +[p-\zeta (\nabla \cdot \mathbf {u} )]\mathbf {I} -\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right)=\rho \mathbf {a} .} $$ ∂ ∂ t ( ρ u ) + ∇ ⋅ ( ρ u ⊗ u + [ p − ζ ( ∇ ⋅ u ) ] I − μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] ) = ρ a . {\displaystyle {\frac {\partial }{\partial t}}(\rho \mathbf {u} )+\nabla \cdot \left(\rho \mathbf {u} \otimes \mathbf {u} +[p-\zeta (\nabla \cdot \mathbf {u} )]\mathbf {I} -\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right)=\rho \mathbf {a} .} $$ Apart from its dependence of pressure and temperature, the second viscosity coefficient also depends on the process, that is to say, the second viscosity coefficient is not just a material property. Example: in the case of a sound wave with a definitive frequency that alternatively compresses and expands a fluid element, the second viscosity coefficient depends on the frequency of the wave. This dependence is called the dispersion. In some cases, the second viscosity ζ {\textstyle \zeta } can be assumed to be constant in which case, the effect of the volume viscosity ζ {\textstyle \zeta } is that the mechanical pressure is not equivalent to the thermodynamic pressure: as demonstrated below. ∇ ⋅ ( ∇ ⋅ u ) I = ∇ ( ∇ ⋅ u ) , \nabla \cdot (\nabla \cdot \mathbf {u )\mathbf {I} =\nabla (\nabla \cdot \mathbf {u} ),} p ¯ ≡ p − ζ ∇ ⋅ u , {\bar {p}\equiv p-\zeta \,\nabla \cdot \mathbf {u} ,} However, this difference is usually neglected most of the time (that is whenever we are not dealing with processes such as sound absorption and attenuation of shock waves, where second viscosity coefficient becomes important) by explicitly assuming ζ = 0 {\textstyle \zeta =0} . The assumption of setting ζ = 0 {\textstyle \zeta =0} is called as the Stokes hypothesis. The validity of Stokes hypothesis can be demonstrated for monoatomic gas both experimentally and from the kinetic theory; for other gases and liquids, Stokes hypothesis is generally incorrect. With the Stokes hypothesis, the Navier–Stokes equations become $$ ζ {\textstyle \zeta } $$ $$ ζ {\textstyle \zeta } $$ $$ ∇ ⋅ ( ∇ ⋅ u ) I = ∇ ( ∇ ⋅ u ) , {\displaystyle \nabla \cdot (\nabla \cdot \mathbf {u} )\mathbf {I} =\nabla (\nabla \cdot \mathbf {u} ),} $$ $$ p ¯ ≡ p − ζ ∇ ⋅ u , {\displaystyle {\bar {p}}\equiv p-\zeta \,\nabla \cdot \mathbf {u} ,} $$ $$ ζ = 0 {\textstyle \zeta =0} $$ $$ ζ = 0 {\textstyle \zeta =0} $$ ρ D u D t = ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ρ a . \rho {\frac {\mathrm {D \mathbf {u} }{\mathrm {D} t}}=\rho \left({\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\rho \mathbf {a} .} $$ ρ D u D t = ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ρ a . {\displaystyle \rho {\frac {\mathrm {D} \mathbf {u} }{\mathrm {D} t}}=\rho \left({\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\rho \mathbf {a} .} $$ If the dynamic μ and bulk ζ \zeta viscosities are assumed to be uniform in space, the equations in convective form can be simplified further. By computing the divergence of the stress tensor, since the divergence of tensor ∇ u {\textstyle \nabla \mathbf {u} } is ∇ 2 u {\textstyle \nabla ^{2}\mathbf {u} } and the divergence of tensor ( ∇ u ) T {\textstyle \left(\nabla \mathbf {u} \right)^{\mathrm {T} }} is ∇ ( ∇ ⋅ u ) {\textstyle \nabla \left(\nabla \cdot \mathbf {u} \right)} , one finally arrives to the compressible Navier–Stokes momentum equation: $$ ζ {\displaystyle \zeta } $$ $$ ∇ u {\textstyle \nabla \mathbf {u} } $$ $$ ∇ 2 u {\textstyle \nabla ^{2}\mathbf {u} } $$ $$ ( ∇ u ) T {\textstyle \left(\nabla \mathbf {u} \right)^{\mathrm {T} }} $$ $$ ∇ ( ∇ ⋅ u ) {\textstyle \nabla \left(\nabla \cdot \mathbf {u} \right)} $$ D u D t = − 1 ρ ∇ p + ν ∇ 2 u + ( 1 3 ν + ξ ) ∇ ( ∇ ⋅ u ) + a . {\frac {D\mathbf {u }{Dt}}=-{\frac {1}{\rho }}\nabla p+\nu \,\nabla ^{2}\mathbf {u} +({\tfrac {1}{3}}\nu +\xi )\,\nabla (\nabla \cdot \mathbf {u} )+\mathbf {a} .} $$ D u D t = − 1 ρ ∇ p + ν ∇ 2 u + ( 1 3 ν + ξ ) ∇ ( ∇ ⋅ u ) + a . {\displaystyle {\frac {D\mathbf {u} }{Dt}}=-{\frac {1}{\rho }}\nabla p+\nu \,\nabla ^{2}\mathbf {u} +({\tfrac {1}{3}}\nu +\xi )\,\nabla (\nabla \cdot \mathbf {u} )+\mathbf {a} .} $$ where D D t {\textstyle {\frac {\mathrm {D} }{\mathrm {D} t}}} is the material derivative. ν = μ ρ \nu ={\frac {\mu {\rho }}} is the shear kinematic viscosity and ξ = ζ ρ \xi ={\frac {\zeta {\rho }}} is the bulk kinematic viscosity. The left-hand side changes in the conservation form of the Navier–Stokes momentum equation. By bringing the operator on the flow velocity on the left side, on also has: $$ D D t {\textstyle {\frac {\mathrm {D} }{\mathrm {D} t}}} $$ $$ ν = μ ρ {\displaystyle \nu ={\frac {\mu }{\rho }}} $$ $$ ξ = ζ ρ {\displaystyle \xi ={\frac {\zeta }{\rho }}} $$ ( ∂ ∂ t + u ⋅ ∇ − ν ∇ 2 − ( 1 3 ν + ξ ) ∇ ( ∇ ⋅ ) ) u = − 1 ρ ∇ p + a . \left({\frac {\partial {\partial t}}+\mathbf {u} \cdot \nabla -\nu \,\nabla ^{2}-({\tfrac {1}{3}}\nu +\xi )\,\nabla (\nabla \cdot )\right)\mathbf {u} =-{\frac {1}{\rho }}\nabla p+\mathbf {a} .} $$ ( ∂ ∂ t + u ⋅ ∇ − ν ∇ 2 − ( 1 3 ν + ξ ) ∇ ( ∇ ⋅ ) ) u = − 1 ρ ∇ p + a . {\displaystyle \left({\frac {\partial }{\partial t}}+\mathbf {u} \cdot \nabla -\nu \,\nabla ^{2}-({\tfrac {1}{3}}\nu +\xi )\,\nabla (\nabla \cdot )\right)\mathbf {u} =-{\frac {1}{\rho }}\nabla p+\mathbf {a} .} $$ The convective acceleration term can also be written as u ⋅ ∇ u = ( ∇ × u ) × u + 1 2 ∇ u 2 , \mathbf {u \cdot \nabla \mathbf {u} =(\nabla \times \mathbf {u} )\times \mathbf {u} +{\tfrac {1}{2}}\nabla \mathbf {u} ^{2},} where the vector ( ∇ × u ) × u {\textstyle (\nabla \times \mathbf {u} )\times \mathbf {u} } is known as the Lamb vector. $$ u ⋅ ∇ u = ( ∇ × u ) × u + 1 2 ∇ u 2 , {\displaystyle \mathbf {u} \cdot \nabla \mathbf {u} =(\nabla \times \mathbf {u} )\times \mathbf {u} +{\tfrac {1}{2}}\nabla \mathbf {u} ^{2},} $$ $$ ( ∇ × u ) × u {\textstyle (\nabla \times \mathbf {u} )\times \mathbf {u} } $$ For the special case of an incompressible flow, the pressure constrains the flow so that the volume of fluid elements is constant: isochoric flow resulting in a solenoidal velocity field with ∇ ⋅ u = 0 {\textstyle \nabla \cdot \mathbf {u} =0} . $$ ∇ ⋅ u = 0 {\textstyle \nabla \cdot \mathbf {u} =0} $$ The incompressible momentum Navier–Stokes equation results from the following assumptions on the Cauchy stress tensor: $$ ∇ u {\textstyle \nabla \mathbf {u} } $$ $$ τ {\textstyle {\boldsymbol {\tau }}} $$ $$ μ {\textstyle \mu } $$ $$ τ = 2 μ ε {\displaystyle {\boldsymbol {\tau }}=2\mu {\boldsymbol {\varepsilon }}} $$ where ε = 1 2 ( ∇ u + ∇ u T ) {\boldsymbol {\varepsilon }={\tfrac {1}{2}}\left(\mathbf {\nabla u} +\mathbf {\nabla u} ^{\mathrm {T} }\right)} is the rate-of-strain tensor. So this decomposition can be made explicit as: $$ ε = 1 2 ( ∇ u + ∇ u T ) {\displaystyle {\boldsymbol {\varepsilon }}={\tfrac {1}{2}}\left(\mathbf {\nabla u} +\mathbf {\nabla u} ^{\mathrm {T} }\right)} $$ $$ τ = μ [ ∇ u + ( ∇ u ) T ] {\displaystyle {\boldsymbol {\tau }}=\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }\right]} $$ This is constitutive equation is also called the Newtonian law of viscosity. Dynamic viscosity μ need not be constant – in incompressible flows it can depend on density and on pressure. Any equation that makes explicit one of these transport coefficient in the conservative variables is called an equation of state. The divergence of the deviatoric stress in case of uniform viscosity is given by: ∇ ⋅ τ = 2 μ ∇ ⋅ ε = μ ∇ ⋅ ( ∇ u + ∇ u T ) = μ ∇ 2 u \nabla \cdot {\boldsymbol {\tau }=2\mu \nabla \cdot {\boldsymbol {\varepsilon }}=\mu \nabla \cdot \left(\nabla \mathbf {u} +\nabla \mathbf {u} ^{\mathrm {T} }\right)=\mu \,\nabla ^{2}\mathbf {u} } because ∇ ⋅ u = 0 {\textstyle \nabla \cdot \mathbf {u} =0} for an incompressible fluid. $$ ∇ ⋅ τ = 2 μ ∇ ⋅ ε = μ ∇ ⋅ ( ∇ u + ∇ u T ) = μ ∇ 2 u {\displaystyle \nabla \cdot {\boldsymbol {\tau }}=2\mu \nabla \cdot {\boldsymbol {\varepsilon }}=\mu \nabla \cdot \left(\nabla \mathbf {u} +\nabla \mathbf {u} ^{\mathrm {T} }\right)=\mu \,\nabla ^{2}\mathbf {u} } $$ $$ ∇ ⋅ u = 0 {\textstyle \nabla \cdot \mathbf {u} =0} $$ Incompressibility rules out density and pressure waves like sound or shock waves, so this simplification is not useful if these phenomena are of interest. The incompressible flow assumption typically holds well with all fluids at low Mach numbers (say up to about Mach 0.3), such as for modelling air winds at normal temperatures. the incompressible Navier–Stokes equations are best visualized by dividing for the density: D u D t = ∂ u ∂ t + ( u ⋅ ∇ ) u = ν ∇ 2 u − 1 ρ ∇ p + f . {\frac {D\mathbf {u }{Dt}}={\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} =\nu \,\nabla ^{2}\mathbf {u} -{\frac {1}{\rho }}\nabla p+\mathbf {f} .} $$ D u D t = ∂ u ∂ t + ( u ⋅ ∇ ) u = ν ∇ 2 u − 1 ρ ∇ p + f . {\displaystyle {\frac {D\mathbf {u} }{Dt}}={\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} =\nu \,\nabla ^{2}\mathbf {u} -{\frac {1}{\rho }}\nabla p+\mathbf {f} .} $$ where ν = μ ρ {\textstyle \nu ={\frac {\mu }{\rho }}} is called the kinematic viscosity. By isolating the fluid velocity, one can also state: $$ ν = μ ρ {\textstyle \nu ={\frac {\mu }{\rho }}} $$ ( ∂ ∂ t + u ⋅ ∇ − ν ∇ 2 ) u = − 1 ρ ∇ p + f . \left({\frac {\partial {\partial t}}+\mathbf {u} \cdot \nabla -\nu \,\nabla ^{2}\right)\mathbf {u} =-{\frac {1}{\rho }}\nabla p+\mathbf {f} .} $$ ( ∂ ∂ t + u ⋅ ∇ − ν ∇ 2 ) u = − 1 ρ ∇ p + f . {\displaystyle \left({\frac {\partial }{\partial t}}+\mathbf {u} \cdot \nabla -\nu \,\nabla ^{2}\right)\mathbf {u} =-{\frac {1}{\rho }}\nabla p+\mathbf {f} .} $$ If the density is constant throughout the fluid domain, or, in other words, if all fluid elements have the same density, ρ {\textstyle \rho } , then we have $$ ρ {\textstyle \rho } $$ D u D t = ν ∇ 2 u − ∇ p ρ + f , {\frac {D\mathbf {u }{Dt}}=\nu \,\nabla ^{2}\mathbf {u} -\nabla {\frac {p}{\rho }}+\mathbf {f} ,} $$ D u D t = ν ∇ 2 u − ∇ p ρ + f , {\displaystyle {\frac {D\mathbf {u} }{Dt}}=\nu \,\nabla ^{2}\mathbf {u} -\nabla {\frac {p}{\rho }}+\mathbf {f} ,} $$ where p / ρ {\textstyle p/\rho } is called the unit pressure head. $$ p / ρ {\textstyle p/\rho } $$ In incompressible flows, the pressure field satisfies the Poisson equation, $$ ∇ 2 p = − ρ ∂ u i ∂ x k ∂ u k ∂ x i = − ρ ∂ 2 u i u k ∂ x k x i , {\displaystyle \nabla ^{2}p=-\rho {\frac {\partial u_{i}}{\partial x_{k}}}{\frac {\partial u_{k}}{\partial x_{i}}}=-\rho {\frac {\partial ^{2}u_{i}u_{k}}{\partial x_{k}x_{i}}},} $$ which is obtained by taking the divergence of the momentum equations. Velocity profile (laminar flow): u x = u ( y ) , u y = 0 , u z = 0 u_{x=u(y),\quad u_{y}=0,\quad u_{z}=0} for the x-direction, simplify the Navier–Stokes equation: 0 = − d P d x + μ ( d 2 u d y 2 ) 0=-{\frac {\mathrm {d P}{\mathrm {d} x}}+\mu \left({\frac {\mathrm {d} ^{2}u}{\mathrm {d} y^{2}}}\right)} $$ u x = u ( y ) , u y = 0 , u z = 0 {\displaystyle u_{x}=u(y),\quad u_{y}=0,\quad u_{z}=0} $$ $$ 0 = − d P d x + μ ( d 2 u d y 2 ) {\displaystyle 0=-{\frac {\mathrm {d} P}{\mathrm {d} x}}+\mu \left({\frac {\mathrm {d} ^{2}u}{\mathrm {d} y^{2}}}\right)} $$ Integrate twice to find the velocity profile with boundary conditions y = h, u = 0, y = −h, u = 0: u = 1 2 μ d P d x y 2 + A y + B u={\frac {1{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}y^{2}+Ay+B} $$ u = 1 2 μ d P d x y 2 + A y + B {\displaystyle u={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}y^{2}+Ay+B} $$ From this equation, substitute in the two boundary conditions to get two equations: 0 = 1 2 μ d P d x h 2 + A h + B 0 = 1 2 μ d P d x h 2 − A h + B {\begin{aligned0&={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}+Ah+B\\0&={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}-Ah+B\end{aligned}}} $$ 0 = 1 2 μ d P d x h 2 + A h + B 0 = 1 2 μ d P d x h 2 − A h + B {\displaystyle {\begin{aligned}0&={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}+Ah+B\\0&={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}-Ah+B\end{aligned}}} $$ Add and solve for B: B = − 1 2 μ d P d x h 2 B=-{\frac {1{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}} $$ B = − 1 2 μ d P d x h 2 {\displaystyle B=-{\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}h^{2}} $$ Substitute and solve for A: A = 0 A=0 $$ A = 0 {\displaystyle A=0} $$ Finally this gives the velocity profile: u = 1 2 μ d P d x ( y 2 − h 2 ) u={\frac {1{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}\left(y^{2}-h^{2}\right)} $$ u = 1 2 μ d P d x ( y 2 − h 2 ) {\displaystyle u={\frac {1}{2\mu }}{\frac {\mathrm {d} P}{\mathrm {d} x}}\left(y^{2}-h^{2}\right)} $$ It is well worth observing the meaning of each term (compare to the Cauchy momentum equation): ∂ u ∂ t ⏟ Variation + ( u ⋅ ∇ ) u ⏟ Convective acceleration ⏞ Inertia (per volume) = ∂ ∂ − ∇ w ⏟ Internal source + ν ∇ 2 u ⏟ Diffusion ⏞ Divergence of stress + g ⏟ External source . \overbrace {{\vphantom {\frac {{}}}\underbrace {\frac {\partial \mathbf {u} }{\partial t}} _{\text{Variation}}+\underbrace {{\vphantom {\frac {}{}}}(\mathbf {u} \cdot \nabla )\mathbf {u} } _{\begin{smallmatrix}{\text{Convective}}\\{\text{acceleration}}\end{smallmatrix}}} ^{\text{Inertia (per volume)}}=\overbrace {{\vphantom {\frac {\partial }{\partial }}}\underbrace {{\vphantom {\frac {}{}}}-\nabla w} _{\begin{smallmatrix}{\text{Internal}}\\{\text{source}}\end{smallmatrix}}+\underbrace {{\vphantom {\frac {}{}}}\nu \nabla ^{2}\mathbf {u} } _{\text{Diffusion}}} ^{\text{Divergence of stress}}+\underbrace {{\vphantom {\frac {}{}}}\mathbf {g} } _{\begin{smallmatrix}{\text{External}}\\{\text{source}}\end{smallmatrix}}.} $$ ∂ u ∂ t ⏟ Variation + ( u ⋅ ∇ ) u ⏟ Convective acceleration ⏞ Inertia (per volume) = ∂ ∂ − ∇ w ⏟ Internal source + ν ∇ 2 u ⏟ Diffusion ⏞ Divergence of stress + g ⏟ External source . {\displaystyle \overbrace {{\vphantom {\frac {}{}}}\underbrace {\frac {\partial \mathbf {u} }{\partial t}} _{\text{Variation}}+\underbrace {{\vphantom {\frac {}{}}}(\mathbf {u} \cdot \nabla )\mathbf {u} } _{\begin{smallmatrix}{\text{Convective}}\\{\text{acceleration}}\end{smallmatrix}}} ^{\text{Inertia (per volume)}}=\overbrace {{\vphantom {\frac {\partial }{\partial }}}\underbrace {{\vphantom {\frac {}{}}}-\nabla w} _{\begin{smallmatrix}{\text{Internal}}\\{\text{source}}\end{smallmatrix}}+\underbrace {{\vphantom {\frac {}{}}}\nu \nabla ^{2}\mathbf {u} } _{\text{Diffusion}}} ^{\text{Divergence of stress}}+\underbrace {{\vphantom {\frac {}{}}}\mathbf {g} } _{\begin{smallmatrix}{\text{External}}\\{\text{source}}\end{smallmatrix}}.} $$ The higher-order term, namely the shear stress divergence ∇ ⋅ τ {\textstyle \nabla \cdot {\boldsymbol {\tau }}} , has simply reduced to the vector Laplacian term μ ∇ 2 u {\textstyle \mu \nabla ^{2}\mathbf {u} } . This Laplacian term can be interpreted as the difference between the velocity at a point and the mean velocity in a small surrounding volume. This implies that – for a Newtonian fluid – viscosity operates as a diffusion of momentum, in much the same way as the heat conduction. In fact neglecting the convection term, incompressible Navier–Stokes equations lead to a vector diffusion equation (namely Stokes equations), but in general the convection term is present, so incompressible Navier–Stokes equations belong to the class of convection–diffusion equations. $$ ∇ ⋅ τ {\textstyle \nabla \cdot {\boldsymbol {\tau }}} $$ $$ μ ∇ 2 u {\textstyle \mu \nabla ^{2}\mathbf {u} } $$ In the usual case of an external field being a conservative field: g = − ∇ φ \mathbf {g =-\nabla \varphi } by defining the hydraulic head: h ≡ w + φ h\equiv w+\varphi $$ g = − ∇ φ {\displaystyle \mathbf {g} =-\nabla \varphi } $$ $$ h ≡ w + φ {\displaystyle h\equiv w+\varphi } $$ one can finally condense the whole source in one term, arriving to the incompressible Navier–Stokes equation with conservative external field: ∂ u ∂ t + ( u ⋅ ∇ ) u − ν ∇ 2 u = − ∇ h . {\frac {\partial \mathbf {u }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} -\nu \,\nabla ^{2}\mathbf {u} =-\nabla h.} $$ ∂ u ∂ t + ( u ⋅ ∇ ) u − ν ∇ 2 u = − ∇ h . {\displaystyle {\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} -\nu \,\nabla ^{2}\mathbf {u} =-\nabla h.} $$ The incompressible Navier–Stokes equations with uniform density and viscosity and conservative external field is the fundamental equation of hydraulics. The domain for these equations is commonly a 3 or less dimensional Euclidean space, for which an orthogonal coordinate reference frame is usually set to explicit the system of scalar partial differential equations to be solved. In 3-dimensional orthogonal coordinate systems are 3: Cartesian, cylindrical, and spherical. Expressing the Navier–Stokes vector equation in Cartesian coordinates is quite straightforward and not much influenced by the number of dimensions of the euclidean space employed, and this is the case also for the first-order terms (like the variation and convection ones) also in non-cartesian orthogonal coordinate systems. But for the higher order terms (the two coming from the divergence of the deviatoric stress that distinguish Navier–Stokes equations from Euler equations) some tensor calculus is required for deducing an expression in non-cartesian orthogonal coordinate systems. A special case of the fundamental equation of hydraulics is the Bernoulli's equation. The incompressible Navier–Stokes equation is composite, the sum of two orthogonal equations, ∂ u ∂ t = Π S ( − ( u ⋅ ∇ ) u + ν ∇ 2 u ) + f S ρ − 1 ∇ p = Π I ( − ( u ⋅ ∇ ) u + ν ∇ 2 u ) + f I {\begin{aligned{\frac {\partial \mathbf {u} }{\partial t}}&=\Pi ^{S}\left(-(\mathbf {u} \cdot \nabla )\mathbf {u} +\nu \,\nabla ^{2}\mathbf {u} \right)+\mathbf {f} ^{S}\\\rho ^{-1}\,\nabla p&=\Pi ^{I}\left(-(\mathbf {u} \cdot \nabla )\mathbf {u} +\nu \,\nabla ^{2}\mathbf {u} \right)+\mathbf {f} ^{I}\end{aligned}}} where Π S {\textstyle \Pi ^{S}} and Π I {\textstyle \Pi ^{I}} are solenoidal and irrotational projection operators satisfying Π S + Π I = 1 {\textstyle \Pi ^{S}+\Pi ^{I}=1} , and f S {\textstyle \mathbf {f} ^{S}} and f I {\textstyle \mathbf {f} ^{I}} are the non-conservative and conservative parts of the body force. This result follows from the Helmholtz theorem (also known as the fundamental theorem of vector calculus). The first equation is a pressureless governing equation for the velocity, while the second equation for the pressure is a functional of the velocity and is related to the pressure Poisson equation. $$ ∂ u ∂ t = Π S ( − ( u ⋅ ∇ ) u + ν ∇ 2 u ) + f S ρ − 1 ∇ p = Π I ( − ( u ⋅ ∇ ) u + ν ∇ 2 u ) + f I {\displaystyle {\begin{aligned}{\frac {\partial \mathbf {u} }{\partial t}}&=\Pi ^{S}\left(-(\mathbf {u} \cdot \nabla )\mathbf {u} +\nu \,\nabla ^{2}\mathbf {u} \right)+\mathbf {f} ^{S}\\\rho ^{-1}\,\nabla p&=\Pi ^{I}\left(-(\mathbf {u} \cdot \nabla )\mathbf {u} +\nu \,\nabla ^{2}\mathbf {u} \right)+\mathbf {f} ^{I}\end{aligned}}} $$ $$ Π S {\textstyle \Pi ^{S}} $$ $$ Π I {\textstyle \Pi ^{I}} $$ $$ Π S + Π I = 1 {\textstyle \Pi ^{S}+\Pi ^{I}=1} $$ $$ f S {\textstyle \mathbf {f} ^{S}} $$ $$ f I {\textstyle \mathbf {f} ^{I}} $$ The explicit functional form of the projection operator in 3D is found from the Helmholtz Theorem: Π S F ( r ) = 1 4 π ∇ × ∫ ∇ ′ × F ( r ′ ) | r − r ′ | d V ′ , Π I = 1 − Π S \Pi ^{S\,\mathbf {F} (\mathbf {r} )={\frac {1}{4\pi }}\nabla \times \int {\frac {\nabla ^{\prime }\times \mathbf {F} (\mathbf {r} ')}{|\mathbf {r} -\mathbf {r} '|}}\,\mathrm {d} V',\quad \Pi ^{I}=1-\Pi ^{S}} with a similar structure in 2D. Thus the governing equation is an integro-differential equation similar to Coulomb and Biot–Savart law, not convenient for numerical computation. $$ Π S F ( r ) = 1 4 π ∇ × ∫ ∇ ′ × F ( r ′ ) | r − r ′ | d V ′ , Π I = 1 − Π S {\displaystyle \Pi ^{S}\,\mathbf {F} (\mathbf {r} )={\frac {1}{4\pi }}\nabla \times \int {\frac {\nabla ^{\prime }\times \mathbf {F} (\mathbf {r} ')}{|\mathbf {r} -\mathbf {r} '|}}\,\mathrm {d} V',\quad \Pi ^{I}=1-\Pi ^{S}} $$ An equivalent weak or variational form of the equation, proved to produce the same velocity solution as the Navier–Stokes equation, is given by, ( w , ∂ u ∂ t ) = − ( w , ( u ⋅ ∇ ) u ) − ν ( ∇ w : ∇ u ) + ( w , f S ) \left(\mathbf {w ,{\frac {\partial \mathbf {u} }{\partial t}}\right)=-{\bigl (}\mathbf {w} ,\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} {\bigr )}-\nu \left(\nabla \mathbf {w} :\nabla \mathbf {u} \right)+\left(\mathbf {w} ,\mathbf {f} ^{S}\right)} $$ ( w , ∂ u ∂ t ) = − ( w , ( u ⋅ ∇ ) u ) − ν ( ∇ w : ∇ u ) + ( w , f S ) {\displaystyle \left(\mathbf {w} ,{\frac {\partial \mathbf {u} }{\partial t}}\right)=-{\bigl (}\mathbf {w} ,\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} {\bigr )}-\nu \left(\nabla \mathbf {w} :\nabla \mathbf {u} \right)+\left(\mathbf {w} ,\mathbf {f} ^{S}\right)} $$ for divergence-free test functions w {\textstyle \mathbf {w} } satisfying appropriate boundary conditions. Here, the projections are accomplished by the orthogonality of the solenoidal and irrotational function spaces. The discrete form of this is eminently suited to finite element computation of divergence-free flow, as we shall see in the next section. There one will be able to address the question "How does one specify pressure-driven (Poiseuille) problems with a pressureless governing equation?". $$ w {\textstyle \mathbf {w} } $$ The absence of pressure forces from the governing velocity equation demonstrates that the equation is not a dynamic one, but rather a kinematic equation where the divergence-free condition serves the role of a conservation equation. This all would seem to refute the frequent statements that the incompressible pressure enforces the divergence-free condition. Consider the incompressible Navier–Stokes equations for a Newtonian fluid of constant density ρ {\textstyle \rho } in a domain Ω ⊂ R d ( d = 2 , 3 ) \Omega \subset \mathbb {R ^{d}\quad (d=2,3)} with boundary ∂ Ω = Γ D ∪ Γ N , \partial \Omega =\Gamma _{D\cup \Gamma _{N},} being Γ D {\textstyle \Gamma _{D}} and Γ N {\textstyle \Gamma _{N}} portions of the boundary where respectively a Dirichlet and a Neumann boundary condition is applied ( Γ D ∩ Γ N = ∅ {\textstyle \Gamma _{D}\cap \Gamma _{N}=\emptyset } ): { ρ ∂ u ∂ t + ρ ( u ⋅ ∇ ) u − ∇ ⋅ σ ( u , p ) = f in Ω × ( 0 , T ) ∇ ⋅ u = 0 in Ω × ( 0 , T ) u = g on Γ D × ( 0 , T ) σ ( u , p ) n ^ = h on Γ N × ( 0 , T ) u ( 0 ) = u 0 in Ω × { 0 } {\begin{cases\rho {\dfrac {\partial \mathbf {u} }{\partial t}}+\rho (\mathbf {u} \cdot \nabla )\mathbf {u} -\nabla \cdot {\boldsymbol {\sigma }}(\mathbf {u} ,p)=\mathbf {f} &{\text{ in }}\Omega \times (0,T)\\\nabla \cdot \mathbf {u} =0&{\text{ in }}\Omega \times (0,T)\\\mathbf {u} =\mathbf {g} &{\text{ on }}\Gamma _{D}\times (0,T)\\{\boldsymbol {\sigma }}(\mathbf {u} ,p){\hat {\mathbf {n} }}=\mathbf {h} &{\text{ on }}\Gamma _{N}\times (0,T)\\\mathbf {u} (0)=\mathbf {u} _{0}&{\text{ in }}\Omega \times \{0\}\end{cases}}} u {\textstyle \mathbf {u} } is the fluid velocity, p {\textstyle p} the fluid pressure, f {\textstyle \mathbf {f} } a given forcing term, n ^ {\hat {\mathbf {n }}} the outward directed unit normal vector to Γ N {\textstyle \Gamma _{N}} , and σ ( u , p ) {\textstyle {\boldsymbol {\sigma }}(\mathbf {u} ,p)} the viscous stress tensor defined as: σ ( u , p ) = − p I + 2 μ ε ( u ) . {\boldsymbol {\sigma }(\mathbf {u} ,p)=-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} ).} Let μ {\textstyle \mu } be the dynamic viscosity of the fluid, I {\textstyle \mathbf {I} } the second-order identity tensor and ε ( u ) {\textstyle {\boldsymbol {\varepsilon }}(\mathbf {u} )} the strain-rate tensor defined as: ε ( u ) = 1 2 ( ( ∇ u ) + ( ∇ u ) T ) . {\boldsymbol {\varepsilon }(\mathbf {u} )={\frac {1}{2}}\left(\left(\nabla \mathbf {u} \right)+\left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right).} The functions g {\textstyle \mathbf {g} } and h {\textstyle \mathbf {h} } are given Dirichlet and Neumann boundary data, while u 0 {\textstyle \mathbf {u} _{0}} is the initial condition. The first equation is the momentum balance equation, while the second represents the mass conservation, namely the continuity equation. Assuming constant dynamic viscosity, using the vectorial identity ∇ ⋅ ( ∇ f ) T = ∇ ( ∇ ⋅ f ) \nabla \cdot \left(\nabla \mathbf {f \right)^{\mathrm {T} }=\nabla (\nabla \cdot \mathbf {f} )} and exploiting mass conservation, the divergence of the total stress tensor in the momentum equation can also be expressed as: ∇ ⋅ σ ( u , p ) = ∇ ⋅ ( − p I + 2 μ ε ( u ) ) = − ∇ p + 2 μ ∇ ⋅ ε ( u ) = − ∇ p + 2 μ ∇ ⋅ [ 1 2 ( ( ∇ u ) + ( ∇ u ) T ) ] = − ∇ p + μ ( Δ u + ∇ ⋅ ( ∇ u ) T ) = − ∇ p + μ ( Δ u + ∇ ( ∇ ⋅ u ) ⏟ = 0 ) = − ∇ p + μ Δ u . {\begin{aligned\nabla \cdot {\boldsymbol {\sigma }}(\mathbf {u} ,p)&=\nabla \cdot \left(-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} )\right)\\&=-\nabla p+2\mu \nabla \cdot {\boldsymbol {\varepsilon }}(\mathbf {u} )\\&=-\nabla p+2\mu \nabla \cdot \left[{\tfrac {1}{2}}\left(\left(\nabla \mathbf {u} \right)+\left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right)\right]\\&=-\nabla p+\mu \left(\Delta \mathbf {u} +\nabla \cdot \left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right)\\&=-\nabla p+\mu {\bigl (}\Delta \mathbf {u} +\nabla \underbrace {(\nabla \cdot \mathbf {u} )} _{=0}{\bigr )}=-\nabla p+\mu \,\Delta \mathbf {u} .\end{aligned}}} Moreover, note that the Neumann boundary conditions can be rearranged as: σ ( u , p ) n ^ = ( − p I + 2 μ ε ( u ) ) n ^ = − p n ^ + μ ∂ u ∂ n ^ . {\boldsymbol {\sigma }(\mathbf {u} ,p){\hat {\mathbf {n} }}=\left(-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} )\right){\hat {\mathbf {n} }}=-p{\hat {\mathbf {n} }}+\mu {\frac {\partial {\boldsymbol {u}}}{\partial {\hat {\mathbf {n} }}}}.} $$ ρ {\textstyle \rho } $$ $$ Ω ⊂ R d ( d = 2 , 3 ) {\displaystyle \Omega \subset \mathbb {R} ^{d}\quad (d=2,3)} $$ $$ ∂ Ω = Γ D ∪ Γ N , {\displaystyle \partial \Omega =\Gamma _{D}\cup \Gamma _{N},} $$ $$ Γ D {\textstyle \Gamma _{D}} $$ $$ Γ N {\textstyle \Gamma _{N}} $$ $$ Γ D ∩ Γ N = ∅ {\textstyle \Gamma _{D}\cap \Gamma _{N}=\emptyset } $$ $$ { ρ ∂ u ∂ t + ρ ( u ⋅ ∇ ) u − ∇ ⋅ σ ( u , p ) = f in Ω × ( 0 , T ) ∇ ⋅ u = 0 in Ω × ( 0 , T ) u = g on Γ D × ( 0 , T ) σ ( u , p ) n ^ = h on Γ N × ( 0 , T ) u ( 0 ) = u 0 in Ω × { 0 } {\displaystyle {\begin{cases}\rho {\dfrac {\partial \mathbf {u} }{\partial t}}+\rho (\mathbf {u} \cdot \nabla )\mathbf {u} -\nabla \cdot {\boldsymbol {\sigma }}(\mathbf {u} ,p)=\mathbf {f} &{\text{ in }}\Omega \times (0,T)\\\nabla \cdot \mathbf {u} =0&{\text{ in }}\Omega \times (0,T)\\\mathbf {u} =\mathbf {g} &{\text{ on }}\Gamma _{D}\times (0,T)\\{\boldsymbol {\sigma }}(\mathbf {u} ,p){\hat {\mathbf {n} }}=\mathbf {h} &{\text{ on }}\Gamma _{N}\times (0,T)\\\mathbf {u} (0)=\mathbf {u} _{0}&{\text{ in }}\Omega \times \{0\}\end{cases}}} $$ $$ u {\textstyle \mathbf {u} } $$ $$ p {\textstyle p} $$ $$ f {\textstyle \mathbf {f} } $$ $$ n ^ {\displaystyle {\hat {\mathbf {n} }}} $$ $$ Γ N {\textstyle \Gamma _{N}} $$ $$ σ ( u , p ) {\textstyle {\boldsymbol {\sigma }}(\mathbf {u} ,p)} $$ $$ σ ( u , p ) = − p I + 2 μ ε ( u ) . {\displaystyle {\boldsymbol {\sigma }}(\mathbf {u} ,p)=-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} ).} $$ $$ μ {\textstyle \mu } $$ $$ I {\textstyle \mathbf {I} } $$ $$ ε ( u ) {\textstyle {\boldsymbol {\varepsilon }}(\mathbf {u} )} $$ $$ ε ( u ) = 1 2 ( ( ∇ u ) + ( ∇ u ) T ) . {\displaystyle {\boldsymbol {\varepsilon }}(\mathbf {u} )={\frac {1}{2}}\left(\left(\nabla \mathbf {u} \right)+\left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right).} $$ $$ g {\textstyle \mathbf {g} } $$ $$ h {\textstyle \mathbf {h} } $$ $$ u 0 {\textstyle \mathbf {u} _{0}} $$ $$ ∇ ⋅ ( ∇ f ) T = ∇ ( ∇ ⋅ f ) {\displaystyle \nabla \cdot \left(\nabla \mathbf {f} \right)^{\mathrm {T} }=\nabla (\nabla \cdot \mathbf {f} )} $$ $$ ∇ ⋅ σ ( u , p ) = ∇ ⋅ ( − p I + 2 μ ε ( u ) ) = − ∇ p + 2 μ ∇ ⋅ ε ( u ) = − ∇ p + 2 μ ∇ ⋅ [ 1 2 ( ( ∇ u ) + ( ∇ u ) T ) ] = − ∇ p + μ ( Δ u + ∇ ⋅ ( ∇ u ) T ) = − ∇ p + μ ( Δ u + ∇ ( ∇ ⋅ u ) ⏟ = 0 ) = − ∇ p + μ Δ u . {\displaystyle {\begin{aligned}\nabla \cdot {\boldsymbol {\sigma }}(\mathbf {u} ,p)&=\nabla \cdot \left(-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} )\right)\\&=-\nabla p+2\mu \nabla \cdot {\boldsymbol {\varepsilon }}(\mathbf {u} )\\&=-\nabla p+2\mu \nabla \cdot \left[{\tfrac {1}{2}}\left(\left(\nabla \mathbf {u} \right)+\left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right)\right]\\&=-\nabla p+\mu \left(\Delta \mathbf {u} +\nabla \cdot \left(\nabla \mathbf {u} \right)^{\mathrm {T} }\right)\\&=-\nabla p+\mu {\bigl (}\Delta \mathbf {u} +\nabla \underbrace {(\nabla \cdot \mathbf {u} )} _{=0}{\bigr )}=-\nabla p+\mu \,\Delta \mathbf {u} .\end{aligned}}} $$ $$ σ ( u , p ) n ^ = ( − p I + 2 μ ε ( u ) ) n ^ = − p n ^ + μ ∂ u ∂ n ^ . {\displaystyle {\boldsymbol {\sigma }}(\mathbf {u} ,p){\hat {\mathbf {n} }}=\left(-p\mathbf {I} +2\mu {\boldsymbol {\varepsilon }}(\mathbf {u} )\right){\hat {\mathbf {n} }}=-p{\hat {\mathbf {n} }}+\mu {\frac {\partial {\boldsymbol {u}}}{\partial {\hat {\mathbf {n} }}}}.} $$ In order to find the weak form of the Navier–Stokes equations, firstly, consider the momentum equation ρ ∂ u ∂ t − μ Δ u + ρ ( u ⋅ ∇ ) u + ∇ p = f \rho {\frac {\partial \mathbf {u }{\partial t}}-\mu \Delta \mathbf {u} +\rho (\mathbf {u} \cdot \nabla )\mathbf {u} +\nabla p=\mathbf {f} } multiply it for a test function v {\textstyle \mathbf {v} } , defined in a suitable space V {\textstyle V} , and integrate both members with respect to the domain Ω {\textstyle \Omega } : ∫ Ω ρ ∂ u ∂ t ⋅ v − ∫ Ω μ Δ u ⋅ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v + ∫ Ω ∇ p ⋅ v = ∫ Ω f ⋅ v \int \limits _{\Omega \rho {\frac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} -\int \limits _{\Omega }\mu \Delta \mathbf {u} \cdot \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} +\int \limits _{\Omega }\nabla p\cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} } Counter-integrating by parts the diffusive and the pressure terms and by using the Gauss' theorem: − ∫ Ω μ Δ u ⋅ v = ∫ Ω μ ∇ u ⋅ ∇ v − ∫ ∂ Ω μ ∂ u ∂ n ^ ⋅ v ∫ Ω ∇ p ⋅ v = − ∫ Ω p ∇ ⋅ v + ∫ ∂ Ω p v ⋅ n ^ {\begin{aligned-\int \limits _{\Omega }\mu \Delta \mathbf {u} \cdot \mathbf {v} &=\int _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} -\int \limits _{\partial \Omega }\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}\cdot \mathbf {v} \\\int \limits _{\Omega }\nabla p\cdot \mathbf {v} &=-\int \limits _{\Omega }p\nabla \cdot \mathbf {v} +\int \limits _{\partial \Omega }p\mathbf {v} \cdot {\hat {\mathbf {n} }}\end{aligned}}} $$ ρ ∂ u ∂ t − μ Δ u + ρ ( u ⋅ ∇ ) u + ∇ p = f {\displaystyle \rho {\frac {\partial \mathbf {u} }{\partial t}}-\mu \Delta \mathbf {u} +\rho (\mathbf {u} \cdot \nabla )\mathbf {u} +\nabla p=\mathbf {f} } $$ $$ v {\textstyle \mathbf {v} } $$ $$ V {\textstyle V} $$ $$ Ω {\textstyle \Omega } $$ $$ ∫ Ω ρ ∂ u ∂ t ⋅ v − ∫ Ω μ Δ u ⋅ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v + ∫ Ω ∇ p ⋅ v = ∫ Ω f ⋅ v {\displaystyle \int \limits _{\Omega }\rho {\frac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} -\int \limits _{\Omega }\mu \Delta \mathbf {u} \cdot \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} +\int \limits _{\Omega }\nabla p\cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} } $$ $$ − ∫ Ω μ Δ u ⋅ v = ∫ Ω μ ∇ u ⋅ ∇ v − ∫ ∂ Ω μ ∂ u ∂ n ^ ⋅ v ∫ Ω ∇ p ⋅ v = − ∫ Ω p ∇ ⋅ v + ∫ ∂ Ω p v ⋅ n ^ {\displaystyle {\begin{aligned}-\int \limits _{\Omega }\mu \Delta \mathbf {u} \cdot \mathbf {v} &=\int _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} -\int \limits _{\partial \Omega }\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}\cdot \mathbf {v} \\\int \limits _{\Omega }\nabla p\cdot \mathbf {v} &=-\int \limits _{\Omega }p\nabla \cdot \mathbf {v} +\int \limits _{\partial \Omega }p\mathbf {v} \cdot {\hat {\mathbf {n} }}\end{aligned}}} $$ Using these relations, one gets: ∫ Ω ρ ∂ u ∂ t ⋅ v + ∫ Ω μ ∇ u ⋅ ∇ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v − ∫ Ω p ∇ ⋅ v = ∫ Ω f ⋅ v + ∫ ∂ Ω ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v ∀ v ∈ V . \int \limits _{\Omega \rho {\dfrac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} +\int \limits _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} -\int \limits _{\Omega }p\nabla \cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} +\int \limits _{\partial \Omega }\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} \quad \forall \mathbf {v} \in V.} In the same fashion, the continuity equation is multiplied for a test function q belonging to a space Q {\textstyle Q} and integrated in the domain Ω {\textstyle \Omega } : ∫ Ω q ∇ ⋅ u = 0. ∀ q ∈ Q . \int \limits _{\Omega q\nabla \cdot \mathbf {u} =0.\quad \forall q\in Q.} The space functions are chosen as follows: V = [ H 0 1 ( Ω ) ] d = { v ∈ [ H 1 ( Ω ) ] d : v = 0 on Γ D } , Q = L 2 ( Ω ) {\begin{alignedV=\left[H_{0}^{1}(\Omega )\right]^{d}&=\left\{\mathbf {v} \in \left[H^{1}(\Omega )\right]^{d}:\quad \mathbf {v} =\mathbf {0} {\text{ on }}\Gamma _{D}\right\},\\Q&=L^{2}(\Omega )\end{aligned}}} Considering that the test function v vanishes on the Dirichlet boundary and considering the Neumann condition, the integral on the boundary can be rearranged as: ∫ ∂ Ω ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v = ∫ Γ D ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v ⏟ v = 0 on Γ D + ∫ Γ N ∫ Γ N ( μ ∂ u ∂ n ^ − p n ^ ) ⏟ = h on Γ N ⋅ v = ∫ Γ N h ⋅ v . \int \limits _{\partial \Omega \left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} =\underbrace {\int \limits _{\Gamma _{D}}\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} } _{\mathbf {v} =\mathbf {0} {\text{ on }}\Gamma _{D}\ }+\int \limits _{\Gamma _{N}}\underbrace {{\vphantom {\int \limits _{\Gamma _{N}}}}\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)} _{=\mathbf {h} {\text{ on }}\Gamma _{N}}\cdot \mathbf {v} =\int \limits _{\Gamma _{N}}\mathbf {h} \cdot \mathbf {v} .} Having this in mind, the weak formulation of the Navier–Stokes equations is expressed as: find u ∈ L 2 ( R + [ H 1 ( Ω ) ] d ) ∩ C 0 ( R + [ L 2 ( Ω ) ] d ) such that: { ∫ Ω ρ ∂ u ∂ t ⋅ v + ∫ Ω μ ∇ u ⋅ ∇ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v − ∫ Ω p ∇ ⋅ v = ∫ Ω f ⋅ v + ∫ Γ N h ⋅ v ∀ v ∈ V , ∫ Ω q ∇ ⋅ u = 0 ∀ q ∈ Q . {\begin{aligned&{\text{find }}\mathbf {u} \in L^{2}\left(\mathbb {R} ^{+}\;\left[H^{1}(\Omega )\right]^{d}\right)\cap C^{0}\left(\mathbb {R} ^{+}\;\left[L^{2}(\Omega )\right]^{d}\right){\text{ such that: }}\\[5pt]&\quad {\begin{cases}\displaystyle \int \limits _{\Omega }\rho {\dfrac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} +\int \limits _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} -\int \limits _{\Omega }p\nabla \cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} +\int \limits _{\Gamma _{N}}\mathbf {h} \cdot \mathbf {v} \quad \forall \mathbf {v} \in V,\\\displaystyle \int \limits _{\Omega }q\nabla \cdot \mathbf {u} =0\quad \forall q\in Q.\end{cases}}\end{aligned}}} $$ ∫ Ω ρ ∂ u ∂ t ⋅ v + ∫ Ω μ ∇ u ⋅ ∇ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v − ∫ Ω p ∇ ⋅ v = ∫ Ω f ⋅ v + ∫ ∂ Ω ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v ∀ v ∈ V . {\displaystyle \int \limits _{\Omega }\rho {\dfrac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} +\int \limits _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} -\int \limits _{\Omega }p\nabla \cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} +\int \limits _{\partial \Omega }\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} \quad \forall \mathbf {v} \in V.} $$ $$ Q {\textstyle Q} $$ $$ Ω {\textstyle \Omega } $$ $$ ∫ Ω q ∇ ⋅ u = 0. ∀ q ∈ Q . {\displaystyle \int \limits _{\Omega }q\nabla \cdot \mathbf {u} =0.\quad \forall q\in Q.} $$ $$ V = [ H 0 1 ( Ω ) ] d = { v ∈ [ H 1 ( Ω ) ] d : v = 0 on Γ D } , Q = L 2 ( Ω ) {\displaystyle {\begin{aligned}V=\left[H_{0}^{1}(\Omega )\right]^{d}&=\left\{\mathbf {v} \in \left[H^{1}(\Omega )\right]^{d}:\quad \mathbf {v} =\mathbf {0} {\text{ on }}\Gamma _{D}\right\},\\Q&=L^{2}(\Omega )\end{aligned}}} $$ $$ ∫ ∂ Ω ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v = ∫ Γ D ( μ ∂ u ∂ n ^ − p n ^ ) ⋅ v ⏟ v = 0 on Γ D + ∫ Γ N ∫ Γ N ( μ ∂ u ∂ n ^ − p n ^ ) ⏟ = h on Γ N ⋅ v = ∫ Γ N h ⋅ v . {\displaystyle \int \limits _{\partial \Omega }\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} =\underbrace {\int \limits _{\Gamma _{D}}\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)\cdot \mathbf {v} } _{\mathbf {v} =\mathbf {0} {\text{ on }}\Gamma _{D}\ }+\int \limits _{\Gamma _{N}}\underbrace {{\vphantom {\int \limits _{\Gamma _{N}}}}\left(\mu {\frac {\partial \mathbf {u} }{\partial {\hat {\mathbf {n} }}}}-p{\hat {\mathbf {n} }}\right)} _{=\mathbf {h} {\text{ on }}\Gamma _{N}}\cdot \mathbf {v} =\int \limits _{\Gamma _{N}}\mathbf {h} \cdot \mathbf {v} .} $$ $$ find u ∈ L 2 ( R + [ H 1 ( Ω ) ] d ) ∩ C 0 ( R + [ L 2 ( Ω ) ] d ) such that: { ∫ Ω ρ ∂ u ∂ t ⋅ v + ∫ Ω μ ∇ u ⋅ ∇ v + ∫ Ω ρ ( u ⋅ ∇ ) u ⋅ v − ∫ Ω p ∇ ⋅ v = ∫ Ω f ⋅ v + ∫ Γ N h ⋅ v ∀ v ∈ V , ∫ Ω q ∇ ⋅ u = 0 ∀ q ∈ Q . {\displaystyle {\begin{aligned}&{\text{find }}\mathbf {u} \in L^{2}\left(\mathbb {R} ^{+}\;\left[H^{1}(\Omega )\right]^{d}\right)\cap C^{0}\left(\mathbb {R} ^{+}\;\left[L^{2}(\Omega )\right]^{d}\right){\text{ such that: }}\\[5pt]&\quad {\begin{cases}\displaystyle \int \limits _{\Omega }\rho {\dfrac {\partial \mathbf {u} }{\partial t}}\cdot \mathbf {v} +\int \limits _{\Omega }\mu \nabla \mathbf {u} \cdot \nabla \mathbf {v} +\int \limits _{\Omega }\rho (\mathbf {u} \cdot \nabla )\mathbf {u} \cdot \mathbf {v} -\int \limits _{\Omega }p\nabla \cdot \mathbf {v} =\int \limits _{\Omega }\mathbf {f} \cdot \mathbf {v} +\int \limits _{\Gamma _{N}}\mathbf {h} \cdot \mathbf {v} \quad \forall \mathbf {v} \in V,\\\displaystyle \int \limits _{\Omega }q\nabla \cdot \mathbf {u} =0\quad \forall q\in Q.\end{cases}}\end{aligned}}} $$ With partitioning of the problem domain and defining basis functions on the partitioned domain, the discrete form of the governing equation is ( w i , ∂ u j ∂ t ) = − ( w i , ( u ⋅ ∇ ) u j ) − ν ( ∇ w i : ∇ u j ) + ( w i , f S ) . \left(\mathbf {w _{i},{\frac {\partial \mathbf {u} _{j}}{\partial t}}\right)=-{\bigl (}\mathbf {w} _{i},\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} _{j}{\bigr )}-\nu \left(\nabla \mathbf {w} _{i}:\nabla \mathbf {u} _{j}\right)+\left(\mathbf {w} _{i},\mathbf {f} ^{S}\right).} $$ ( w i , ∂ u j ∂ t ) = − ( w i , ( u ⋅ ∇ ) u j ) − ν ( ∇ w i : ∇ u j ) + ( w i , f S ) . {\displaystyle \left(\mathbf {w} _{i},{\frac {\partial \mathbf {u} _{j}}{\partial t}}\right)=-{\bigl (}\mathbf {w} _{i},\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} _{j}{\bigr )}-\nu \left(\nabla \mathbf {w} _{i}:\nabla \mathbf {u} _{j}\right)+\left(\mathbf {w} _{i},\mathbf {f} ^{S}\right).} $$ It is desirable to choose basis functions that reflect the essential feature of incompressible flow – the elements must be divergence-free. While the velocity is the variable of interest, the existence of the stream function or vector potential is necessary by the Helmholtz theorem. Further, to determine fluid flow in the absence of a pressure gradient, one can specify the difference of stream function values across a 2D channel, or the line integral of the tangential component of the vector potential around the channel in 3D, the flow being given by Stokes' theorem. Discussion will be restricted to 2D in the following. We further restrict discussion to continuous Hermite finite elements which have at least first-derivative degrees-of-freedom. With this, one can draw a large number of candidate triangular and rectangular elements from the plate-bending literature. These elements have derivatives as components of the gradient. In 2D, the gradient and curl of a scalar are clearly orthogonal, given by the expressions, ∇ φ = ( ∂ φ ∂ x , ∂ φ ∂ y ) T , ∇ × φ = ( ∂ φ ∂ y , − ∂ φ ∂ x ) T . {\begin{aligned\nabla \varphi &=\left({\frac {\partial \varphi }{\partial x}},\,{\frac {\partial \varphi }{\partial y}}\right)^{\mathrm {T} },\\[5pt]\nabla \times \varphi &=\left({\frac {\partial \varphi }{\partial y}},\,-{\frac {\partial \varphi }{\partial x}}\right)^{\mathrm {T} }.\end{aligned}}} $$ ∇ φ = ( ∂ φ ∂ x , ∂ φ ∂ y ) T , ∇ × φ = ( ∂ φ ∂ y , − ∂ φ ∂ x ) T . {\displaystyle {\begin{aligned}\nabla \varphi &=\left({\frac {\partial \varphi }{\partial x}},\,{\frac {\partial \varphi }{\partial y}}\right)^{\mathrm {T} },\\[5pt]\nabla \times \varphi &=\left({\frac {\partial \varphi }{\partial y}},\,-{\frac {\partial \varphi }{\partial x}}\right)^{\mathrm {T} }.\end{aligned}}} $$ Adopting continuous plate-bending elements, interchanging the derivative degrees-of-freedom and changing the sign of the appropriate one gives many families of stream function elements. Taking the curl of the scalar stream function elements gives divergence-free velocity elements. The requirement that the stream function elements be continuous assures that the normal component of the velocity is continuous across element interfaces, all that is necessary for vanishing divergence on these interfaces. Boundary conditions are simple to apply. The stream function is constant on no-flow surfaces, with no-slip velocity conditions on surfaces. Stream function differences across open channels determine the flow. No boundary conditions are necessary on open boundaries, though consistent values may be used with some problems. These are all Dirichlet conditions. The algebraic equations to be solved are simple to set up, but of course are non-linear, requiring iteration of the linearized equations. Similar considerations apply to three-dimensions, but extension from 2D is not immediate because of the vector nature of the potential, and there exists no simple relation between the gradient and the curl as was the case in 2D. Recovering pressure from the velocity field is easy. The discrete weak equation for the pressure gradient is, ( g i , ∇ p ) = − ( g i , ( u ⋅ ∇ ) u j ) − ν ( ∇ g i : ∇ u j ) + ( g i , f I ) (\mathbf {g _{i},\nabla p)=-\left(\mathbf {g} _{i},\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} _{j}\right)-\nu \left(\nabla \mathbf {g} _{i}:\nabla \mathbf {u} _{j}\right)+\left(\mathbf {g} _{i},\mathbf {f} ^{I}\right)} $$ ( g i , ∇ p ) = − ( g i , ( u ⋅ ∇ ) u j ) − ν ( ∇ g i : ∇ u j ) + ( g i , f I ) {\displaystyle (\mathbf {g} _{i},\nabla p)=-\left(\mathbf {g} _{i},\left(\mathbf {u} \cdot \nabla \right)\mathbf {u} _{j}\right)-\nu \left(\nabla \mathbf {g} _{i}:\nabla \mathbf {u} _{j}\right)+\left(\mathbf {g} _{i},\mathbf {f} ^{I}\right)} $$ where the test/weight functions are irrotational. Any conforming scalar finite element may be used. However, the pressure gradient field may also be of interest. In this case, one can use scalar Hermite elements for the pressure. For the test/weight functions g i {\textstyle \mathbf {g} _{i}} one would choose the irrotational vector elements obtained from the gradient of the pressure element. $$ g i {\textstyle \mathbf {g} _{i}} $$ The rotating frame of reference introduces some interesting pseudo-forces into the equations through the material derivative term. Consider a stationary inertial frame of reference K {\textstyle K} , and a non-inertial frame of reference K ′ {\textstyle K'} , which is translating with velocity U ( t ) {\textstyle \mathbf {U} (t)} and rotating with angular velocity Ω ( t ) {\textstyle \Omega (t)} with respect to the stationary frame. The Navier–Stokes equation observed from the non-inertial frame then becomes $$ K {\textstyle K} $$ $$ K ′ {\textstyle K'} $$ $$ U ( t ) {\textstyle \mathbf {U} (t)} $$ $$ Ω ( t ) {\textstyle \Omega (t)} $$ ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ∇ [ ζ ( ∇ ⋅ u ) ] + ρ f − ρ [ 2 Ω × u + Ω × ( Ω × x ) + d U d t + d Ω d t × x ] . \rho \left({\frac {\partial \mathbf {u }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\nabla [\zeta (\nabla \cdot \mathbf {u} )]+\rho \mathbf {f} -\rho \left[2\mathbf {\Omega } \times \mathbf {u} +\mathbf {\Omega } \times (\mathbf {\Omega } \times \mathbf {x} )+{\frac {\mathrm {d} \mathbf {U} }{\mathrm {d} t}}+{\frac {\mathrm {d} \mathbf {\Omega } }{\mathrm {d} t}}\times \mathbf {x} \right].} $$ ρ ( ∂ u ∂ t + ( u ⋅ ∇ ) u ) = − ∇ p + ∇ ⋅ { μ [ ∇ u + ( ∇ u ) T − 2 3 ( ∇ ⋅ u ) I ] } + ∇ [ ζ ( ∇ ⋅ u ) ] + ρ f − ρ [ 2 Ω × u + Ω × ( Ω × x ) + d U d t + d Ω d t × x ] . {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+(\mathbf {u} \cdot \nabla )\mathbf {u} \right)=-\nabla p+\nabla \cdot \left\{\mu \left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{\mathrm {T} }-{\tfrac {2}{3}}(\nabla \cdot \mathbf {u} )\mathbf {I} \right]\right\}+\nabla [\zeta (\nabla \cdot \mathbf {u} )]+\rho \mathbf {f} -\rho \left[2\mathbf {\Omega } \times \mathbf {u} +\mathbf {\Omega } \times (\mathbf {\Omega } \times \mathbf {x} )+{\frac {\mathrm {d} \mathbf {U} }{\mathrm {d} t}}+{\frac {\mathrm {d} \mathbf {\Omega } }{\mathrm {d} t}}\times \mathbf {x} \right].} $$ Here x {\textstyle \mathbf {x} } and u {\textstyle \mathbf {u} } are measured in the non-inertial frame. The first term in the parenthesis represents Coriolis acceleration, the second term is due to centrifugal acceleration, the third is due to the linear acceleration of K ′ {\textstyle K'} with respect to K {\textstyle K} and the fourth term is due to the angular acceleration of K ′ {\textstyle K'} with respect to K {\textstyle K} . $$ x {\textstyle \mathbf {x} } $$ $$ u {\textstyle \mathbf {u} } $$ $$ K ′ {\textstyle K'} $$ $$ K {\textstyle K} $$ $$ K ′ {\textstyle K'} $$ $$ K {\textstyle K} $$ The Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state. Regardless of the flow assumptions, a statement of the conservation of mass is generally necessary. This is achieved through the mass continuity equation, as discussed above in the "General continuum equations" within this article, as follows: D m D t = ∭ V ( D ρ D t + ρ ( ∇ ⋅ u ) ) d V D ρ D t + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ( ∇ ρ ) ⋅ u + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\begin{aligned{\frac {\mathbf {D} m}{\mathbf {Dt} }}&={\iiint \limits _{V}}({{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot \mathbf {u} )})dV\\{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot {\mathbf {u} })&={\frac {\partial \rho }{\partial t}}+({\nabla \rho })\cdot {\mathbf {u} }+{\rho }(\nabla \cdot \mathbf {u} )={\frac {\partial \rho }{\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0\end{aligned}}} A fluid media for which the density ( ρ \rho ) is constant is called incompressible. Therefore, the rate of change of density ( ρ \rho ) with respect to time ( ∂ ρ ∂ t ) ({\frac {\partial \rho {\partial t}})} and the gradient of density ( ∇ ρ ) (\nabla \rho ) are equal to zero ( 0 ) (0) . In this case the general equation of continuity, ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\frac {\partial \rho {\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0} , reduces to: ρ ( ∇ ⋅ u ) = 0 \rho (\nabla {\cdot {\mathbf {u} })=0} . Furthermore, assuming that density ( ρ \rho ) is a non-zero constant ( ρ ≠ 0 ) (\rho \neq 0) means that the right-hand side of the equation ( 0 ) (0) is divisible by density ( ρ \rho ). Therefore, the continuity equation for an incompressible fluid reduces further to: ( ∇ ⋅ u ) = 0 (\nabla {\cdot {\mathbf {u }})=0} This relationship, ( ∇ ⋅ u ) = 0 {\textstyle (\nabla {\cdot {\mathbf {u} }})=0} , identifies that the divergence of the flow velocity vector ( u \mathbf {u } ) is equal to zero ( 0 ) (0) , which means that for an incompressible fluid the flow velocity field is a solenoidal vector field or a divergence-free vector field. Note that this relationship can be expanded upon due to its uniqueness with the vector Laplace operator ( ∇ 2 u = ∇ ( ∇ ⋅ u ) − ∇ × ( ∇ × u ) ) (\nabla ^{2\mathbf {u} =\nabla (\nabla \cdot \mathbf {u} )-\nabla \times (\nabla \times \mathbf {u} ))} , and vorticity ( ω → = ∇ × u ) ({\vec {\omega }=\nabla \times \mathbf {u} )} which is now expressed like so, for an incompressible fluid: ∇ 2 u = − ( ∇ × ( ∇ × u ) ) = − ( ∇ × ω → ) \nabla ^{2\mathbf {u} =-(\nabla \times (\nabla \times \mathbf {u} ))=-(\nabla \times {\vec {\omega }})} $$ D m D t = ∭ V ( D ρ D t + ρ ( ∇ ⋅ u ) ) d V D ρ D t + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ( ∇ ρ ) ⋅ u + ρ ( ∇ ⋅ u ) = ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\displaystyle {\begin{aligned}{\frac {\mathbf {D} m}{\mathbf {Dt} }}&={\iiint \limits _{V}}({{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot \mathbf {u} )})dV\\{\frac {\mathbf {D} \rho }{\mathbf {Dt} }}+\rho (\nabla \cdot {\mathbf {u} })&={\frac {\partial \rho }{\partial t}}+({\nabla \rho })\cdot {\mathbf {u} }+{\rho }(\nabla \cdot \mathbf {u} )={\frac {\partial \rho }{\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0\end{aligned}}} $$ $$ ρ {\displaystyle \rho } $$ $$ ρ {\displaystyle \rho } $$ $$ ( ∂ ρ ∂ t ) {\displaystyle ({\frac {\partial \rho }{\partial t}})} $$ $$ ( ∇ ρ ) {\displaystyle (\nabla \rho )} $$ $$ ( 0 ) {\displaystyle (0)} $$ $$ ∂ ρ ∂ t + ∇ ⋅ ( ρ u ) = 0 {\displaystyle {\frac {\partial \rho }{\partial t}}+\nabla \cdot ({\rho \mathbf {u} })=0} $$ $$ ρ ( ∇ ⋅ u ) = 0 {\displaystyle \rho (\nabla {\cdot }{\mathbf {u} })=0} $$ $$ ρ {\displaystyle \rho } $$ $$ ( ρ ≠ 0 ) {\displaystyle (\rho \neq 0)} $$ $$ ( 0 ) {\displaystyle (0)} $$ $$ ρ {\displaystyle \rho } $$ $$ ( ∇ ⋅ u ) = 0 {\displaystyle (\nabla {\cdot {\mathbf {u} }})=0} $$ $$ ( ∇ ⋅ u ) = 0 {\textstyle (\nabla {\cdot {\mathbf {u} }})=0} $$ $$ u {\displaystyle \mathbf {u} } $$ $$ ( 0 ) {\displaystyle (0)} $$ $$ ( ∇ 2 u = ∇ ( ∇ ⋅ u ) − ∇ × ( ∇ × u ) ) {\displaystyle (\nabla ^{2}\mathbf {u} =\nabla (\nabla \cdot \mathbf {u} )-\nabla \times (\nabla \times \mathbf {u} ))} $$ $$ ( ω → = ∇ × u ) {\displaystyle ({\vec {\omega }}=\nabla \times \mathbf {u} )} $$ $$ ∇ 2 u = − ( ∇ × ( ∇ × u ) ) = − ( ∇ × ω → ) {\displaystyle \nabla ^{2}\mathbf {u} =-(\nabla \times (\nabla \times \mathbf {u} ))=-(\nabla \times {\vec {\omega }})} $$ Taking the curl of the incompressible Navier–Stokes equation results in the elimination of pressure. This is especially easy to see if 2D Cartesian flow is assumed (like in the degenerate 3D case with u z = 0 {\textstyle u_{z}=0} and no dependence of anything on z {\textstyle z} ), where the equations reduce to: ρ ( ∂ u x ∂ t + u x ∂ u x ∂ x + u y ∂ u x ∂ y ) = − ∂ p ∂ x + μ ( ∂ 2 u x ∂ x 2 + ∂ 2 u x ∂ y 2 ) + ρ g x ρ ( ∂ u y ∂ t + u x ∂ u y ∂ x + u y ∂ u y ∂ y ) = − ∂ p ∂ y + μ ( ∂ 2 u y ∂ x 2 + ∂ 2 u y ∂ y 2 ) + ρ g y . {\begin{aligned\rho \left({\frac {\partial u_{x}}{\partial t}}+u_{x}{\frac {\partial u_{x}}{\partial x}}+u_{y}{\frac {\partial u_{x}}{\partial y}}\right)&=-{\frac {\partial p}{\partial x}}+\mu \left({\frac {\partial ^{2}u_{x}}{\partial x^{2}}}+{\frac {\partial ^{2}u_{x}}{\partial y^{2}}}\right)+\rho g_{x}\\\rho \left({\frac {\partial u_{y}}{\partial t}}+u_{x}{\frac {\partial u_{y}}{\partial x}}+u_{y}{\frac {\partial u_{y}}{\partial y}}\right)&=-{\frac {\partial p}{\partial y}}+\mu \left({\frac {\partial ^{2}u_{y}}{\partial x^{2}}}+{\frac {\partial ^{2}u_{y}}{\partial y^{2}}}\right)+\rho g_{y}.\end{aligned}}} $$ u z = 0 {\textstyle u_{z}=0} $$ $$ z {\textstyle z} $$ $$ ρ ( ∂ u x ∂ t + u x ∂ u x ∂ x + u y ∂ u x ∂ y ) = − ∂ p ∂ x + μ ( ∂ 2 u x ∂ x 2 + ∂ 2 u x ∂ y 2 ) + ρ g x ρ ( ∂ u y ∂ t + u x ∂ u y ∂ x + u y ∂ u y ∂ y ) = − ∂ p ∂ y + μ ( ∂ 2 u y ∂ x 2 + ∂ 2 u y ∂ y 2 ) + ρ g y . {\displaystyle {\begin{aligned}\rho \left({\frac {\partial u_{x}}{\partial t}}+u_{x}{\frac {\partial u_{x}}{\partial x}}+u_{y}{\frac {\partial u_{x}}{\partial y}}\right)&=-{\frac {\partial p}{\partial x}}+\mu \left({\frac {\partial ^{2}u_{x}}{\partial x^{2}}}+{\frac {\partial ^{2}u_{x}}{\partial y^{2}}}\right)+\rho g_{x}\\\rho \left({\frac {\partial u_{y}}{\partial t}}+u_{x}{\frac {\partial u_{y}}{\partial x}}+u_{y}{\frac {\partial u_{y}}{\partial y}}\right)&=-{\frac {\partial p}{\partial y}}+\mu \left({\frac {\partial ^{2}u_{y}}{\partial x^{2}}}+{\frac {\partial ^{2}u_{y}}{\partial y^{2}}}\right)+\rho g_{y}.\end{aligned}}} $$ Differentiating the first with respect to y {\textstyle y} , the second with respect to x {\textstyle x} and subtracting the resulting equations will eliminate pressure and any conservative force. For incompressible flow, defining the stream function ψ {\textstyle \psi } through u x = ∂ ψ ∂ y ; u y = − ∂ ψ ∂ x u_{x={\frac {\partial \psi }{\partial y}};\quad u_{y}=-{\frac {\partial \psi }{\partial x}}} results in mass continuity being unconditionally satisfied (given the stream function is continuous), and then incompressible Newtonian 2D momentum and mass conservation condense into one equation: ∂ ∂ t ( ∇ 2 ψ ) + ∂ ψ ∂ y ∂ ∂ x ( ∇ 2 ψ ) − ∂ ψ ∂ x ∂ ∂ y ( ∇ 2 ψ ) = ν ∇ 4 ψ {\frac {\partial {\partial t}}\left(\nabla ^{2}\psi \right)+{\frac {\partial \psi }{\partial y}}{\frac {\partial }{\partial x}}\left(\nabla ^{2}\psi \right)-{\frac {\partial \psi }{\partial x}}{\frac {\partial }{\partial y}}\left(\nabla ^{2}\psi \right)=\nu \nabla ^{4}\psi } $$ y {\textstyle y} $$ $$ x {\textstyle x} $$ $$ ψ {\textstyle \psi } $$ $$ u x = ∂ ψ ∂ y ; u y = − ∂ ψ ∂ x {\displaystyle u_{x}={\frac {\partial \psi }{\partial y}};\quad u_{y}=-{\frac {\partial \psi }{\partial x}}} $$ $$ ∂ ∂ t ( ∇ 2 ψ ) + ∂ ψ ∂ y ∂ ∂ x ( ∇ 2 ψ ) − ∂ ψ ∂ x ∂ ∂ y ( ∇ 2 ψ ) = ν ∇ 4 ψ {\displaystyle {\frac {\partial }{\partial t}}\left(\nabla ^{2}\psi \right)+{\frac {\partial \psi }{\partial y}}{\frac {\partial }{\partial x}}\left(\nabla ^{2}\psi \right)-{\frac {\partial \psi }{\partial x}}{\frac {\partial }{\partial y}}\left(\nabla ^{2}\psi \right)=\nu \nabla ^{4}\psi } $$ where ∇ 4 {\textstyle \nabla ^{4}} is the 2D biharmonic operator and ν {\textstyle \nu } is the kinematic viscosity, ν = μ ρ {\textstyle \nu ={\frac {\mu }{\rho }}} . We can also express this compactly using the Jacobian determinant: ∂ ∂ t ( ∇ 2 ψ ) + ∂ ( ψ , ∇ 2 ψ ) ∂ ( y , x ) = ν ∇ 4 ψ . {\frac {\partial {\partial t}}\left(\nabla ^{2}\psi \right)+{\frac {\partial \left(\psi ,\nabla ^{2}\psi \right)}{\partial (y,x)}}=\nu \nabla ^{4}\psi .} $$ ∇ 4 {\textstyle \nabla ^{4}} $$ $$ ν {\textstyle \nu } $$ $$ ν = μ ρ {\textstyle \nu ={\frac {\mu }{\rho }}} $$ $$ ∂ ∂ t ( ∇ 2 ψ ) + ∂ ( ψ , ∇ 2 ψ ) ∂ ( y , x ) = ν ∇ 4 ψ . {\displaystyle {\frac {\partial }{\partial t}}\left(\nabla ^{2}\psi \right)+{\frac {\partial \left(\psi ,\nabla ^{2}\psi \right)}{\partial (y,x)}}=\nu \nabla ^{4}\psi .} $$ This single equation together with appropriate boundary conditions describes 2D fluid flow, taking only kinematic viscosity as a parameter. Note that the equation for creeping flow results when the left side is assumed zero. In axisymmetric flow another stream function formulation, called the Stokes stream function, can be used to describe the velocity components of an incompressible flow with one scalar function. The incompressible Navier–Stokes equation is a differential algebraic equation, having the inconvenient feature that there is no explicit mechanism for advancing the pressure in time. Consequently, much effort has been expended to eliminate the pressure from all or part of the computational process. The stream function formulation eliminates the pressure but only in two dimensions and at the expense of introducing higher derivatives and elimination of the velocity, which is the primary variable of interest. The Navier–Stokes equations are nonlinear partial differential equations in the general case and so remain in almost every real situation. In some cases, such as one-dimensional flow and Stokes flow (or creeping flow), the equations can be simplified to linear equations. The nonlinearity makes most problems difficult or impossible to solve and is the main contributor to the turbulence that the equations model. The nonlinearity is due to convective acceleration, which is an acceleration associated with the change in velocity over position. Hence, any convective flow, whether turbulent or not, will involve nonlinearity. An example of convective but laminar (nonturbulent) flow would be the passage of a viscous fluid (for example, oil) through a small converging nozzle. Such flows, whether exactly solvable or not, can often be thoroughly studied and understood. Turbulence is the time-dependent chaotic behaviour seen in many fluid flows. It is generally believed that it is due to the inertia of the fluid as a whole: the culmination of time-dependent and convective acceleration; hence flows where inertial effects are small tend to be laminar (the Reynolds number quantifies how much the flow is affected by inertia). It is believed, though not known with certainty, that the Navier–Stokes equations describe turbulence properly. The numerical solution of the Navier–Stokes equations for turbulent flow is extremely difficult, and due to the significantly different mixing-length scales that are involved in turbulent flow, the stable solution of this requires such a fine mesh resolution that the computational time becomes significantly infeasible for calculation or direct numerical simulation. Attempts to solve turbulent flow using a laminar solver typically result in a time-unsteady solution, which fails to converge appropriately. To counter this, time-averaged equations such as the Reynolds-averaged Navier–Stokes equations (RANS), supplemented with turbulence models, are used in practical computational fluid dynamics (CFD) applications when modeling turbulent flows. Some models include the Spalart–Allmaras, k–ω, k–ε, and SST models, which add a variety of additional equations to bring closure to the RANS equations. Large eddy simulation (LES) can also be used to solve these equations numerically. This approach is computationally more expensive—in time and in computer memory—than RANS, but produces better results because it explicitly resolves the larger turbulent scales. Together with supplemental equations (for example, conservation of mass) and well-formulated boundary conditions, the Navier–Stokes equations seem to model fluid motion accurately; even turbulent flows seem (on average) to agree with real world observations. The Navier–Stokes equations assume that the fluid being studied is a continuum (it is infinitely divisible and not composed of particles such as atoms or molecules), and is not moving at relativistic velocities. At very small scales or under extreme conditions, real fluids made out of discrete molecules will produce results different from the continuous fluids modeled by the Navier–Stokes equations. For example, capillarity of internal layers in fluids appears for flow with high gradients. For large Knudsen number of the problem, the Boltzmann equation may be a suitable replacement. Failing that, one may have to resort to molecular dynamics or various hybrid methods. Another limitation is simply the complicated nature of the equations. Time-tested formulations exist for common fluid families, but the application of the Navier–Stokes equations to less common families tends to result in very complicated formulations and often to open research problems. For this reason, these equations are usually written for Newtonian fluids where the viscosity model is linear; truly general models for the flow of other kinds of fluids (such as blood) do not exist. The Navier–Stokes equations, even when written explicitly for specific fluids, are rather generic in nature and their proper application to specific problems can be very diverse. This is partly because there is an enormous variety of problems that may be modeled, ranging from as simple as the distribution of static pressure to as complicated as multiphase flow driven by surface tension. Generally, application to specific problems begins with some flow assumptions and initial/boundary condition formulation, this may be followed by scale analysis to further simplify the problem. Assume steady, parallel, one-dimensional, non-convective pressure-driven flow between parallel plates, the resulting scaled (dimensionless) boundary value problem is: d 2 u d y 2 = − 1 ; u ( 0 ) = u ( 1 ) = 0. {\frac {\mathrm {d ^{2}u}{\mathrm {d} y^{2}}}=-1;\quad u(0)=u(1)=0.} $$ d 2 u d y 2 = − 1 ; u ( 0 ) = u ( 1 ) = 0. {\displaystyle {\frac {\mathrm {d} ^{2}u}{\mathrm {d} y^{2}}}=-1;\quad u(0)=u(1)=0.} $$ The boundary condition is the no slip condition. This problem is easily solved for the flow field: u ( y ) = y − y 2 2 . u(y)={\frac {y-y^{2}{2}}.} $$ u ( y ) = y − y 2 2 . {\displaystyle u(y)={\frac {y-y^{2}}{2}}.} $$ From this point onward, more quantities of interest can be easily obtained, such as viscous drag force or net flow rate. Difficulties may arise when the problem becomes slightly more complicated. A seemingly modest twist on the parallel flow above would be the radial flow between parallel plates; this involves convection and thus non-linearity. The velocity field may be represented by a function f(z) that must satisfy: d 2 f d z 2 + R f 2 = − 1 ; f ( − 1 ) = f ( 1 ) = 0. {\frac {\mathrm {d ^{2}f}{\mathrm {d} z^{2}}}+Rf^{2}=-1;\quad f(-1)=f(1)=0.} $$ d 2 f d z 2 + R f 2 = − 1 ; f ( − 1 ) = f ( 1 ) = 0. {\displaystyle {\frac {\mathrm {d} ^{2}f}{\mathrm {d} z^{2}}}+Rf^{2}=-1;\quad f(-1)=f(1)=0.} $$ This ordinary differential equation is what is obtained when the Navier–Stokes equations are written and the flow assumptions applied (additionally, the pressure gradient is solved for). The nonlinear term makes this a very difficult problem to solve analytically (a lengthy implicit solution may be found which involves elliptic integrals and roots of cubic polynomials). Issues with the actual existence of solutions arise for R > 1.41 {\textstyle R>1.41} (approximately; this is not √2), the parameter R {\textstyle R} being the Reynolds number with appropriately chosen scales. This is an example of flow assumptions losing their applicability, and an example of the difficulty in "high" Reynolds number flows. $$ R > 1.41 {\textstyle R>1.41} $$ $$ R {\textstyle R} $$ A type of natural convection that can be described by the Navier–Stokes equation is the Rayleigh–Bénard convection. It is one of the most commonly studied convection phenomena because of its analytical and experimental accessibility. Some exact solutions to the Navier–Stokes equations exist. Examples of degenerate cases—with the non-linear terms in the Navier–Stokes equations equal to zero—are Poiseuille flow, Couette flow and the oscillatory Stokes boundary layer. But also, more interesting examples, solutions to the full non-linear equations, exist, such as Jeffery–Hamel flow, Von Kármán swirling flow, stagnation point flow, Landau–Squire jet, and Taylor–Green vortex. Note that the existence of these exact solutions does not imply they are stable: turbulence may develop at higher Reynolds numbers. Under additional assumptions, the component parts can be separated. For example, in the case of an unbounded planar domain with two-dimensional — incompressible and stationary — flow in polar coordinates (r,φ), the velocity components (ur,uφ) and pressure p are: u r = A r , u φ = B ( 1 r − r A ν + 1 ) , p = − A 2 + B 2 2 r 2 − 2 B 2 ν r A ν A + B 2 r ( 2 A ν + 2 ) 2 A ν + 2 {\begin{alignedu_{r}&={\frac {A}{r}},\\u_{\varphi }&=B\left({\frac {1}{r}}-r^{{\frac {A}{\nu }}+1}\right),\\p&=-{\frac {A^{2}+B^{2}}{2r^{2}}}-{\frac {2B^{2}\nu r^{\frac {A}{\nu }}}{A}}+{\frac {B^{2}r^{\left({\frac {2A}{\nu }}+2\right)}}{{\frac {2A}{\nu }}+2}}\end{aligned}}} $$ u r = A r , u φ = B ( 1 r − r A ν + 1 ) , p = − A 2 + B 2 2 r 2 − 2 B 2 ν r A ν A + B 2 r ( 2 A ν + 2 ) 2 A ν + 2 {\displaystyle {\begin{aligned}u_{r}&={\frac {A}{r}},\\u_{\varphi }&=B\left({\frac {1}{r}}-r^{{\frac {A}{\nu }}+1}\right),\\p&=-{\frac {A^{2}+B^{2}}{2r^{2}}}-{\frac {2B^{2}\nu r^{\frac {A}{\nu }}}{A}}+{\frac {B^{2}r^{\left({\frac {2A}{\nu }}+2\right)}}{{\frac {2A}{\nu }}+2}}\end{aligned}}} $$ where A and B are arbitrary constants. This solution is valid in the domain r ≥ 1 and for A < −2ν. In Cartesian coordinates, when the viscosity is zero (ν = 0), this is: v ( x , y ) = 1 x 2 + y 2 ( A x + B y A y − B x ) , p ( x , y ) = − A 2 + B 2 2 ( x 2 + y 2 ) {\begin{aligned\mathbf {v} (x,y)&={\frac {1}{x^{2}+y^{2}}}{\begin{pmatrix}Ax+By\\Ay-Bx\end{pmatrix}},\\p(x,y)&=-{\frac {A^{2}+B^{2}}{2\left(x^{2}+y^{2}\right)}}\end{aligned}}} $$ v ( x , y ) = 1 x 2 + y 2 ( A x + B y A y − B x ) , p ( x , y ) = − A 2 + B 2 2 ( x 2 + y 2 ) {\displaystyle {\begin{aligned}\mathbf {v} (x,y)&={\frac {1}{x^{2}+y^{2}}}{\begin{pmatrix}Ax+By\\Ay-Bx\end{pmatrix}},\\p(x,y)&=-{\frac {A^{2}+B^{2}}{2\left(x^{2}+y^{2}\right)}}\end{aligned}}} $$ For example, in the case of an unbounded Euclidean domain with three-dimensional — incompressible, stationary and with zero viscosity (ν = 0) — radial flow in Cartesian coordinates (x,y,z), the velocity vector v and pressure p are:[citation needed] v ( x , y , z ) = A x 2 + y 2 + z 2 ( x y z ) , p ( x , y , z ) = − A 2 2 ( x 2 + y 2 + z 2 ) . {\begin{aligned\mathbf {v} (x,y,z)&={\frac {A}{x^{2}+y^{2}+z^{2}}}{\begin{pmatrix}x\\y\\z\end{pmatrix}},\\p(x,y,z)&=-{\frac {A^{2}}{2\left(x^{2}+y^{2}+z^{2}\right)}}.\end{aligned}}} $$ v ( x , y , z ) = A x 2 + y 2 + z 2 ( x y z ) , p ( x , y , z ) = − A 2 2 ( x 2 + y 2 + z 2 ) . {\displaystyle {\begin{aligned}\mathbf {v} (x,y,z)&={\frac {A}{x^{2}+y^{2}+z^{2}}}{\begin{pmatrix}x\\y\\z\end{pmatrix}},\\p(x,y,z)&=-{\frac {A^{2}}{2\left(x^{2}+y^{2}+z^{2}\right)}}.\end{aligned}}} $$ There is a singularity at x = y = z = 0. A steady-state example with no singularities comes from considering the flow along the lines of a Hopf fibration. Let r {\textstyle r} be a constant radius of the inner coil. One set of solutions is given by: ρ ( x , y , z ) = 3 B r 2 + x 2 + y 2 + z 2 p ( x , y , z ) = − A 2 B ( r 2 + x 2 + y 2 + z 2 ) 3 u ( x , y , z ) = A ( r 2 + x 2 + y 2 + z 2 ) 2 ( 2 ( − r y + x z ) 2 ( r x + y z ) r 2 − x 2 − y 2 + z 2 ) g = 0 μ = 0 {\begin{aligned\rho (x,y,z)&={\frac {3B}{r^{2}+x^{2}+y^{2}+z^{2}}}\\p(x,y,z)&={\frac {-A^{2}B}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{3}}}\\\mathbf {u} (x,y,z)&={\frac {A}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{2}}}{\begin{pmatrix}2(-ry+xz)\\2(rx+yz)\\r^{2}-x^{2}-y^{2}+z^{2}\end{pmatrix}}\\g&=0\\\mu &=0\end{aligned}}} $$ r {\textstyle r} $$ $$ ρ ( x , y , z ) = 3 B r 2 + x 2 + y 2 + z 2 p ( x , y , z ) = − A 2 B ( r 2 + x 2 + y 2 + z 2 ) 3 u ( x , y , z ) = A ( r 2 + x 2 + y 2 + z 2 ) 2 ( 2 ( − r y + x z ) 2 ( r x + y z ) r 2 − x 2 − y 2 + z 2 ) g = 0 μ = 0 {\displaystyle {\begin{aligned}\rho (x,y,z)&={\frac {3B}{r^{2}+x^{2}+y^{2}+z^{2}}}\\p(x,y,z)&={\frac {-A^{2}B}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{3}}}\\\mathbf {u} (x,y,z)&={\frac {A}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{2}}}{\begin{pmatrix}2(-ry+xz)\\2(rx+yz)\\r^{2}-x^{2}-y^{2}+z^{2}\end{pmatrix}}\\g&=0\\\mu &=0\end{aligned}}} $$ for arbitrary constants A {\textstyle A} and B {\textstyle B} . This is a solution in a non-viscous gas (compressible fluid) whose density, velocities and pressure goes to zero far from the origin. (Note this is not a solution to the Clay Millennium problem because that refers to incompressible fluids where ρ {\textstyle \rho } is a constant, and neither does it deal with the uniqueness of the Navier–Stokes equations with respect to any turbulence properties.) It is also worth pointing out that the components of the velocity vector are exactly those from the Pythagorean quadruple parametrization. Other choices of density and pressure are possible with the same velocity field: $$ A {\textstyle A} $$ $$ B {\textstyle B} $$ $$ ρ {\textstyle \rho } $$ Another choice of pressure and density with the same velocity vector above is one where the pressure and density fall to zero at the origin and are highest in the central loop at z = 0, x2 + y2 = r2: ρ ( x , y , z ) = 20 B ( x 2 + y 2 ) ( r 2 + x 2 + y 2 + z 2 ) 3 p ( x , y , z ) = − A 2 B ( r 2 + x 2 + y 2 + z 2 ) 4 + − 4 A 2 B ( x 2 + y 2 ) ( r 2 + x 2 + y 2 + z 2 ) 5 . {\begin{aligned\rho (x,y,z)&={\frac {20B\left(x^{2}+y^{2}\right)}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{3}}}\\p(x,y,z)&={\frac {-A^{2}B}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{4}}}+{\frac {-4A^{2}B\left(x^{2}+y^{2}\right)}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{5}}}.\end{aligned}}} $$ ρ ( x , y , z ) = 20 B ( x 2 + y 2 ) ( r 2 + x 2 + y 2 + z 2 ) 3 p ( x , y , z ) = − A 2 B ( r 2 + x 2 + y 2 + z 2 ) 4 + − 4 A 2 B ( x 2 + y 2 ) ( r 2 + x 2 + y 2 + z 2 ) 5 . {\displaystyle {\begin{aligned}\rho (x,y,z)&={\frac {20B\left(x^{2}+y^{2}\right)}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{3}}}\\p(x,y,z)&={\frac {-A^{2}B}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{4}}}+{\frac {-4A^{2}B\left(x^{2}+y^{2}\right)}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{5}}}.\end{aligned}}} $$ In fact in general there are simple solutions for any polynomial function f where the density is: ρ ( x , y , z ) = 1 r 2 + x 2 + y 2 + z 2 f ( x 2 + y 2 ( r 2 + x 2 + y 2 + z 2 ) 2 ) . \rho (x,y,z)={\frac {1{r^{2}+x^{2}+y^{2}+z^{2}}}f\left({\frac {x^{2}+y^{2}}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{2}}}\right).} $$ ρ ( x , y , z ) = 1 r 2 + x 2 + y 2 + z 2 f ( x 2 + y 2 ( r 2 + x 2 + y 2 + z 2 ) 2 ) . {\displaystyle \rho (x,y,z)={\frac {1}{r^{2}+x^{2}+y^{2}+z^{2}}}f\left({\frac {x^{2}+y^{2}}{\left(r^{2}+x^{2}+y^{2}+z^{2}\right)^{2}}}\right).} $$ Two examples of periodic fully-three-dimensional viscous solutions are described in. These solutions are defined on a three-dimensional torus T 3 = [ 0 , L ] 3 \mathbb {T ^{3}=[0,L]^{3}} and are characterized by positive and negative helicity respectively. The solution with positive helicity is given by: u x = 4 2 3 3 U 0 [ sin ⁡ ( k x − π / 3 ) cos ⁡ ( k y + π / 3 ) sin ⁡ ( k z + π / 2 ) − cos ⁡ ( k z − π / 3 ) sin ⁡ ( k x + π / 3 ) sin ⁡ ( k y + π / 2 ) ] e − 3 ν k 2 t u y = 4 2 3 3 U 0 [ sin ⁡ ( k y − π / 3 ) cos ⁡ ( k z + π / 3 ) sin ⁡ ( k x + π / 2 ) − cos ⁡ ( k x − π / 3 ) sin ⁡ ( k y + π / 3 ) sin ⁡ ( k z + π / 2 ) ] e − 3 ν k 2 t u z = 4 2 3 3 U 0 [ sin ⁡ ( k z − π / 3 ) cos ⁡ ( k x + π / 3 ) sin ⁡ ( k y + π / 2 ) − cos ⁡ ( k y − π / 3 ) sin ⁡ ( k z + π / 3 ) sin ⁡ ( k x + π / 2 ) ] e − 3 ν k 2 t {\begin{alignedu_{x}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(kx-\pi /3)\cos(ky+\pi /3)\sin(kz+\pi /2)-\cos(kz-\pi /3)\sin(kx+\pi /3)\sin(ky+\pi /2)\,\right]e^{-3\nu k^{2}t}\\u_{y}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(ky-\pi /3)\cos(kz+\pi /3)\sin(kx+\pi /2)-\cos(kx-\pi /3)\sin(ky+\pi /3)\sin(kz+\pi /2)\,\right]e^{-3\nu k^{2}t}\\u_{z}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(kz-\pi /3)\cos(kx+\pi /3)\sin(ky+\pi /2)-\cos(ky-\pi /3)\sin(kz+\pi /3)\sin(kx+\pi /2)\,\right]e^{-3\nu k^{2}t}\end{aligned}}} where k = 2 π / L k=2\pi /L is the wave number and the velocity components are normalized so that the average kinetic energy per unit of mass is U 0 2 / 2 U_{0^{2}/2} at t = 0 t=0 . The pressure field is obtained from the velocity field as p = p 0 − ρ 0 ‖ u ‖ 2 / 2 p=p_{0-\rho _{0}\|{\boldsymbol {u}}\|^{2}/2} (where p 0 p_{0} and ρ 0 \rho _{0} are reference values for the pressure and density fields respectively). Since both the solutions belong to the class of Beltrami flow, the vorticity field is parallel to the velocity and, for the case with positive helicity, is given by ω = 3 k u \omega ={\sqrt {3}\,k\,{\boldsymbol {u}}} . These solutions can be regarded as a generalization in three dimensions of the classic two-dimensional Taylor-Green Taylor–Green vortex. $$ T 3 = [ 0 , L ] 3 {\displaystyle \mathbb {T} ^{3}=[0,L]^{3}} $$ $$ u x = 4 2 3 3 U 0 [ sin ⁡ ( k x − π / 3 ) cos ⁡ ( k y + π / 3 ) sin ⁡ ( k z + π / 2 ) − cos ⁡ ( k z − π / 3 ) sin ⁡ ( k x + π / 3 ) sin ⁡ ( k y + π / 2 ) ] e − 3 ν k 2 t u y = 4 2 3 3 U 0 [ sin ⁡ ( k y − π / 3 ) cos ⁡ ( k z + π / 3 ) sin ⁡ ( k x + π / 2 ) − cos ⁡ ( k x − π / 3 ) sin ⁡ ( k y + π / 3 ) sin ⁡ ( k z + π / 2 ) ] e − 3 ν k 2 t u z = 4 2 3 3 U 0 [ sin ⁡ ( k z − π / 3 ) cos ⁡ ( k x + π / 3 ) sin ⁡ ( k y + π / 2 ) − cos ⁡ ( k y − π / 3 ) sin ⁡ ( k z + π / 3 ) sin ⁡ ( k x + π / 2 ) ] e − 3 ν k 2 t {\displaystyle {\begin{aligned}u_{x}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(kx-\pi /3)\cos(ky+\pi /3)\sin(kz+\pi /2)-\cos(kz-\pi /3)\sin(kx+\pi /3)\sin(ky+\pi /2)\,\right]e^{-3\nu k^{2}t}\\u_{y}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(ky-\pi /3)\cos(kz+\pi /3)\sin(kx+\pi /2)-\cos(kx-\pi /3)\sin(ky+\pi /3)\sin(kz+\pi /2)\,\right]e^{-3\nu k^{2}t}\\u_{z}&={\frac {4{\sqrt {2}}}{3{\sqrt {3}}}}\,U_{0}\left[\,\sin(kz-\pi /3)\cos(kx+\pi /3)\sin(ky+\pi /2)-\cos(ky-\pi /3)\sin(kz+\pi /3)\sin(kx+\pi /2)\,\right]e^{-3\nu k^{2}t}\end{aligned}}} $$ $$ k = 2 π / L {\displaystyle k=2\pi /L} $$ $$ U 0 2 / 2 {\displaystyle U_{0}^{2}/2} $$ $$ t = 0 {\displaystyle t=0} $$ $$ p = p 0 − ρ 0 ‖ u ‖ 2 / 2 {\displaystyle p=p_{0}-\rho _{0}\|{\boldsymbol {u}}\|^{2}/2} $$ $$ p 0 {\displaystyle p_{0}} $$ $$ ρ 0 {\displaystyle \rho _{0}} $$ $$ ω = 3 k u {\displaystyle \omega ={\sqrt {3}}\,k\,{\boldsymbol {u}}} $$ Wyld diagrams are bookkeeping graphs that correspond to the Navier–Stokes equations via a perturbation expansion of the fundamental continuum mechanics. Similar to the Feynman diagrams in quantum field theory, these diagrams are an extension of Keldysh's technique for nonequilibrium processes in fluid dynamics. In other words, these diagrams assign graphs to the (often) turbulent phenomena in turbulent fluids by allowing correlated and interacting fluid particles to obey stochastic processes associated to pseudo-random functions in probability distributions. Note that the formulas in this section make use of the single-line notation for partial derivatives, where, e.g. ∂ x u {\textstyle \partial _{x}u} means the partial derivative of u {\textstyle u} with respect to x {\textstyle x} , and ∂ y 2 f θ {\textstyle \partial _{y}^{2}f_{\theta }} means the second-order partial derivative of f θ {\textstyle f_{\theta }} with respect to y {\textstyle y} . $$ ∂ x u {\textstyle \partial _{x}u} $$ $$ u {\textstyle u} $$ $$ x {\textstyle x} $$ $$ ∂ y 2 f θ {\textstyle \partial _{y}^{2}f_{\theta }} $$ $$ f θ {\textstyle f_{\theta }} $$ $$ y {\textstyle y} $$ A 2022 paper provides a less costly, dynamical and recurrent solution of the Navier-Stokes equation for 3D turbulent fluid flows. On suitably short time scales, the dynamics of turbulence is deterministic. From the general form of the Navier–Stokes, with the velocity vector expanded as u = ( u x , u y , u z ) {\textstyle \mathbf {u} =(u_{x},u_{y},u_{z})} , sometimes respectively named u {\textstyle u} , v {\textstyle v} , w {\textstyle w} , we may write the vector equation explicitly, x : ρ ( ∂ t u x + u x ∂ x u x + u y ∂ y u x + u z ∂ z u x ) = − ∂ x p + μ ( ∂ x 2 u x + ∂ y 2 u x + ∂ z 2 u x ) + 1 3 μ ∂ x ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g x {\begin{alignedx:\ &\rho \left({\partial _{t}u_{x}}+u_{x}\,{\partial _{x}u_{x}}+u_{y}\,{\partial _{y}u_{x}}+u_{z}\,{\partial _{z}u_{x}}\right)\\&\quad =-\partial _{x}p+\mu \left({\partial _{x}^{2}u_{x}}+{\partial _{y}^{2}u_{x}}+{\partial _{z}^{2}u_{x}}\right)+{\frac {1}{3}}\mu \ \partial _{x}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{x}\\\end{aligned}}} y : ρ ( ∂ t u y + u x ∂ x u y + u y ∂ y u y + u z ∂ z u y ) = − ∂ y p + μ ( ∂ x 2 u y + ∂ y 2 u y + ∂ z 2 u y ) + 1 3 μ ∂ y ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g y {\begin{alignedy:\ &\rho \left({\partial _{t}u_{y}}+u_{x}{\partial _{x}u_{y}}+u_{y}{\partial _{y}u_{y}}+u_{z}{\partial _{z}u_{y}}\right)\\&\quad =-{\partial _{y}p}+\mu \left({\partial _{x}^{2}u_{y}}+{\partial _{y}^{2}u_{y}}+{\partial _{z}^{2}u_{y}}\right)+{\frac {1}{3}}\mu \ \partial _{y}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{y}\\\end{aligned}}} z : ρ ( ∂ t u z + u x ∂ x u z + u y ∂ y u z + u z ∂ z u z ) = − ∂ z p + μ ( ∂ x 2 u z + ∂ y 2 u z + ∂ z 2 u z ) + 1 3 μ ∂ z ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g z . {\begin{alignedz:\ &\rho \left({\partial _{t}u_{z}}+u_{x}{\partial _{x}u_{z}}+u_{y}{\partial _{y}u_{z}}+u_{z}{\partial _{z}u_{z}}\right)\\&\quad =-{\partial _{z}p}+\mu \left({\partial _{x}^{2}u_{z}}+{\partial _{y}^{2}u_{z}}+{\partial _{z}^{2}u_{z}}\right)+{\frac {1}{3}}\mu \ \partial _{z}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{z}.\end{aligned}}} $$ u = ( u x , u y , u z ) {\textstyle \mathbf {u} =(u_{x},u_{y},u_{z})} $$ $$ u {\textstyle u} $$ $$ v {\textstyle v} $$ $$ w {\textstyle w} $$ $$ x : ρ ( ∂ t u x + u x ∂ x u x + u y ∂ y u x + u z ∂ z u x ) = − ∂ x p + μ ( ∂ x 2 u x + ∂ y 2 u x + ∂ z 2 u x ) + 1 3 μ ∂ x ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g x {\displaystyle {\begin{aligned}x:\ &\rho \left({\partial _{t}u_{x}}+u_{x}\,{\partial _{x}u_{x}}+u_{y}\,{\partial _{y}u_{x}}+u_{z}\,{\partial _{z}u_{x}}\right)\\&\quad =-\partial _{x}p+\mu \left({\partial _{x}^{2}u_{x}}+{\partial _{y}^{2}u_{x}}+{\partial _{z}^{2}u_{x}}\right)+{\frac {1}{3}}\mu \ \partial _{x}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{x}\\\end{aligned}}} $$ $$ y : ρ ( ∂ t u y + u x ∂ x u y + u y ∂ y u y + u z ∂ z u y ) = − ∂ y p + μ ( ∂ x 2 u y + ∂ y 2 u y + ∂ z 2 u y ) + 1 3 μ ∂ y ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g y {\displaystyle {\begin{aligned}y:\ &\rho \left({\partial _{t}u_{y}}+u_{x}{\partial _{x}u_{y}}+u_{y}{\partial _{y}u_{y}}+u_{z}{\partial _{z}u_{y}}\right)\\&\quad =-{\partial _{y}p}+\mu \left({\partial _{x}^{2}u_{y}}+{\partial _{y}^{2}u_{y}}+{\partial _{z}^{2}u_{y}}\right)+{\frac {1}{3}}\mu \ \partial _{y}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{y}\\\end{aligned}}} $$ $$ z : ρ ( ∂ t u z + u x ∂ x u z + u y ∂ y u z + u z ∂ z u z ) = − ∂ z p + μ ( ∂ x 2 u z + ∂ y 2 u z + ∂ z 2 u z ) + 1 3 μ ∂ z ( ∂ x u x + ∂ y u y + ∂ z u z ) + ρ g z . {\displaystyle {\begin{aligned}z:\ &\rho \left({\partial _{t}u_{z}}+u_{x}{\partial _{x}u_{z}}+u_{y}{\partial _{y}u_{z}}+u_{z}{\partial _{z}u_{z}}\right)\\&\quad =-{\partial _{z}p}+\mu \left({\partial _{x}^{2}u_{z}}+{\partial _{y}^{2}u_{z}}+{\partial _{z}^{2}u_{z}}\right)+{\frac {1}{3}}\mu \ \partial _{z}\left({\partial _{x}u_{x}}+{\partial _{y}u_{y}}+{\partial _{z}u_{z}}\right)+\rho g_{z}.\end{aligned}}} $$ Note that gravity has been accounted for as a body force, and the values of g x {\textstyle g_{x}} , g y {\textstyle g_{y}} , g z {\textstyle g_{z}} will depend on the orientation of gravity with respect to the chosen set of coordinates. $$ g x {\textstyle g_{x}} $$ $$ g y {\textstyle g_{y}} $$ $$ g z {\textstyle g_{z}} $$ The continuity equation reads: ∂ t ρ + ∂ x ( ρ u x ) + ∂ y ( ρ u y ) + ∂ z ( ρ u z ) = 0. \partial _{t\rho +\partial _{x}(\rho u_{x})+\partial _{y}(\rho u_{y})+\partial _{z}(\rho u_{z})=0.} $$ ∂ t ρ + ∂ x ( ρ u x ) + ∂ y ( ρ u y ) + ∂ z ( ρ u z ) = 0. {\displaystyle \partial _{t}\rho +\partial _{x}(\rho u_{x})+\partial _{y}(\rho u_{y})+\partial _{z}(\rho u_{z})=0.} $$ When the flow is incompressible, ρ {\textstyle \rho } does not change for any fluid particle, and its material derivative vanishes: D ρ D t = 0 {\textstyle {\frac {\mathrm {D} \rho }{\mathrm {D} t}}=0} . The continuity equation is reduced to: ∂ x u x + ∂ y u y + ∂ z u z = 0. \partial _{xu_{x}+\partial _{y}u_{y}+\partial _{z}u_{z}=0.} $$ ρ {\textstyle \rho } $$ $$ D ρ D t = 0 {\textstyle {\frac {\mathrm {D} \rho }{\mathrm {D} t}}=0} $$ $$ ∂ x u x + ∂ y u y + ∂ z u z = 0. {\displaystyle \partial _{x}u_{x}+\partial _{y}u_{y}+\partial _{z}u_{z}=0.} $$ Thus, for the incompressible version of the Navier–Stokes equation the second part of the viscous terms fall away (see Incompressible flow). This system of four equations comprises the most commonly used and studied form. Though comparatively more compact than other representations, this is still a nonlinear system of partial differential equations for which solutions are difficult to obtain. A change of variables on the Cartesian equations will yield the following momentum equations for r {\textstyle r} , ϕ {\textstyle \phi } , and z {\textstyle z} r : ρ ( ∂ t u r + u r ∂ r u r + u φ r ∂ φ u r + u z ∂ z u r − u φ 2 r ) = − ∂ r p + μ ( 1 r ∂ r ( r ∂ r u r ) + 1 r 2 ∂ φ 2 u r + ∂ z 2 u r − u r r 2 − 2 r 2 ∂ φ u φ ) + 1 3 μ ∂ r ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g r {\begin{alignedr:\ &\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{r}}+u_{z}{\partial _{z}u_{r}}-{\frac {u_{\varphi }^{2}}{r}}\right)\\&\quad =-{\partial _{r}p}\\&\qquad +\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{r}}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{r}}+{\partial _{z}^{2}u_{r}}-{\frac {u_{r}}{r^{2}}}-{\frac {2}{r^{2}}}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{r}\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{r}\\[8px]\end{aligned}}} φ : ρ ( ∂ t u φ + u r ∂ r u φ + u φ r ∂ φ u φ + u z ∂ z u φ + u r u φ r ) = − 1 r ∂ φ p + μ ( 1 r ∂ r ( r ∂ r u φ ) + 1 r 2 ∂ φ 2 u φ + ∂ z 2 u φ − u φ r 2 + 2 r 2 ∂ φ u r ) + 1 3 μ 1 r ∂ φ ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g φ {\begin{aligned\varphi :\ &\rho \left({\partial _{t}u_{\varphi }}+u_{r}{\partial _{r}u_{\varphi }}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{\varphi }}+u_{z}{\partial _{z}u_{\varphi }}+{\frac {u_{r}u_{\varphi }}{r}}\right)\\&\quad =-{\frac {1}{r}}{\partial _{\varphi }p}\\&\qquad +\mu \left({\frac {1}{r}}\ \partial _{r}\left(r{\partial _{r}u_{\varphi }}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{\varphi }}+{\partial _{z}^{2}u_{\varphi }}-{\frac {u_{\varphi }}{r^{2}}}+{\frac {2}{r^{2}}}{\partial _{\varphi }u_{r}}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r}}\partial _{\varphi }\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{\varphi }\\[8px]\end{aligned}}} z : ρ ( ∂ t u z + u r ∂ r u z + u φ r ∂ φ u z + u z ∂ z u z ) = − ∂ z p + μ ( 1 r ∂ r ( r ∂ r u z ) + 1 r 2 ∂ φ 2 u z + ∂ z 2 u z ) + 1 3 μ ∂ z ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g z . {\begin{alignedz:\ &\rho \left({\partial _{t}u_{z}}+u_{r}{\partial _{r}u_{z}}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{z}}+u_{z}{\partial _{z}u_{z}}\right)\\&\quad =-{\partial _{z}p}\\&\qquad +\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{z}}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{z}}+{\partial _{z}^{2}u_{z}}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{z}\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{z}.\end{aligned}}} $$ r {\textstyle r} $$ $$ ϕ {\textstyle \phi } $$ $$ z {\textstyle z} $$ $$ r : ρ ( ∂ t u r + u r ∂ r u r + u φ r ∂ φ u r + u z ∂ z u r − u φ 2 r ) = − ∂ r p + μ ( 1 r ∂ r ( r ∂ r u r ) + 1 r 2 ∂ φ 2 u r + ∂ z 2 u r − u r r 2 − 2 r 2 ∂ φ u φ ) + 1 3 μ ∂ r ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g r {\displaystyle {\begin{aligned}r:\ &\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{r}}+u_{z}{\partial _{z}u_{r}}-{\frac {u_{\varphi }^{2}}{r}}\right)\\&\quad =-{\partial _{r}p}\\&\qquad +\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{r}}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{r}}+{\partial _{z}^{2}u_{r}}-{\frac {u_{r}}{r^{2}}}-{\frac {2}{r^{2}}}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{r}\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{r}\\[8px]\end{aligned}}} $$ $$ φ : ρ ( ∂ t u φ + u r ∂ r u φ + u φ r ∂ φ u φ + u z ∂ z u φ + u r u φ r ) = − 1 r ∂ φ p + μ ( 1 r ∂ r ( r ∂ r u φ ) + 1 r 2 ∂ φ 2 u φ + ∂ z 2 u φ − u φ r 2 + 2 r 2 ∂ φ u r ) + 1 3 μ 1 r ∂ φ ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g φ {\displaystyle {\begin{aligned}\varphi :\ &\rho \left({\partial _{t}u_{\varphi }}+u_{r}{\partial _{r}u_{\varphi }}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{\varphi }}+u_{z}{\partial _{z}u_{\varphi }}+{\frac {u_{r}u_{\varphi }}{r}}\right)\\&\quad =-{\frac {1}{r}}{\partial _{\varphi }p}\\&\qquad +\mu \left({\frac {1}{r}}\ \partial _{r}\left(r{\partial _{r}u_{\varphi }}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{\varphi }}+{\partial _{z}^{2}u_{\varphi }}-{\frac {u_{\varphi }}{r^{2}}}+{\frac {2}{r^{2}}}{\partial _{\varphi }u_{r}}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r}}\partial _{\varphi }\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{\varphi }\\[8px]\end{aligned}}} $$ $$ z : ρ ( ∂ t u z + u r ∂ r u z + u φ r ∂ φ u z + u z ∂ z u z ) = − ∂ z p + μ ( 1 r ∂ r ( r ∂ r u z ) + 1 r 2 ∂ φ 2 u z + ∂ z 2 u z ) + 1 3 μ ∂ z ( 1 r ∂ r ( r u r ) + 1 r ∂ φ u φ + ∂ z u z ) + ρ g z . {\displaystyle {\begin{aligned}z:\ &\rho \left({\partial _{t}u_{z}}+u_{r}{\partial _{r}u_{z}}+{\frac {u_{\varphi }}{r}}{\partial _{\varphi }u_{z}}+u_{z}{\partial _{z}u_{z}}\right)\\&\quad =-{\partial _{z}p}\\&\qquad +\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{z}}\right)+{\frac {1}{r^{2}}}{\partial _{\varphi }^{2}u_{z}}+{\partial _{z}^{2}u_{z}}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{z}\left({\frac {1}{r}}{\partial _{r}\left(ru_{r}\right)}+{\frac {1}{r}}{\partial _{\varphi }u_{\varphi }}+{\partial _{z}u_{z}}\right)\\&\qquad +\rho g_{z}.\end{aligned}}} $$ The gravity components will generally not be constants, however for most applications either the coordinates are chosen so that the gravity components are constant or else it is assumed that gravity is counteracted by a pressure field (for example, flow in horizontal pipe is treated normally without gravity and without a vertical pressure gradient). The continuity equation is: ∂ t ρ + 1 r ∂ r ( ρ r u r ) + 1 r ∂ φ ( ρ u φ ) + ∂ z ( ρ u z ) = 0. {\partial _{t\rho }+{\frac {1}{r}}\partial _{r}\left(\rho ru_{r}\right)+{\frac {1}{r}}{\partial _{\varphi }\left(\rho u_{\varphi }\right)}+{\partial _{z}\left(\rho u_{z}\right)}=0.} $$ ∂ t ρ + 1 r ∂ r ( ρ r u r ) + 1 r ∂ φ ( ρ u φ ) + ∂ z ( ρ u z ) = 0. {\displaystyle {\partial _{t}\rho }+{\frac {1}{r}}\partial _{r}\left(\rho ru_{r}\right)+{\frac {1}{r}}{\partial _{\varphi }\left(\rho u_{\varphi }\right)}+{\partial _{z}\left(\rho u_{z}\right)}=0.} $$ This cylindrical representation of the incompressible Navier–Stokes equations is the second most commonly seen (the first being Cartesian above). Cylindrical coordinates are chosen to take advantage of symmetry, so that a velocity component can disappear. A very common case is axisymmetric flow with the assumption of no tangential velocity ( u ϕ = 0 {\textstyle u_{\phi }=0} ), and the remaining quantities are independent of ϕ {\textstyle \phi } : ρ ( ∂ t u r + u r ∂ r u r + u z ∂ z u r ) = − ∂ r p + μ ( 1 r ∂ r ( r ∂ r u r ) + ∂ z 2 u r − u r r 2 ) + ρ g r ρ ( ∂ t u z + u r ∂ r u z + u z ∂ z u z ) = − ∂ z p + μ ( 1 r ∂ r ( r ∂ r u z ) + ∂ z 2 u z ) + ρ g z 1 r ∂ r ( r u r ) + ∂ z u z = 0. {\begin{aligned\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+u_{z}{\partial _{z}u_{r}}\right)&=-{\partial _{r}p}+\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{r}}\right)+{\partial _{z}^{2}u_{r}}-{\frac {u_{r}}{r^{2}}}\right)+\rho g_{r}\\\rho \left({\partial _{t}u_{z}}+u_{r}{\partial _{r}u_{z}}+u_{z}{\partial _{z}u_{z}}\right)&=-{\partial _{z}p}+\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{z}}\right)+{\partial _{z}^{2}u_{z}}\right)+\rho g_{z}\\{\frac {1}{r}}\partial _{r}\left(ru_{r}\right)+{\partial _{z}u_{z}}&=0.\end{aligned}}} $$ u ϕ = 0 {\textstyle u_{\phi }=0} $$ $$ ϕ {\textstyle \phi } $$ $$ ρ ( ∂ t u r + u r ∂ r u r + u z ∂ z u r ) = − ∂ r p + μ ( 1 r ∂ r ( r ∂ r u r ) + ∂ z 2 u r − u r r 2 ) + ρ g r ρ ( ∂ t u z + u r ∂ r u z + u z ∂ z u z ) = − ∂ z p + μ ( 1 r ∂ r ( r ∂ r u z ) + ∂ z 2 u z ) + ρ g z 1 r ∂ r ( r u r ) + ∂ z u z = 0. {\displaystyle {\begin{aligned}\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+u_{z}{\partial _{z}u_{r}}\right)&=-{\partial _{r}p}+\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{r}}\right)+{\partial _{z}^{2}u_{r}}-{\frac {u_{r}}{r^{2}}}\right)+\rho g_{r}\\\rho \left({\partial _{t}u_{z}}+u_{r}{\partial _{r}u_{z}}+u_{z}{\partial _{z}u_{z}}\right)&=-{\partial _{z}p}+\mu \left({\frac {1}{r}}\partial _{r}\left(r{\partial _{r}u_{z}}\right)+{\partial _{z}^{2}u_{z}}\right)+\rho g_{z}\\{\frac {1}{r}}\partial _{r}\left(ru_{r}\right)+{\partial _{z}u_{z}}&=0.\end{aligned}}} $$ In spherical coordinates, the r {\textstyle r} , ϕ {\textstyle \phi } , and θ {\textstyle \theta } momentum equations are (note the convention used: θ {\textstyle \theta } is polar angle, or colatitude, 0 ≤ θ ≤ π {\textstyle 0\leq \theta \leq \pi } ): r : ρ ( ∂ t u r + u r ∂ r u r + u φ r sin ⁡ θ ∂ φ u r + u θ r ∂ θ u r − u φ 2 + u θ 2 r ) = − ∂ r p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u r ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u r + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u r ) − 2 u r + ∂ θ u θ + u θ cot ⁡ θ r 2 − 2 r 2 sin ⁡ θ ∂ φ u φ ) + 1 3 μ ∂ r ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g r {\begin{alignedr:\ &\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{r}}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{r}}-{\frac {u_{\varphi }^{2}+u_{\theta }^{2}}{r}}\right)\\&\quad =-{\partial _{r}p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{r}}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{r}}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{r}}\right)-2{\frac {u_{r}+{\partial _{\theta }u_{\theta }}+u_{\theta }\cot \theta }{r^{2}}}-{\frac {2}{r^{2}\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{r}\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{r}\\[8px]\end{aligned}}} φ : ρ ( ∂ t u φ + u r ∂ r u φ + u φ r sin ⁡ θ ∂ φ u φ + u θ r ∂ θ u φ + u r u φ + u φ u θ cot ⁡ θ r ) = − 1 r sin ⁡ θ ∂ φ p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u φ ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u φ + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u φ ) + 2 sin ⁡ θ ∂ φ u r + 2 cos ⁡ θ ∂ φ u θ − u φ r 2 sin 2 ⁡ θ ) + 1 3 μ 1 r sin ⁡ θ ∂ φ ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g φ {\begin{aligned\varphi :\ &\rho \left({\partial _{t}u_{\varphi }}+u_{r}{\partial _{r}u_{\varphi }}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{\varphi }}+{\frac {u_{r}u_{\varphi }+u_{\varphi }u_{\theta }\cot \theta }{r}}\right)\\&\quad =-{\frac {1}{r\sin \theta }}{\partial _{\varphi }p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{\varphi }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{\varphi }}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{\varphi }}\right)+{\frac {2\sin \theta {\partial _{\varphi }u_{r}}+2\cos \theta {\partial _{\varphi }u_{\theta }}-u_{\varphi }}{r^{2}\sin ^{2}\theta }}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r\sin \theta }}\partial _{\varphi }\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{\varphi }\\[8px]\end{aligned}}} θ : ρ ( ∂ t u θ + u r ∂ r u θ + u φ r sin ⁡ θ ∂ φ u θ + u θ r ∂ θ u θ + u r u θ − u φ 2 cot ⁡ θ r ) = − 1 r ∂ θ p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u θ ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u θ + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u θ ) + 2 r 2 ∂ θ u r − u θ + 2 cos ⁡ θ ∂ φ u φ r 2 sin 2 ⁡ θ ) + 1 3 μ 1 r ∂ θ ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g θ . {\begin{aligned\theta :\ &\rho \left({\partial _{t}u_{\theta }}+u_{r}{\partial _{r}u_{\theta }}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{\theta }}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{\theta }}+{\frac {u_{r}u_{\theta }-u_{\varphi }^{2}\cot \theta }{r}}\right)\\&\quad =-{\frac {1}{r}}{\partial _{\theta }p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{\theta }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{\theta }}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{\theta }}\right)+{\frac {2}{r^{2}}}{\partial _{\theta }u_{r}}-{\frac {u_{\theta }+2\cos \theta {\partial _{\varphi }u_{\varphi }}}{r^{2}\sin ^{2}\theta }}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r}}\partial _{\theta }\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{\theta }.\end{aligned}}} $$ r {\textstyle r} $$ $$ ϕ {\textstyle \phi } $$ $$ θ {\textstyle \theta } $$ $$ θ {\textstyle \theta } $$ $$ 0 ≤ θ ≤ π {\textstyle 0\leq \theta \leq \pi } $$ $$ r : ρ ( ∂ t u r + u r ∂ r u r + u φ r sin ⁡ θ ∂ φ u r + u θ r ∂ θ u r − u φ 2 + u θ 2 r ) = − ∂ r p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u r ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u r + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u r ) − 2 u r + ∂ θ u θ + u θ cot ⁡ θ r 2 − 2 r 2 sin ⁡ θ ∂ φ u φ ) + 1 3 μ ∂ r ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g r {\displaystyle {\begin{aligned}r:\ &\rho \left({\partial _{t}u_{r}}+u_{r}{\partial _{r}u_{r}}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{r}}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{r}}-{\frac {u_{\varphi }^{2}+u_{\theta }^{2}}{r}}\right)\\&\quad =-{\partial _{r}p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{r}}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{r}}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{r}}\right)-2{\frac {u_{r}+{\partial _{\theta }u_{\theta }}+u_{\theta }\cot \theta }{r^{2}}}-{\frac {2}{r^{2}\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +{\frac {1}{3}}\mu \partial _{r}\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{r}\\[8px]\end{aligned}}} $$ $$ φ : ρ ( ∂ t u φ + u r ∂ r u φ + u φ r sin ⁡ θ ∂ φ u φ + u θ r ∂ θ u φ + u r u φ + u φ u θ cot ⁡ θ r ) = − 1 r sin ⁡ θ ∂ φ p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u φ ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u φ + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u φ ) + 2 sin ⁡ θ ∂ φ u r + 2 cos ⁡ θ ∂ φ u θ − u φ r 2 sin 2 ⁡ θ ) + 1 3 μ 1 r sin ⁡ θ ∂ φ ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g φ {\displaystyle {\begin{aligned}\varphi :\ &\rho \left({\partial _{t}u_{\varphi }}+u_{r}{\partial _{r}u_{\varphi }}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{\varphi }}+{\frac {u_{r}u_{\varphi }+u_{\varphi }u_{\theta }\cot \theta }{r}}\right)\\&\quad =-{\frac {1}{r\sin \theta }}{\partial _{\varphi }p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{\varphi }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{\varphi }}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{\varphi }}\right)+{\frac {2\sin \theta {\partial _{\varphi }u_{r}}+2\cos \theta {\partial _{\varphi }u_{\theta }}-u_{\varphi }}{r^{2}\sin ^{2}\theta }}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r\sin \theta }}\partial _{\varphi }\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{\varphi }\\[8px]\end{aligned}}} $$ $$ θ : ρ ( ∂ t u θ + u r ∂ r u θ + u φ r sin ⁡ θ ∂ φ u θ + u θ r ∂ θ u θ + u r u θ − u φ 2 cot ⁡ θ r ) = − 1 r ∂ θ p + μ ( 1 r 2 ∂ r ( r 2 ∂ r u θ ) + 1 r 2 sin 2 ⁡ θ ∂ φ 2 u θ + 1 r 2 sin ⁡ θ ∂ θ ( sin ⁡ θ ∂ θ u θ ) + 2 r 2 ∂ θ u r − u θ + 2 cos ⁡ θ ∂ φ u φ r 2 sin 2 ⁡ θ ) + 1 3 μ 1 r ∂ θ ( 1 r 2 ∂ r ( r 2 u r ) + 1 r sin ⁡ θ ∂ θ ( u θ sin ⁡ θ ) + 1 r sin ⁡ θ ∂ φ u φ ) + ρ g θ . {\displaystyle {\begin{aligned}\theta :\ &\rho \left({\partial _{t}u_{\theta }}+u_{r}{\partial _{r}u_{\theta }}+{\frac {u_{\varphi }}{r\sin \theta }}{\partial _{\varphi }u_{\theta }}+{\frac {u_{\theta }}{r}}{\partial _{\theta }u_{\theta }}+{\frac {u_{r}u_{\theta }-u_{\varphi }^{2}\cot \theta }{r}}\right)\\&\quad =-{\frac {1}{r}}{\partial _{\theta }p}\\&\qquad +\mu \left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}{\partial _{r}u_{\theta }}\right)+{\frac {1}{r^{2}\sin ^{2}\theta }}{\partial _{\varphi }^{2}u_{\theta }}+{\frac {1}{r^{2}\sin \theta }}\partial _{\theta }\left(\sin \theta {\partial _{\theta }u_{\theta }}\right)+{\frac {2}{r^{2}}}{\partial _{\theta }u_{r}}-{\frac {u_{\theta }+2\cos \theta {\partial _{\varphi }u_{\varphi }}}{r^{2}\sin ^{2}\theta }}\right)\\&\qquad +{\frac {1}{3}}\mu {\frac {1}{r}}\partial _{\theta }\left({\frac {1}{r^{2}}}\partial _{r}\left(r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(u_{\theta }\sin \theta \right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }u_{\varphi }}\right)\\&\qquad +\rho g_{\theta }.\end{aligned}}} $$ Mass continuity will read: ∂ t ρ + 1 r 2 ∂ r ( ρ r 2 u r ) + 1 r sin ⁡ θ ∂ φ ( ρ u φ ) + 1 r sin ⁡ θ ∂ θ ( sin ⁡ θ ρ u θ ) = 0. {\partial _{t\rho }+{\frac {1}{r^{2}}}\partial _{r}\left(\rho r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }(\rho u_{\varphi })}+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(\sin \theta \rho u_{\theta }\right)=0.} $$ ∂ t ρ + 1 r 2 ∂ r ( ρ r 2 u r ) + 1 r sin ⁡ θ ∂ φ ( ρ u φ ) + 1 r sin ⁡ θ ∂ θ ( sin ⁡ θ ρ u θ ) = 0. {\displaystyle {\partial _{t}\rho }+{\frac {1}{r^{2}}}\partial _{r}\left(\rho r^{2}u_{r}\right)+{\frac {1}{r\sin \theta }}{\partial _{\varphi }(\rho u_{\varphi })}+{\frac {1}{r\sin \theta }}\partial _{\theta }\left(\sin \theta \rho u_{\theta }\right)=0.} $$ These equations could be (slightly) compacted by, for example, factoring 1 r 2 {\textstyle {\frac {1}{r^{2}}}} from the viscous terms. However, doing so would undesirably alter the structure of the Laplacian and other quantities. $$ 1 r 2 {\textstyle {\frac {1}{r^{2}}}} $$ The Navier–Stokes equations are used extensively in video games in order to model a wide variety of natural phenomena. Simulations of small-scale gaseous fluids, such as fire and smoke, are often based on the seminal paper "Real-Time Fluid Dynamics for Games" by Jos Stam, which elaborates one of the methods proposed in Stam's earlier, more famous paper "Stable Fluids" from 1999. Stam proposes stable fluid simulation using a Navier–Stokes solution method from 1968, coupled with an unconditionally stable semi-Lagrangian advection scheme, as first proposed in 1992. More recent implementations based upon this work run on the game systems graphics processing unit (GPU) as opposed to the central processing unit (CPU) and achieve a much higher degree of performance. Many improvements have been proposed to Stam's original work, which suffers inherently from high numerical dissipation in both velocity and mass. An introduction to interactive fluid simulation can be found in the 2007 ACM SIGGRAPH course, Fluid Simulation for Computer Animation.

End of content from: https://en.wikipedia.org/wiki/Navier–Stokes_equations

--------------------------------------------------------------------------------

